{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72d7f98",
   "metadata": {},
   "source": [
    "Train one MLP for each slice. Average the predictions of the 11 MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c6eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing on device: cuda:1\n",
      "Loading models from: slice_specific_models\n",
      "Testing on slices: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]\n",
      "\n",
      "--- Testing Slice: 120 ---\n",
      "Found 296 valid files for slice 120 in split test.\n",
      "Loaded model weights from slice_specific_models/best_model_slice_120.pth\n",
      "Slice 120 Test MAE: 7.078\n",
      "\n",
      "--- Testing Slice: 121 ---\n",
      "Found 296 valid files for slice 121 in split test.\n",
      "Loaded model weights from slice_specific_models/best_model_slice_121.pth\n",
      "Slice 121 Test MAE: 7.898\n",
      "\n",
      "--- Testing Slice: 122 ---\n",
      "Found 296 valid files for slice 122 in split test.\n",
      "Loaded model weights from slice_specific_models/best_model_slice_122.pth\n",
      "Slice 122 Test MAE: 7.434\n",
      "\n",
      "--- Testing Slice: 123 ---\n",
      "Found 296 valid files for slice 123 in split test.\n",
      "Loaded model weights from slice_specific_models/best_model_slice_123.pth\n",
      "Slice 123 Test MAE: 7.943\n",
      "\n",
      "--- Testing Slice: 124 ---\n",
      "Found 296 valid files for slice 124 in split test.\n",
      "Loaded model weights from slice_specific_models/best_model_slice_124.pth\n",
      "Slice 124 Test MAE: 8.027\n",
      "\n",
      "--- Testing Slice: 125 ---\n",
      "Found 296 valid files for slice 125 in split test.\n",
      "Loaded model weights from slice_specific_models/best_model_slice_125.pth\n",
      "Slice 125 Test MAE: 7.792\n",
      "\n",
      "--- Testing Slice: 126 ---\n",
      "Found 296 valid files for slice 126 in split test.\n",
      "Loaded model weights from slice_specific_models/best_model_slice_126.pth\n",
      "Slice 126 Test MAE: 8.044\n",
      "\n",
      "--- Testing Slice: 127 ---\n",
      "Found 296 valid files for slice 127 in split test.\n",
      "Loaded model weights from slice_specific_models/best_model_slice_127.pth\n",
      "Slice 127 Test MAE: 7.612\n",
      "\n",
      "--- Testing Slice: 128 ---\n",
      "Found 296 valid files for slice 128 in split test.\n",
      "Loaded model weights from slice_specific_models/best_model_slice_128.pth\n",
      "Slice 128 Test MAE: 7.178\n",
      "\n",
      "--- Testing Slice: 129 ---\n",
      "Found 296 valid files for slice 129 in split test.\n",
      "Loaded model weights from slice_specific_models/best_model_slice_129.pth\n",
      "Slice 129 Test MAE: 6.965\n",
      "\n",
      "--- Testing Slice: 130 ---\n",
      "Found 296 valid files for slice 130 in split test.\n",
      "Loaded model weights from slice_specific_models/best_model_slice_130.pth\n",
      "Slice 130 Test MAE: 7.697\n",
      "\n",
      "--- Calculating MAE for Averaged Predictions ---\n",
      "\n",
      "Number of subjects with complete predictions across all 11 slices: 296\n",
      "MAE using averaged predictions: 6.523\n",
      "\n",
      "--- Individual Slice Model Test MAEs ---\n",
      "Slice 120: 7.077907085418701\n",
      "Slice 121: 7.898168563842773\n",
      "Slice 122: 7.433528423309326\n",
      "Slice 123: 7.9434895515441895\n",
      "Slice 124: 8.02660846710205\n",
      "Slice 125: 7.792254447937012\n",
      "Slice 126: 8.043877601623535\n",
      "Slice 127: 7.612329483032227\n",
      "Slice 128: 7.177746772766113\n",
      "Slice 129: 6.965033531188965\n",
      "Slice 130: 7.696554660797119\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Configuration (Should match training script) ---\n",
    "FEATURE_ROOT = Path(\"/data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_coronal_120_130\")\n",
    "CSV_PATH = Path(\"/data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\")\n",
    "MODELS_OUTPUT_DIR = Path(\"./slice_specific_models\") # Directory where best models are saved\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32 # Use the same batch size as validation/training if possible, or adjust\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Define the target coronal slices (Must match training and preprocessing)\n",
    "CORONAL_SLICE_CENTER = 125\n",
    "CORONAL_SLICE_RANGE = 5\n",
    "TARGET_CORONAL_INDICES = list(range(CORONAL_SLICE_CENTER - CORONAL_SLICE_RANGE,\n",
    "                                    CORONAL_SLICE_CENTER + CORONAL_SLICE_RANGE + 1)) # e.g., 120 to 130\n",
    "\n",
    "# --- Model Definition (Ensure this class definition is available) ---\n",
    "class AgeMLPWithAttentionBN(nn.Module):\n",
    "    # ... (Paste the full class definition here as in the training script) ...\n",
    "    def __init__(self, input_dim=256, embed_dim=256, num_heads=8,\n",
    "                 hidden_dim1=128, hidden_dim2=64, hidden_dim3=32, dropout_rate=0.3):\n",
    "        super(AgeMLPWithAttentionBN, self).__init__()\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\")\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=False)\n",
    "        self.norm_attn = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1), nn.BatchNorm1d(hidden_dim1), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2), nn.BatchNorm1d(hidden_dim2), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3), nn.BatchNorm1d(hidden_dim3), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_attn = x.unsqueeze(0)\n",
    "        attn_output, _ = self.attention(x_attn, x_attn, x_attn)\n",
    "        x = self.norm_attn(x_attn + attn_output)\n",
    "        x = x.squeeze(0)\n",
    "        output = self.mlp(x)\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "\n",
    "# --- Dataset Definition (Modified to return subject ID) ---\n",
    "class SliceAgePredictionDataset(Dataset):\n",
    "    # ... (Paste the full class definition here, but modify __getitem__) ...\n",
    "    def __init__(self, feature_root, csv_path, split, slice_index):\n",
    "        self.feature_root = Path(feature_root)\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.split = split\n",
    "        self.slice_index = slice_index\n",
    "        self.split_dir = self.feature_root / self.split\n",
    "\n",
    "        if not self.split_dir.is_dir():\n",
    "            raise FileNotFoundError(f\"Split directory not found: {self.split_dir}\")\n",
    "        if not self.csv_path.is_file():\n",
    "            raise FileNotFoundError(f\"CSV file not found: {self.csv_path}\")\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(self.csv_path)\n",
    "            self.meta_dict = df.set_index('filename')['age'].to_dict()\n",
    "            self.subject_id_to_filename = {\n",
    "                fname.replace(\".nii.gz\", \"\").replace(\".nii\", \"\"): fname\n",
    "                for fname in self.meta_dict.keys()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading or processing CSV {self.csv_path}: {e}\")\n",
    "\n",
    "        all_subject_dirs = [d for d in self.split_dir.iterdir() if d.is_dir()]\n",
    "        self.valid_slice_files = []\n",
    "        self.file_to_subject_id = {} # Map file path back to subject_id\n",
    "\n",
    "        for subject_dir in all_subject_dirs:\n",
    "            subject_id = subject_dir.name\n",
    "            slice_filename = f\"slice_{self.slice_index}.npy\"\n",
    "            expected_slice_path = subject_dir / slice_filename\n",
    "            original_filename = self.subject_id_to_filename.get(subject_id)\n",
    "\n",
    "            if original_filename and original_filename in self.meta_dict:\n",
    "                if expected_slice_path.is_file():\n",
    "                    self.valid_slice_files.append(expected_slice_path)\n",
    "                    self.file_to_subject_id[expected_slice_path] = subject_id # Store mapping\n",
    "\n",
    "        if not self.valid_slice_files:\n",
    "             raise RuntimeError(f\"No valid slice files found for slice {self.slice_index} in {self.split_dir} with matching metadata.\")\n",
    "        print(f\"Found {len(self.valid_slice_files)} valid files for slice {self.slice_index} in split {self.split}.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_slice_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slice_path = self.valid_slice_files[idx]\n",
    "        subject_id = self.file_to_subject_id[slice_path] # Retrieve subject_id\n",
    "        original_filename = self.subject_id_to_filename.get(subject_id)\n",
    "\n",
    "        if not original_filename:\n",
    "             raise ValueError(f\"Could not find original filename for subject ID {subject_id}\")\n",
    "\n",
    "        try:\n",
    "            embedding = np.load(slice_path)\n",
    "            embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "\n",
    "            if embedding_tensor.shape == (1, 256, 64, 64):\n",
    "                 pooled_embedding = F.adaptive_avg_pool2d(embedding_tensor, (1, 1)).squeeze()\n",
    "            elif embedding_tensor.shape == (256, 64, 64):\n",
    "                 pooled_embedding = F.adaptive_avg_pool2d(embedding_tensor.unsqueeze(0), (1, 1)).squeeze()\n",
    "            elif embedding_tensor.shape == (256,):\n",
    "                 pooled_embedding = embedding_tensor\n",
    "            else:\n",
    "                 raise ValueError(f\"Unexpected embedding shape {embedding_tensor.shape} for {slice_path}\")\n",
    "\n",
    "            if pooled_embedding.shape[0] != 256:\n",
    "                 raise ValueError(f\"Pooled embedding shape is not 256 for {slice_path}: {pooled_embedding.shape}\")\n",
    "\n",
    "            age = self.meta_dict[original_filename]\n",
    "            age_tensor = torch.tensor(age, dtype=torch.float32)\n",
    "\n",
    "            # Return subject_id along with features and age\n",
    "            return pooled_embedding, age_tensor, subject_id\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing file {slice_path}: {e}\")\n",
    "            raise e\n",
    "\n",
    "# --- Evaluation Function (Modified to return predictions and IDs) ---\n",
    "@torch.no_grad()\n",
    "def predict_on_test_set(model, loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_ages = []\n",
    "    all_subject_ids = []\n",
    "\n",
    "    for features, ages, subject_ids in loader:\n",
    "        features = features.to(device)\n",
    "        # ages = ages.to(device) # No need to move ages if only used on CPU later\n",
    "\n",
    "        predictions = model(features)\n",
    "\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_ages.extend(ages.cpu().numpy())\n",
    "        all_subject_ids.extend(subject_ids) # subject_ids are already strings\n",
    "\n",
    "    return np.array(all_predictions), np.array(all_ages), all_subject_ids\n",
    "\n",
    "\n",
    "# --- Main Testing Logic ---\n",
    "print(f\"Starting testing on device: {DEVICE}\")\n",
    "print(f\"Loading models from: {MODELS_OUTPUT_DIR}\")\n",
    "print(f\"Testing on slices: {TARGET_CORONAL_INDICES}\")\n",
    "\n",
    "individual_model_maes = {}\n",
    "# Use defaultdict to easily append predictions per subject\n",
    "# Stores {subject_id: [pred_slice_1, pred_slice_2, ...]}\n",
    "all_slice_predictions = defaultdict(list)\n",
    "# Stores {subject_id: ground_truth_age} - only need to store once\n",
    "ground_truth_ages = {}\n",
    "\n",
    "for slice_idx in TARGET_CORONAL_INDICES:\n",
    "    print(f\"\\n--- Testing Slice: {slice_idx} ---\")\n",
    "    model_path = MODELS_OUTPUT_DIR / f\"best_model_slice_{slice_idx}.pth\"\n",
    "\n",
    "    if not model_path.exists():\n",
    "        print(f\"Warning: Model file not found for slice {slice_idx} at {model_path}. Skipping.\")\n",
    "        individual_model_maes[slice_idx] = np.nan\n",
    "        continue\n",
    "\n",
    "    # --- Load Data for the current slice ---\n",
    "    try:\n",
    "        test_dataset = SliceAgePredictionDataset(FEATURE_ROOT, CSV_PATH, 'test', slice_idx)\n",
    "        # Important: Use shuffle=False for testing to keep order consistent if needed,\n",
    "        # and to correctly map predictions back to subjects.\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                 num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    except (FileNotFoundError, ValueError, RuntimeError) as e:\n",
    "        print(f\"Error initializing dataset/loader for test split, slice {slice_idx}: {e}\")\n",
    "        print(f\"Skipping testing for slice {slice_idx}.\")\n",
    "        individual_model_maes[slice_idx] = np.nan\n",
    "        continue\n",
    "\n",
    "    # --- Load Model ---\n",
    "    model = AgeMLPWithAttentionBN().to(DEVICE)\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        print(f\"Loaded model weights from {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model weights for slice {slice_idx}: {e}\")\n",
    "        individual_model_maes[slice_idx] = np.nan\n",
    "        del model, test_dataset, test_loader\n",
    "        gc.collect()\n",
    "        if DEVICE.startswith('cuda'): torch.cuda.empty_cache()\n",
    "        continue\n",
    "\n",
    "    # --- Get Predictions ---\n",
    "    predictions, ages, subject_ids = predict_on_test_set(model, test_loader, DEVICE)\n",
    "\n",
    "    # --- Calculate and Store Individual MAE ---\n",
    "    if len(predictions) > 0:\n",
    "        mae = np.mean(np.abs(predictions - ages))\n",
    "        individual_model_maes[slice_idx] = mae\n",
    "        print(f\"Slice {slice_idx} Test MAE: {mae:.3f}\")\n",
    "\n",
    "        # --- Store predictions and ground truth for averaging ---\n",
    "        for i, subj_id in enumerate(subject_ids):\n",
    "            all_slice_predictions[subj_id].append(predictions[i])\n",
    "            # Store ground truth age only once per subject\n",
    "            if subj_id not in ground_truth_ages:\n",
    "                ground_truth_ages[subj_id] = ages[i]\n",
    "    else:\n",
    "        print(f\"No predictions generated for slice {slice_idx}.\")\n",
    "        individual_model_maes[slice_idx] = np.nan\n",
    "\n",
    "\n",
    "    # Clean up GPU memory\n",
    "    del model, test_dataset, test_loader, predictions, ages, subject_ids\n",
    "    gc.collect()\n",
    "    if DEVICE.startswith('cuda'):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --- Calculate MAE for Averaged Predictions ---\n",
    "print(\"\\n--- Calculating MAE for Averaged Predictions ---\")\n",
    "average_predictions = []\n",
    "corresponding_ground_truths = []\n",
    "subjects_with_complete_preds = 0\n",
    "\n",
    "# Check subjects that have predictions for *all* target slices\n",
    "num_target_slices = len(TARGET_CORONAL_INDICES)\n",
    "subject_ids_in_order = sorted(all_slice_predictions.keys()) # Ensure consistent order\n",
    "\n",
    "for subj_id in subject_ids_in_order:\n",
    "    preds = all_slice_predictions[subj_id]\n",
    "    if len(preds) == num_target_slices: # Ensure we have a prediction from each slice model\n",
    "        avg_pred = np.mean(preds)\n",
    "        average_predictions.append(avg_pred)\n",
    "        corresponding_ground_truths.append(ground_truth_ages[subj_id])\n",
    "        subjects_with_complete_preds += 1\n",
    "    else:\n",
    "        print(f\"Warning: Subject {subj_id} has {len(preds)} predictions, expected {num_target_slices}. Skipping for average MAE calculation.\")\n",
    "\n",
    "if subjects_with_complete_preds > 0:\n",
    "    average_predictions = np.array(average_predictions)\n",
    "    corresponding_ground_truths = np.array(corresponding_ground_truths)\n",
    "    average_mae = np.mean(np.abs(average_predictions - corresponding_ground_truths))\n",
    "    print(f\"\\nNumber of subjects with complete predictions across all {num_target_slices} slices: {subjects_with_complete_preds}\")\n",
    "    print(f\"MAE using averaged predictions: {average_mae:.3f}\")\n",
    "else:\n",
    "    print(\"\\nCould not calculate average MAE: No subjects had predictions for all required slices.\")\n",
    "\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n--- Individual Slice Model Test MAEs ---\")\n",
    "for slice_idx in TARGET_CORONAL_INDICES:\n",
    "    mae = individual_model_maes.get(slice_idx, 'N/A')\n",
    "    if isinstance(mae, float) and not np.isnan(mae):\n",
    "        print(f\"Slice {slice_idx}: {mae:.3f}\")\n",
    "    else:\n",
    "        print(f\"Slice {slice_idx}: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e5dca",
   "metadata": {},
   "source": [
    "Early fusion with concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ed9f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting concatenated feature MLP training...\n",
      "Target Slices: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]\n",
      "Concatenated Feature Dim: 2816\n",
      "Using device: cuda:1\n",
      "Feature Root: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_coronal_120_130\n",
      "CSV Path: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Best model will be saved to: concatenated_mlp_model.pth\n",
      "Hyperparameters: Epochs=500, LR=0.0001, Batch=32, Dropout=0.4, ES_Patience=50\n",
      "Setting up datasets...\n",
      "\n",
      "[Dataset Init] Split: train, Concatenating Slices: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]\n",
      "Loading features from base: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_coronal_120_130\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Scanning 2275 potential subject directories...\n",
      "Warning: 1 subject directories did not have corresponding metadata.\n",
      "Found 2274 valid subjects for split train.\n",
      "\n",
      "[Dataset Init] Split: validation, Concatenating Slices: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]\n",
      "Loading features from base: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_coronal_120_130\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Scanning 280 potential subject directories...\n",
      "Found 280 valid subjects for split validation.\n",
      "Setting up dataloaders...\n",
      "Initializing model, optimizer, scheduler...\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/500 | Train Loss: 53.7838 | Val Loss: 53.3006 | Val MAE: 53.301\n",
      "  -> New best Val MAE: 53.301. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 2/500 | Train Loss: 53.0572 | Val Loss: 52.2549 | Val MAE: 52.255\n",
      "  -> New best Val MAE: 52.255. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 3/500 | Train Loss: 52.2527 | Val Loss: 51.8032 | Val MAE: 51.803\n",
      "  -> New best Val MAE: 51.803. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 4/500 | Train Loss: 51.4861 | Val Loss: 51.3933 | Val MAE: 51.393\n",
      "  -> New best Val MAE: 51.393. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 5/500 | Train Loss: 50.6354 | Val Loss: 50.6648 | Val MAE: 50.665\n",
      "  -> New best Val MAE: 50.665. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 6/500 | Train Loss: 49.7637 | Val Loss: 50.0652 | Val MAE: 50.065\n",
      "  -> New best Val MAE: 50.065. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 7/500 | Train Loss: 48.8683 | Val Loss: 49.1659 | Val MAE: 49.166\n",
      "  -> New best Val MAE: 49.166. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 8/500 | Train Loss: 47.9280 | Val Loss: 48.4112 | Val MAE: 48.411\n",
      "  -> New best Val MAE: 48.411. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 9/500 | Train Loss: 46.9410 | Val Loss: 47.6199 | Val MAE: 47.620\n",
      "  -> New best Val MAE: 47.620. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 10/500 | Train Loss: 45.8577 | Val Loss: 46.3211 | Val MAE: 46.321\n",
      "  -> New best Val MAE: 46.321. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 11/500 | Train Loss: 44.7220 | Val Loss: 45.0600 | Val MAE: 45.060\n",
      "  -> New best Val MAE: 45.060. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 12/500 | Train Loss: 43.5043 | Val Loss: 43.8231 | Val MAE: 43.823\n",
      "  -> New best Val MAE: 43.823. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 13/500 | Train Loss: 42.2147 | Val Loss: 42.3196 | Val MAE: 42.320\n",
      "  -> New best Val MAE: 42.320. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 14/500 | Train Loss: 40.8367 | Val Loss: 40.8887 | Val MAE: 40.889\n",
      "  -> New best Val MAE: 40.889. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 15/500 | Train Loss: 39.3452 | Val Loss: 39.2540 | Val MAE: 39.254\n",
      "  -> New best Val MAE: 39.254. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 16/500 | Train Loss: 37.7558 | Val Loss: 37.6016 | Val MAE: 37.602\n",
      "  -> New best Val MAE: 37.602. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 17/500 | Train Loss: 36.0434 | Val Loss: 35.8283 | Val MAE: 35.828\n",
      "  -> New best Val MAE: 35.828. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 18/500 | Train Loss: 34.3389 | Val Loss: 36.3079 | Val MAE: 36.308\n",
      "Epoch 19/500 | Train Loss: 32.5605 | Val Loss: 32.4490 | Val MAE: 32.449\n",
      "  -> New best Val MAE: 32.449. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 20/500 | Train Loss: 30.8246 | Val Loss: 31.5816 | Val MAE: 31.582\n",
      "  -> New best Val MAE: 31.582. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 21/500 | Train Loss: 29.1430 | Val Loss: 29.1007 | Val MAE: 29.101\n",
      "  -> New best Val MAE: 29.101. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 22/500 | Train Loss: 27.3851 | Val Loss: 26.7088 | Val MAE: 26.709\n",
      "  -> New best Val MAE: 26.709. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 23/500 | Train Loss: 25.7483 | Val Loss: 27.7346 | Val MAE: 27.735\n",
      "Epoch 24/500 | Train Loss: 24.0131 | Val Loss: 25.0979 | Val MAE: 25.098\n",
      "  -> New best Val MAE: 25.098. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 25/500 | Train Loss: 22.4190 | Val Loss: 20.8788 | Val MAE: 20.879\n",
      "  -> New best Val MAE: 20.879. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 26/500 | Train Loss: 20.9189 | Val Loss: 20.9960 | Val MAE: 20.996\n",
      "Epoch 27/500 | Train Loss: 19.3025 | Val Loss: 25.2468 | Val MAE: 25.247\n",
      "Epoch 28/500 | Train Loss: 17.7836 | Val Loss: 22.3607 | Val MAE: 22.361\n",
      "Epoch 29/500 | Train Loss: 16.1203 | Val Loss: 17.7416 | Val MAE: 17.742\n",
      "  -> New best Val MAE: 17.742. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 30/500 | Train Loss: 15.0089 | Val Loss: 23.7496 | Val MAE: 23.750\n",
      "Epoch 31/500 | Train Loss: 13.8601 | Val Loss: 17.4015 | Val MAE: 17.402\n",
      "  -> New best Val MAE: 17.402. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 32/500 | Train Loss: 12.8321 | Val Loss: 16.6708 | Val MAE: 16.671\n",
      "  -> New best Val MAE: 16.671. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 33/500 | Train Loss: 11.5912 | Val Loss: 13.4563 | Val MAE: 13.456\n",
      "  -> New best Val MAE: 13.456. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 34/500 | Train Loss: 10.9535 | Val Loss: 14.3561 | Val MAE: 14.356\n",
      "Epoch 35/500 | Train Loss: 10.5084 | Val Loss: 16.9252 | Val MAE: 16.925\n",
      "Epoch 36/500 | Train Loss: 9.6976 | Val Loss: 8.3582 | Val MAE: 8.358\n",
      "  -> New best Val MAE: 8.358. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 37/500 | Train Loss: 9.1838 | Val Loss: 9.6895 | Val MAE: 9.690\n",
      "Epoch 38/500 | Train Loss: 8.9971 | Val Loss: 10.2367 | Val MAE: 10.237\n",
      "Epoch 39/500 | Train Loss: 8.9487 | Val Loss: 9.0400 | Val MAE: 9.040\n",
      "Epoch 40/500 | Train Loss: 8.4888 | Val Loss: 7.7797 | Val MAE: 7.780\n",
      "  -> New best Val MAE: 7.780. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 41/500 | Train Loss: 8.3585 | Val Loss: 8.9142 | Val MAE: 8.914\n",
      "Epoch 42/500 | Train Loss: 8.1101 | Val Loss: 9.8096 | Val MAE: 9.810\n",
      "Epoch 43/500 | Train Loss: 7.9648 | Val Loss: 7.6687 | Val MAE: 7.669\n",
      "  -> New best Val MAE: 7.669. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 44/500 | Train Loss: 8.0966 | Val Loss: 10.4155 | Val MAE: 10.416\n",
      "Epoch 45/500 | Train Loss: 7.6780 | Val Loss: 7.4206 | Val MAE: 7.421\n",
      "  -> New best Val MAE: 7.421. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 46/500 | Train Loss: 7.7309 | Val Loss: 7.8887 | Val MAE: 7.889\n",
      "Epoch 47/500 | Train Loss: 7.6252 | Val Loss: 10.9343 | Val MAE: 10.934\n",
      "Epoch 48/500 | Train Loss: 7.5723 | Val Loss: 8.9692 | Val MAE: 8.969\n",
      "Epoch 49/500 | Train Loss: 7.4600 | Val Loss: 8.9537 | Val MAE: 8.954\n",
      "Epoch 50/500 | Train Loss: 7.2911 | Val Loss: 9.1846 | Val MAE: 9.185\n",
      "Epoch 51/500 | Train Loss: 7.5236 | Val Loss: 7.5824 | Val MAE: 7.582\n",
      "Epoch 52/500 | Train Loss: 7.2794 | Val Loss: 8.4681 | Val MAE: 8.468\n",
      "Epoch 53/500 | Train Loss: 7.3183 | Val Loss: 7.5382 | Val MAE: 7.538\n",
      "Epoch 54/500 | Train Loss: 7.2301 | Val Loss: 7.1063 | Val MAE: 7.106\n",
      "  -> New best Val MAE: 7.106. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 55/500 | Train Loss: 7.3262 | Val Loss: 9.0769 | Val MAE: 9.077\n",
      "Epoch 56/500 | Train Loss: 7.2158 | Val Loss: 7.2990 | Val MAE: 7.299\n",
      "Epoch 57/500 | Train Loss: 7.2517 | Val Loss: 7.5181 | Val MAE: 7.518\n",
      "Epoch 58/500 | Train Loss: 7.3917 | Val Loss: 7.2832 | Val MAE: 7.283\n",
      "Epoch 59/500 | Train Loss: 6.8995 | Val Loss: 7.2473 | Val MAE: 7.247\n",
      "Epoch 60/500 | Train Loss: 7.1077 | Val Loss: 7.2763 | Val MAE: 7.276\n",
      "Epoch 61/500 | Train Loss: 6.9581 | Val Loss: 7.4587 | Val MAE: 7.459\n",
      "Epoch 62/500 | Train Loss: 6.6842 | Val Loss: 8.3829 | Val MAE: 8.383\n",
      "Epoch 63/500 | Train Loss: 6.9696 | Val Loss: 7.2561 | Val MAE: 7.256\n",
      "Epoch 64/500 | Train Loss: 6.7811 | Val Loss: 7.8769 | Val MAE: 7.877\n",
      "Epoch 65/500 | Train Loss: 6.8160 | Val Loss: 10.8821 | Val MAE: 10.882\n",
      "Epoch 66/500 | Train Loss: 6.6273 | Val Loss: 7.9705 | Val MAE: 7.971\n",
      "Epoch 67/500 | Train Loss: 7.1266 | Val Loss: 9.8957 | Val MAE: 9.896\n",
      "Epoch 68/500 | Train Loss: 6.8521 | Val Loss: 10.3246 | Val MAE: 10.325\n",
      "Epoch 69/500 | Train Loss: 6.6511 | Val Loss: 9.8059 | Val MAE: 9.806\n",
      "Epoch 70/500 | Train Loss: 6.6225 | Val Loss: 7.4384 | Val MAE: 7.438\n",
      "Epoch 71/500 | Train Loss: 6.5844 | Val Loss: 7.1872 | Val MAE: 7.187\n",
      "Epoch 72/500 | Train Loss: 6.5134 | Val Loss: 7.2396 | Val MAE: 7.240\n",
      "Epoch 73/500 | Train Loss: 6.4797 | Val Loss: 11.5262 | Val MAE: 11.526\n",
      "Epoch 74/500 | Train Loss: 6.5299 | Val Loss: 7.6299 | Val MAE: 7.630\n",
      "Epoch 75/500 | Train Loss: 6.4577 | Val Loss: 7.9135 | Val MAE: 7.913\n",
      "Epoch 76/500 | Train Loss: 6.6232 | Val Loss: 7.2246 | Val MAE: 7.225\n",
      "Epoch 77/500 | Train Loss: 6.6144 | Val Loss: 11.2217 | Val MAE: 11.222\n",
      "Epoch 78/500 | Train Loss: 6.4332 | Val Loss: 7.5087 | Val MAE: 7.509\n",
      "Epoch 79/500 | Train Loss: 6.3642 | Val Loss: 7.6855 | Val MAE: 7.685\n",
      "Epoch 80/500 | Train Loss: 6.3820 | Val Loss: 8.2492 | Val MAE: 8.249\n",
      "Epoch 81/500 | Train Loss: 6.3112 | Val Loss: 7.5088 | Val MAE: 7.509\n",
      "Epoch 82/500 | Train Loss: 6.4198 | Val Loss: 7.4491 | Val MAE: 7.449\n",
      "Epoch 83/500 | Train Loss: 6.4699 | Val Loss: 7.8558 | Val MAE: 7.856\n",
      "Epoch 84/500 | Train Loss: 6.3237 | Val Loss: 7.8726 | Val MAE: 7.873\n",
      "Epoch 85/500 | Train Loss: 6.0245 | Val Loss: 8.2184 | Val MAE: 8.218\n",
      "Epoch 86/500 | Train Loss: 6.5777 | Val Loss: 9.6613 | Val MAE: 9.661\n",
      "Epoch 87/500 | Train Loss: 6.0527 | Val Loss: 7.1354 | Val MAE: 7.135\n",
      "Epoch 88/500 | Train Loss: 6.2459 | Val Loss: 8.1137 | Val MAE: 8.114\n",
      "Epoch 89/500 | Train Loss: 6.3690 | Val Loss: 7.8927 | Val MAE: 7.893\n",
      "Epoch 90/500 | Train Loss: 6.1076 | Val Loss: 6.6924 | Val MAE: 6.692\n",
      "  -> New best Val MAE: 6.692. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 91/500 | Train Loss: 5.9738 | Val Loss: 7.1867 | Val MAE: 7.187\n",
      "Epoch 92/500 | Train Loss: 6.1503 | Val Loss: 7.4407 | Val MAE: 7.441\n",
      "Epoch 93/500 | Train Loss: 6.0053 | Val Loss: 7.3047 | Val MAE: 7.305\n",
      "Epoch 94/500 | Train Loss: 6.2327 | Val Loss: 7.3248 | Val MAE: 7.325\n",
      "Epoch 95/500 | Train Loss: 6.0693 | Val Loss: 7.7380 | Val MAE: 7.738\n",
      "Epoch 96/500 | Train Loss: 5.9795 | Val Loss: 8.4242 | Val MAE: 8.424\n",
      "Epoch 97/500 | Train Loss: 6.0451 | Val Loss: 7.1101 | Val MAE: 7.110\n",
      "Epoch 98/500 | Train Loss: 6.2622 | Val Loss: 7.2753 | Val MAE: 7.275\n",
      "Epoch 99/500 | Train Loss: 6.1827 | Val Loss: 7.2881 | Val MAE: 7.288\n",
      "Epoch 100/500 | Train Loss: 6.3275 | Val Loss: 9.0345 | Val MAE: 9.034\n",
      "Epoch 101/500 | Train Loss: 6.0953 | Val Loss: 7.3919 | Val MAE: 7.392\n",
      "Epoch 102/500 | Train Loss: 5.8722 | Val Loss: 7.8245 | Val MAE: 7.825\n",
      "Epoch 103/500 | Train Loss: 5.9200 | Val Loss: 7.0101 | Val MAE: 7.010\n",
      "Epoch 104/500 | Train Loss: 5.8573 | Val Loss: 7.8491 | Val MAE: 7.849\n",
      "Epoch 105/500 | Train Loss: 6.2115 | Val Loss: 7.2201 | Val MAE: 7.220\n",
      "Epoch 106/500 | Train Loss: 6.0777 | Val Loss: 6.7790 | Val MAE: 6.779\n",
      "Epoch 107/500 | Train Loss: 5.8420 | Val Loss: 6.9600 | Val MAE: 6.960\n",
      "Epoch 108/500 | Train Loss: 5.8410 | Val Loss: 7.5217 | Val MAE: 7.522\n",
      "Epoch 109/500 | Train Loss: 5.8964 | Val Loss: 7.6458 | Val MAE: 7.646\n",
      "Epoch 110/500 | Train Loss: 5.8679 | Val Loss: 7.1406 | Val MAE: 7.141\n",
      "Epoch 111/500 | Train Loss: 6.0192 | Val Loss: 7.7397 | Val MAE: 7.740\n",
      "Epoch 112/500 | Train Loss: 5.9344 | Val Loss: 7.5857 | Val MAE: 7.586\n",
      "Epoch 113/500 | Train Loss: 6.1031 | Val Loss: 7.0024 | Val MAE: 7.002\n",
      "Epoch 114/500 | Train Loss: 5.9105 | Val Loss: 8.8846 | Val MAE: 8.885\n",
      "Epoch 115/500 | Train Loss: 5.9505 | Val Loss: 6.9299 | Val MAE: 6.930\n",
      "Epoch 116/500 | Train Loss: 5.7477 | Val Loss: 7.2033 | Val MAE: 7.203\n",
      "Epoch 117/500 | Train Loss: 5.7662 | Val Loss: 7.3634 | Val MAE: 7.363\n",
      "Epoch 118/500 | Train Loss: 5.7624 | Val Loss: 7.2403 | Val MAE: 7.240\n",
      "Epoch 119/500 | Train Loss: 5.9865 | Val Loss: 7.6783 | Val MAE: 7.678\n",
      "Epoch 120/500 | Train Loss: 5.9800 | Val Loss: 7.6080 | Val MAE: 7.608\n",
      "Epoch 121/500 | Train Loss: 5.7457 | Val Loss: 6.8297 | Val MAE: 6.830\n",
      "Epoch 122/500 | Train Loss: 5.7276 | Val Loss: 8.1190 | Val MAE: 8.119\n",
      "Epoch 123/500 | Train Loss: 5.9139 | Val Loss: 7.6750 | Val MAE: 7.675\n",
      "Epoch 124/500 | Train Loss: 5.7555 | Val Loss: 6.7661 | Val MAE: 6.766\n",
      "Epoch 125/500 | Train Loss: 5.6343 | Val Loss: 6.9829 | Val MAE: 6.983\n",
      "Epoch 126/500 | Train Loss: 5.8032 | Val Loss: 7.0410 | Val MAE: 7.041\n",
      "Epoch 127/500 | Train Loss: 5.7533 | Val Loss: 8.3382 | Val MAE: 8.338\n",
      "Epoch 128/500 | Train Loss: 5.8051 | Val Loss: 8.3432 | Val MAE: 8.343\n",
      "Epoch 129/500 | Train Loss: 6.0397 | Val Loss: 7.2941 | Val MAE: 7.294\n",
      "Epoch 130/500 | Train Loss: 5.4552 | Val Loss: 7.1306 | Val MAE: 7.131\n",
      "Epoch 131/500 | Train Loss: 5.9656 | Val Loss: 7.0512 | Val MAE: 7.051\n",
      "Epoch 132/500 | Train Loss: 5.6906 | Val Loss: 7.1605 | Val MAE: 7.160\n",
      "Epoch 133/500 | Train Loss: 5.6629 | Val Loss: 6.8947 | Val MAE: 6.895\n",
      "Epoch 134/500 | Train Loss: 5.5556 | Val Loss: 10.6318 | Val MAE: 10.632\n",
      "Epoch 135/500 | Train Loss: 5.7294 | Val Loss: 8.4321 | Val MAE: 8.432\n",
      "Epoch 136/500 | Train Loss: 5.7804 | Val Loss: 7.4188 | Val MAE: 7.419\n",
      "Epoch 137/500 | Train Loss: 5.5255 | Val Loss: 6.8852 | Val MAE: 6.885\n",
      "Epoch 138/500 | Train Loss: 5.5120 | Val Loss: 6.6180 | Val MAE: 6.618\n",
      "  -> New best Val MAE: 6.618. Saved model to concatenated_mlp_model.pth\n",
      "Epoch 139/500 | Train Loss: 5.9350 | Val Loss: 7.4248 | Val MAE: 7.425\n",
      "Epoch 140/500 | Train Loss: 5.6651 | Val Loss: 7.7127 | Val MAE: 7.713\n",
      "Epoch 141/500 | Train Loss: 5.6424 | Val Loss: 7.0280 | Val MAE: 7.028\n",
      "Epoch 142/500 | Train Loss: 6.1281 | Val Loss: 7.0879 | Val MAE: 7.088\n",
      "Epoch 143/500 | Train Loss: 5.4378 | Val Loss: 8.3992 | Val MAE: 8.399\n",
      "Epoch 144/500 | Train Loss: 5.3523 | Val Loss: 7.8993 | Val MAE: 7.899\n",
      "Epoch 145/500 | Train Loss: 5.7079 | Val Loss: 7.4408 | Val MAE: 7.441\n",
      "Epoch 146/500 | Train Loss: 5.7428 | Val Loss: 7.3632 | Val MAE: 7.363\n",
      "Epoch 147/500 | Train Loss: 5.8289 | Val Loss: 7.8401 | Val MAE: 7.840\n",
      "Epoch 148/500 | Train Loss: 5.5937 | Val Loss: 10.7111 | Val MAE: 10.711\n",
      "Epoch 149/500 | Train Loss: 5.6927 | Val Loss: 7.1414 | Val MAE: 7.141\n",
      "Epoch 150/500 | Train Loss: 5.7933 | Val Loss: 7.2261 | Val MAE: 7.226\n",
      "Epoch 151/500 | Train Loss: 5.7182 | Val Loss: 7.3974 | Val MAE: 7.397\n",
      "Epoch 152/500 | Train Loss: 5.5418 | Val Loss: 7.1189 | Val MAE: 7.119\n",
      "Epoch 153/500 | Train Loss: 5.6843 | Val Loss: 7.1118 | Val MAE: 7.112\n",
      "Epoch 154/500 | Train Loss: 5.4643 | Val Loss: 7.6335 | Val MAE: 7.634\n",
      "Epoch 155/500 | Train Loss: 5.6864 | Val Loss: 7.9694 | Val MAE: 7.969\n",
      "Epoch 156/500 | Train Loss: 5.8251 | Val Loss: 7.0705 | Val MAE: 7.071\n",
      "Epoch 157/500 | Train Loss: 5.6665 | Val Loss: 7.5707 | Val MAE: 7.571\n",
      "Epoch 158/500 | Train Loss: 5.3329 | Val Loss: 6.7253 | Val MAE: 6.725\n",
      "Epoch 159/500 | Train Loss: 5.6247 | Val Loss: 6.8862 | Val MAE: 6.886\n",
      "Epoch 160/500 | Train Loss: 5.6826 | Val Loss: 6.8235 | Val MAE: 6.823\n",
      "Epoch 161/500 | Train Loss: 5.4524 | Val Loss: 6.8765 | Val MAE: 6.876\n",
      "Epoch 162/500 | Train Loss: 5.8241 | Val Loss: 6.9014 | Val MAE: 6.901\n",
      "Epoch 163/500 | Train Loss: 5.4756 | Val Loss: 6.8278 | Val MAE: 6.828\n",
      "Epoch 164/500 | Train Loss: 5.3459 | Val Loss: 7.1686 | Val MAE: 7.169\n",
      "Epoch 165/500 | Train Loss: 5.1008 | Val Loss: 6.6312 | Val MAE: 6.631\n",
      "Epoch 166/500 | Train Loss: 5.5866 | Val Loss: 7.0931 | Val MAE: 7.093\n",
      "Epoch 167/500 | Train Loss: 5.4779 | Val Loss: 6.7170 | Val MAE: 6.717\n",
      "Epoch 168/500 | Train Loss: 5.4410 | Val Loss: 7.1573 | Val MAE: 7.157\n",
      "Epoch 169/500 | Train Loss: 5.7128 | Val Loss: 8.1003 | Val MAE: 8.100\n",
      "Epoch 170/500 | Train Loss: 5.6095 | Val Loss: 6.8085 | Val MAE: 6.809\n",
      "Epoch 171/500 | Train Loss: 5.5750 | Val Loss: 6.6561 | Val MAE: 6.656\n",
      "Epoch 172/500 | Train Loss: 5.5474 | Val Loss: 9.1436 | Val MAE: 9.144\n",
      "Epoch 173/500 | Train Loss: 5.6223 | Val Loss: 7.0357 | Val MAE: 7.036\n",
      "Epoch 174/500 | Train Loss: 5.5277 | Val Loss: 7.8578 | Val MAE: 7.858\n",
      "Epoch 175/500 | Train Loss: 5.3913 | Val Loss: 6.9016 | Val MAE: 6.902\n",
      "Epoch 176/500 | Train Loss: 5.7125 | Val Loss: 7.3864 | Val MAE: 7.386\n",
      "Epoch 177/500 | Train Loss: 5.3448 | Val Loss: 7.1025 | Val MAE: 7.102\n",
      "Epoch 178/500 | Train Loss: 5.6829 | Val Loss: 6.8961 | Val MAE: 6.896\n",
      "Epoch 179/500 | Train Loss: 5.2959 | Val Loss: 11.0927 | Val MAE: 11.093\n",
      "Epoch 180/500 | Train Loss: 5.4946 | Val Loss: 7.1873 | Val MAE: 7.187\n",
      "Epoch 181/500 | Train Loss: 5.4997 | Val Loss: 6.9452 | Val MAE: 6.945\n",
      "Epoch 182/500 | Train Loss: 5.3106 | Val Loss: 8.8322 | Val MAE: 8.832\n",
      "Epoch 183/500 | Train Loss: 5.3688 | Val Loss: 8.5414 | Val MAE: 8.541\n",
      "Epoch 184/500 | Train Loss: 5.4289 | Val Loss: 7.3366 | Val MAE: 7.337\n",
      "Epoch 185/500 | Train Loss: 5.6373 | Val Loss: 7.8594 | Val MAE: 7.859\n",
      "Epoch 186/500 | Train Loss: 5.3081 | Val Loss: 7.3388 | Val MAE: 7.339\n",
      "Epoch 187/500 | Train Loss: 5.4435 | Val Loss: 7.5709 | Val MAE: 7.571\n",
      "Epoch 188/500 | Train Loss: 5.5335 | Val Loss: 6.6711 | Val MAE: 6.671\n",
      "\n",
      "Early stopping triggered after 50 epochs without improvement.\n",
      "\n",
      "--- Training Finished ---\n",
      "Best Validation MAE: 6.618 achieved at epoch 138\n",
      "Best model saved to concatenated_mlp_model.pth\n",
      "\n",
      "Concatenated MLP training script finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "FEATURE_ROOT = Path(\"/data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_coronal_120_130\")\n",
    "CSV_PATH = Path(\"/data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\")\n",
    "MODEL_SAVE_PATH = Path(\"./concatenated_mlp_model.pth\") # Path to save the best model\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "SPLITS = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "# Define the target coronal slices (Must match preprocessing)\n",
    "CORONAL_SLICE_CENTER = 125\n",
    "CORONAL_SLICE_RANGE = 5\n",
    "TARGET_CORONAL_INDICES = list(range(CORONAL_SLICE_CENTER - CORONAL_SLICE_RANGE,\n",
    "                                    CORONAL_SLICE_CENTER + CORONAL_SLICE_RANGE + 1)) # e.g., 120 to 130\n",
    "NUM_SLICES = len(TARGET_CORONAL_INDICES)\n",
    "FEATURE_DIM_PER_SLICE = 256 # After Global Average Pooling\n",
    "CONCAT_FEATURE_DIM = NUM_SLICES * FEATURE_DIM_PER_SLICE # e.g., 11 * 256 = 2816\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "EPOCHS = 500\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.4 # Might need adjustment for the larger input dim\n",
    "SCHEDULER_PATIENCE_PERCENT = 0.10\n",
    "EARLY_STOPPING_PATIENCE = 50\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# --- Dataset Definition (Loads and Concatenates Features) ---\n",
    "class ConcatenatedSliceDataset(Dataset):\n",
    "    def __init__(self, feature_root, csv_path, split, slice_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_root (Path): Base directory containing split folders.\n",
    "            csv_path (Path): Path to the CSV file with metadata.\n",
    "            split (str): The dataset split ('train', 'validation', or 'test').\n",
    "            slice_indices (list[int]): List of slice indices to load and concatenate.\n",
    "        \"\"\"\n",
    "        self.feature_root = Path(feature_root)\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.split = split\n",
    "        self.slice_indices = sorted(slice_indices) # Ensure consistent order\n",
    "        self.num_slices = len(self.slice_indices)\n",
    "        self.split_dir = self.feature_root / self.split\n",
    "\n",
    "        print(f\"\\n[Dataset Init] Split: {self.split}, Concatenating Slices: {self.slice_indices}\")\n",
    "        print(f\"Loading features from base: {self.feature_root}\")\n",
    "        print(f\"Loading metadata from: {self.csv_path}\")\n",
    "\n",
    "        if not self.split_dir.is_dir():\n",
    "            raise FileNotFoundError(f\"Split directory not found: {self.split_dir}\")\n",
    "        if not self.csv_path.is_file():\n",
    "            raise FileNotFoundError(f\"CSV file not found: {self.csv_path}\")\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(self.csv_path)\n",
    "            self.meta_dict = df.set_index('filename')['age'].to_dict()\n",
    "            self.subject_id_to_filename = {\n",
    "                fname.replace(\".nii.gz\", \"\").replace(\".nii\", \"\"): fname\n",
    "                for fname in self.meta_dict.keys()\n",
    "            }\n",
    "            print(f\"Loaded metadata for {len(self.meta_dict)} subjects from CSV.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading or processing CSV {self.csv_path}: {e}\")\n",
    "\n",
    "        # Find subject directories that have metadata and *all* required slice files\n",
    "        all_subject_dirs = [d for d in self.split_dir.iterdir() if d.is_dir()]\n",
    "        self.valid_subject_dirs = []\n",
    "        missing_meta_count = 0\n",
    "        incomplete_slice_count = 0\n",
    "\n",
    "        print(f\"Scanning {len(all_subject_dirs)} potential subject directories...\")\n",
    "\n",
    "        for subject_dir in all_subject_dirs:\n",
    "            subject_id = subject_dir.name\n",
    "            original_filename = self.subject_id_to_filename.get(subject_id)\n",
    "\n",
    "            if original_filename and original_filename in self.meta_dict:\n",
    "                # Check if all required slice files exist for this subject\n",
    "                all_slices_present = True\n",
    "                for slice_idx in self.slice_indices:\n",
    "                    slice_filename = f\"slice_{slice_idx}.npy\"\n",
    "                    expected_slice_path = subject_dir / slice_filename\n",
    "                    if not expected_slice_path.is_file():\n",
    "                        all_slices_present = False\n",
    "                        break # No need to check further slices for this subject\n",
    "\n",
    "                if all_slices_present:\n",
    "                    self.valid_subject_dirs.append(subject_dir)\n",
    "                else:\n",
    "                    incomplete_slice_count += 1\n",
    "            else:\n",
    "                missing_meta_count += 1\n",
    "\n",
    "        if missing_meta_count > 0:\n",
    "            print(f\"Warning: {missing_meta_count} subject directories did not have corresponding metadata.\")\n",
    "        if incomplete_slice_count > 0:\n",
    "            print(f\"Warning: {incomplete_slice_count} subjects with metadata were missing one or more required slices.\")\n",
    "\n",
    "        if not self.valid_subject_dirs:\n",
    "             raise RuntimeError(f\"No subjects found in {self.split_dir} with metadata and all required slices {self.slice_indices}.\")\n",
    "\n",
    "        print(f\"Found {len(self.valid_subject_dirs)} valid subjects for split {self.split}.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_subject_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_dir = self.valid_subject_dirs[idx]\n",
    "        subject_id = subject_dir.name\n",
    "        original_filename = self.subject_id_to_filename.get(subject_id)\n",
    "\n",
    "        if not original_filename:\n",
    "             raise ValueError(f\"Could not find original filename for subject ID {subject_id}\")\n",
    "\n",
    "        concatenated_features = []\n",
    "        try:\n",
    "            for slice_idx in self.slice_indices:\n",
    "                slice_filename = f\"slice_{slice_idx}.npy\"\n",
    "                slice_path = subject_dir / slice_filename\n",
    "                embedding = np.load(slice_path)\n",
    "                embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "\n",
    "                # Apply Global Average Pooling (GAP)\n",
    "                if embedding_tensor.shape == (1, 256, 64, 64):\n",
    "                    pooled_embedding = F.adaptive_avg_pool2d(embedding_tensor, (1, 1)).squeeze() # -> (256,)\n",
    "                elif embedding_tensor.shape == (256, 64, 64):\n",
    "                    pooled_embedding = F.adaptive_avg_pool2d(embedding_tensor.unsqueeze(0), (1, 1)).squeeze()\n",
    "                elif embedding_tensor.shape == (256,):\n",
    "                    pooled_embedding = embedding_tensor\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected embedding shape {embedding_tensor.shape} for {slice_path}\")\n",
    "\n",
    "                if pooled_embedding.shape[0] != FEATURE_DIM_PER_SLICE:\n",
    "                    raise ValueError(f\"Pooled embedding shape is not {FEATURE_DIM_PER_SLICE} for {slice_path}: {pooled_embedding.shape}\")\n",
    "\n",
    "                concatenated_features.append(pooled_embedding)\n",
    "\n",
    "            # Concatenate features from all slices for this subject\n",
    "            final_feature_vector = torch.cat(concatenated_features, dim=0) # Shape: (num_slices * 256,)\n",
    "\n",
    "            # Get the age\n",
    "            age = self.meta_dict[original_filename]\n",
    "            age_tensor = torch.tensor(age, dtype=torch.float32)\n",
    "\n",
    "            return final_feature_vector, age_tensor\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing slices for subject {subject_dir}: {e}\")\n",
    "            raise e\n",
    "\n",
    "# --- Model Definition (Takes concatenated features as input) ---\n",
    "class ConcatenatedAgeMLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONCAT_FEATURE_DIM, hidden_dim1=1024, hidden_dim2=512, hidden_dim3=256, dropout_rate=DROPOUT_RATE):\n",
    "        \"\"\"\n",
    "        MLP for age prediction using concatenated features from multiple slices.\n",
    "        Args:\n",
    "            input_dim (int): Dimension of the concatenated input feature vector (num_slices * feature_dim_per_slice).\n",
    "            hidden_dim1/2/3 (int): Sizes of the hidden layers.\n",
    "            dropout_rate (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(ConcatenatedAgeMLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.BatchNorm1d(hidden_dim1), # Batch Norm is often helpful\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            # Layer 2\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.BatchNorm1d(hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            # Layer 3\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.BatchNorm1d(hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            # Output Layer\n",
    "            nn.Linear(hidden_dim3, 1) # Output is a single value (age)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, input_dim)\n",
    "        output = self.mlp(x)\n",
    "        return output.squeeze(-1) # (batch_size,)\n",
    "\n",
    "\n",
    "# --- Training and Evaluation Functions (Unchanged from previous script) ---\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "    for features, ages in loader:\n",
    "        features, ages = features.to(device), ages.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, ages)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * features.size(0)\n",
    "        num_samples += features.size(0)\n",
    "    return total_loss / num_samples\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    num_samples = 0\n",
    "    for features, ages in loader:\n",
    "        features, ages = features.to(device), ages.to(device)\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, ages) # Use the same criterion (e.g., L1Loss)\n",
    "        total_loss += loss.item() * features.size(0)\n",
    "        mae = F.l1_loss(predictions, ages, reduction='sum') # Sum MAE for batch\n",
    "        total_mae += mae.item()\n",
    "        num_samples += features.size(0)\n",
    "\n",
    "    avg_loss = total_loss / num_samples\n",
    "    avg_mae = total_mae / num_samples\n",
    "    return avg_loss, avg_mae\n",
    "\n",
    "# --- Main Training Script ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting concatenated feature MLP training...\")\n",
    "    print(f\"Target Slices: {TARGET_CORONAL_INDICES}\")\n",
    "    print(f\"Concatenated Feature Dim: {CONCAT_FEATURE_DIM}\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"Feature Root: {FEATURE_ROOT}\")\n",
    "    print(f\"CSV Path: {CSV_PATH}\")\n",
    "    print(f\"Best model will be saved to: {MODEL_SAVE_PATH}\")\n",
    "    print(f\"Hyperparameters: Epochs={EPOCHS}, LR={LEARNING_RATE}, Batch={BATCH_SIZE}, Dropout={DROPOUT_RATE}, ES_Patience={EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "    # --- Setup Datasets and DataLoaders ---\n",
    "    try:\n",
    "        print(\"Setting up datasets...\")\n",
    "        train_dataset = ConcatenatedSliceDataset(FEATURE_ROOT, CSV_PATH, 'train', TARGET_CORONAL_INDICES)\n",
    "        val_dataset = ConcatenatedSliceDataset(FEATURE_ROOT, CSV_PATH, 'validation', TARGET_CORONAL_INDICES)\n",
    "        # Optional: test_dataset = ConcatenatedSliceDataset(FEATURE_ROOT, CSV_PATH, 'test', TARGET_CORONAL_INDICES)\n",
    "\n",
    "        print(\"Setting up dataloaders...\")\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        # Optional: test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    except (FileNotFoundError, ValueError, RuntimeError) as e:\n",
    "        print(f\"Error initializing datasets/loaders: {e}\")\n",
    "        raise SystemExit(\"Dataset/Loader initialization failed.\")\n",
    "\n",
    "    # --- Setup Model, Loss, Optimizer, Scheduler ---\n",
    "    print(\"Initializing model, optimizer, scheduler...\")\n",
    "    model = ConcatenatedAgeMLP(input_dim=CONCAT_FEATURE_DIM, dropout_rate=DROPOUT_RATE).to(DEVICE)\n",
    "    criterion = nn.L1Loss() # MAE Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    scheduler_patience_epochs = max(1, int(EPOCHS * SCHEDULER_PATIENCE_PERCENT))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=scheduler_patience_epochs, factor=0.5, verbose=True)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    best_val_mae = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_epoch = -1\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_mae': []}\n",
    "\n",
    "    print(f\"\\n--- Starting Training ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        val_loss, val_mae = evaluate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_mae'].append(val_mae)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val MAE: {val_mae:.3f}\")\n",
    "\n",
    "        scheduler.step(val_loss) # Step scheduler based on validation loss\n",
    "\n",
    "        # --- Early Stopping & Model Saving ---\n",
    "        if val_mae < best_val_mae:\n",
    "            best_val_mae = val_mae\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"  -> New best Val MAE: {best_val_mae:.3f}. Saved model to {MODEL_SAVE_PATH}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered after {EARLY_STOPPING_PATIENCE} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "        # Optional: Clean up GPU memory periodically\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "             gc.collect()\n",
    "             if DEVICE.startswith('cuda'):\n",
    "                 torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\n--- Training Finished ---\")\n",
    "    if best_epoch != -1:\n",
    "         print(f\"Best Validation MAE: {best_val_mae:.3f} achieved at epoch {best_epoch}\")\n",
    "         print(f\"Best model saved to {MODEL_SAVE_PATH}\")\n",
    "    else:\n",
    "         print(\"No improvement found during training. Model not saved.\")\n",
    "\n",
    "    # --- Optional: Evaluate on Test Set ---\n",
    "    # if 'test_loader' in locals() and MODEL_SAVE_PATH.exists():\n",
    "    #     print(f\"\\n--- Evaluating on Test Set using Best Model ---\")\n",
    "    #     try:\n",
    "    #         model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "    #         test_loss, test_mae = evaluate(model, test_loader, criterion, DEVICE)\n",
    "    #         print(f\"Test Loss: {test_loss:.4f} | Test MAE: {test_mae:.3f}\")\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error during test set evaluation: {e}\")\n",
    "    # else:\n",
    "    #     print(\"\\nSkipping test set evaluation.\")\n",
    "\n",
    "    # You can plot history['train_loss'], history['val_loss'], history['val_mae'] here if needed\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.figure(...) etc.\n",
    "\n",
    "    print(\"\\nConcatenated MLP training script finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d7bf4a",
   "metadata": {},
   "source": [
    "Evaluating early fusion MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86a51bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Dataset Init] Split: test, Concatenating Slices: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]\n",
      "Loading features from base: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_coronal_120_130\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Scanning 296 potential subject directories...\n",
      "Found 296 valid subjects for split test.\n",
      "Test MAE: 6.348\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def test_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "        device (str): Device to run the evaluation on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        float: Mean Absolute Error (MAE) on the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_mae = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    for features, ages in test_loader:\n",
    "        features, ages = features.to(device), ages.to(device)\n",
    "        predictions = model(features)\n",
    "        mae = F.l1_loss(predictions, ages, reduction='sum')  # Sum MAE for the batch\n",
    "        total_mae += mae.item()\n",
    "        num_samples += features.size(0)\n",
    "\n",
    "    avg_mae = total_mae / num_samples\n",
    "    print(f\"Test MAE: {avg_mae:.3f}\")\n",
    "    return avg_mae\n",
    "\n",
    "# Example usage:\n",
    "# Ensure the model is loaded with the best weights\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "test_dataset = ConcatenatedSliceDataset(FEATURE_ROOT, CSV_PATH, 'test', TARGET_CORONAL_INDICES)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_mae = test_model(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb4a3f0",
   "metadata": {},
   "source": [
    "Feature Fusion with Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb4ce12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature fusion (average pooling) MLP training for Coronal slices [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]...\n",
      "Target Slice Identifiers for Fusion: ['120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130']\n",
      "Fused Feature Dim: 256\n",
      "Using device: cuda:1\n",
      "Feature Root: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_coronal_120_130\n",
      "CSV Path: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Best model will be saved to: fused_mlp_model_avg_pool_coronal_120_130.pth\n",
      "Hyperparameters: Epochs=500, LR=0.0001, Batch=32, Dropout=0.3, ES_Patience=40\n",
      "Setting up datasets...\n",
      "\n",
      "[Dataset Init] Split: train, Fusing Coronal Slices: ['120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130']\n",
      "Loading features from base: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_coronal_120_130\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Scanning 2275 potential subject directories...\n",
      "Warning: 1 subject directories did not have corresponding metadata.\n",
      "Found 2274 valid subjects for split train.\n",
      "\n",
      "[Dataset Init] Split: validation, Fusing Coronal Slices: ['120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130']\n",
      "Loading features from base: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_coronal_120_130\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Scanning 280 potential subject directories...\n",
      "Found 280 valid subjects for split validation.\n",
      "\n",
      "[Dataset Init] Split: test, Fusing Coronal Slices: ['120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130']\n",
      "Loading features from base: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_coronal_120_130\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Scanning 296 potential subject directories...\n",
      "Found 296 valid subjects for split test.\n",
      "Setting up dataloaders...\n",
      "Initializing model, optimizer, scheduler...\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 | Train Loss: 54.2875 | Val Loss: 53.6119 | Val MAE: 53.612\n",
      "  -> New best Val MAE: 53.612. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500 | Train Loss: 54.1740 | Val Loss: 53.4688 | Val MAE: 53.469\n",
      "  -> New best Val MAE: 53.469. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/500 | Train Loss: 54.0290 | Val Loss: 53.4572 | Val MAE: 53.457\n",
      "  -> New best Val MAE: 53.457. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500 | Train Loss: 53.9281 | Val Loss: 53.3494 | Val MAE: 53.349\n",
      "  -> New best Val MAE: 53.349. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/500 | Train Loss: 53.8107 | Val Loss: 53.2279 | Val MAE: 53.228\n",
      "  -> New best Val MAE: 53.228. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/500 | Train Loss: 53.6928 | Val Loss: 53.1822 | Val MAE: 53.182\n",
      "  -> New best Val MAE: 53.182. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/500 | Train Loss: 53.5555 | Val Loss: 53.0516 | Val MAE: 53.052\n",
      "  -> New best Val MAE: 53.052. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/500 | Train Loss: 53.4488 | Val Loss: 52.9622 | Val MAE: 52.962\n",
      "  -> New best Val MAE: 52.962. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/500 | Train Loss: 53.3214 | Val Loss: 52.8712 | Val MAE: 52.871\n",
      "  -> New best Val MAE: 52.871. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500 | Train Loss: 53.1788 | Val Loss: 52.7831 | Val MAE: 52.783\n",
      "  -> New best Val MAE: 52.783. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/500 | Train Loss: 53.0538 | Val Loss: 52.6677 | Val MAE: 52.668\n",
      "  -> New best Val MAE: 52.668. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/500 | Train Loss: 52.9132 | Val Loss: 52.5162 | Val MAE: 52.516\n",
      "  -> New best Val MAE: 52.516. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/500 | Train Loss: 52.7215 | Val Loss: 52.4475 | Val MAE: 52.448\n",
      "  -> New best Val MAE: 52.448. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/500 | Train Loss: 52.6025 | Val Loss: 52.3154 | Val MAE: 52.315\n",
      "  -> New best Val MAE: 52.315. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/500 | Train Loss: 52.4676 | Val Loss: 52.1085 | Val MAE: 52.109\n",
      "  -> New best Val MAE: 52.109. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/500 | Train Loss: 52.2945 | Val Loss: 51.9721 | Val MAE: 51.972\n",
      "  -> New best Val MAE: 51.972. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/500 | Train Loss: 52.1428 | Val Loss: 52.0065 | Val MAE: 52.006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/500 | Train Loss: 51.9902 | Val Loss: 51.8506 | Val MAE: 51.851\n",
      "  -> New best Val MAE: 51.851. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/500 | Train Loss: 51.8015 | Val Loss: 51.6434 | Val MAE: 51.643\n",
      "  -> New best Val MAE: 51.643. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500 | Train Loss: 51.6541 | Val Loss: 51.4563 | Val MAE: 51.456\n",
      "  -> New best Val MAE: 51.456. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/500 | Train Loss: 51.4812 | Val Loss: 51.2823 | Val MAE: 51.282\n",
      "  -> New best Val MAE: 51.282. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500 | Train Loss: 51.2659 | Val Loss: 51.1357 | Val MAE: 51.136\n",
      "  -> New best Val MAE: 51.136. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500 | Train Loss: 51.1043 | Val Loss: 51.0856 | Val MAE: 51.086\n",
      "  -> New best Val MAE: 51.086. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500 | Train Loss: 50.9023 | Val Loss: 50.8292 | Val MAE: 50.829\n",
      "  -> New best Val MAE: 50.829. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500 | Train Loss: 50.6954 | Val Loss: 50.6728 | Val MAE: 50.673\n",
      "  -> New best Val MAE: 50.673. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500 | Train Loss: 50.5354 | Val Loss: 50.2349 | Val MAE: 50.235\n",
      "  -> New best Val MAE: 50.235. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500 | Train Loss: 50.3050 | Val Loss: 50.2336 | Val MAE: 50.234\n",
      "  -> New best Val MAE: 50.234. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500 | Train Loss: 50.0587 | Val Loss: 50.0472 | Val MAE: 50.047\n",
      "  -> New best Val MAE: 50.047. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500 | Train Loss: 49.8743 | Val Loss: 49.8691 | Val MAE: 49.869\n",
      "  -> New best Val MAE: 49.869. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500 | Train Loss: 49.6627 | Val Loss: 49.6369 | Val MAE: 49.637\n",
      "  -> New best Val MAE: 49.637. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/500 | Train Loss: 49.4367 | Val Loss: 49.1881 | Val MAE: 49.188\n",
      "  -> New best Val MAE: 49.188. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/500 | Train Loss: 49.2005 | Val Loss: 49.0304 | Val MAE: 49.030\n",
      "  -> New best Val MAE: 49.030. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/500 | Train Loss: 48.9465 | Val Loss: 48.9309 | Val MAE: 48.931\n",
      "  -> New best Val MAE: 48.931. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/500 | Train Loss: 48.6744 | Val Loss: 48.6766 | Val MAE: 48.677\n",
      "  -> New best Val MAE: 48.677. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/500 | Train Loss: 48.4317 | Val Loss: 48.5504 | Val MAE: 48.550\n",
      "  -> New best Val MAE: 48.550. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/500 | Train Loss: 48.1727 | Val Loss: 48.3363 | Val MAE: 48.336\n",
      "  -> New best Val MAE: 48.336. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/500 | Train Loss: 47.8285 | Val Loss: 47.9552 | Val MAE: 47.955\n",
      "  -> New best Val MAE: 47.955. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/500 | Train Loss: 47.6634 | Val Loss: 47.5331 | Val MAE: 47.533\n",
      "  -> New best Val MAE: 47.533. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/500 | Train Loss: 47.3074 | Val Loss: 47.7698 | Val MAE: 47.770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/500 | Train Loss: 47.0557 | Val Loss: 46.8265 | Val MAE: 46.826\n",
      "  -> New best Val MAE: 46.826. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/500 | Train Loss: 46.7474 | Val Loss: 46.5191 | Val MAE: 46.519\n",
      "  -> New best Val MAE: 46.519. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/500 | Train Loss: 46.4259 | Val Loss: 46.6477 | Val MAE: 46.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/500 | Train Loss: 46.1667 | Val Loss: 46.0953 | Val MAE: 46.095\n",
      "  -> New best Val MAE: 46.095. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/500 | Train Loss: 45.8700 | Val Loss: 46.1053 | Val MAE: 46.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/500 | Train Loss: 45.5613 | Val Loss: 45.5622 | Val MAE: 45.562\n",
      "  -> New best Val MAE: 45.562. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/500 | Train Loss: 45.1427 | Val Loss: 45.0556 | Val MAE: 45.056\n",
      "  -> New best Val MAE: 45.056. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500 | Train Loss: 44.8288 | Val Loss: 44.7948 | Val MAE: 44.795\n",
      "  -> New best Val MAE: 44.795. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500 | Train Loss: 44.5475 | Val Loss: 44.4662 | Val MAE: 44.466\n",
      "  -> New best Val MAE: 44.466. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500 | Train Loss: 44.1818 | Val Loss: 44.3608 | Val MAE: 44.361\n",
      "  -> New best Val MAE: 44.361. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500 | Train Loss: 43.8799 | Val Loss: 43.9405 | Val MAE: 43.940\n",
      "  -> New best Val MAE: 43.940. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500 | Train Loss: 43.4296 | Val Loss: 43.7139 | Val MAE: 43.714\n",
      "  -> New best Val MAE: 43.714. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500 | Train Loss: 43.0883 | Val Loss: 43.2210 | Val MAE: 43.221\n",
      "  -> New best Val MAE: 43.221. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500 | Train Loss: 42.7773 | Val Loss: 42.5773 | Val MAE: 42.577\n",
      "  -> New best Val MAE: 42.577. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500 | Train Loss: 42.4658 | Val Loss: 42.3799 | Val MAE: 42.380\n",
      "  -> New best Val MAE: 42.380. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500 | Train Loss: 42.0371 | Val Loss: 41.9236 | Val MAE: 41.924\n",
      "  -> New best Val MAE: 41.924. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500 | Train Loss: 41.6840 | Val Loss: 41.7393 | Val MAE: 41.739\n",
      "  -> New best Val MAE: 41.739. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500 | Train Loss: 41.2728 | Val Loss: 41.2178 | Val MAE: 41.218\n",
      "  -> New best Val MAE: 41.218. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500 | Train Loss: 40.8780 | Val Loss: 40.9335 | Val MAE: 40.934\n",
      "  -> New best Val MAE: 40.934. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500 | Train Loss: 40.5315 | Val Loss: 40.3447 | Val MAE: 40.345\n",
      "  -> New best Val MAE: 40.345. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/500 | Train Loss: 40.0427 | Val Loss: 40.2722 | Val MAE: 40.272\n",
      "  -> New best Val MAE: 40.272. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500 | Train Loss: 39.6840 | Val Loss: 39.7917 | Val MAE: 39.792\n",
      "  -> New best Val MAE: 39.792. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/500 | Train Loss: 39.3937 | Val Loss: 39.1347 | Val MAE: 39.135\n",
      "  -> New best Val MAE: 39.135. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/500 | Train Loss: 38.9201 | Val Loss: 39.0194 | Val MAE: 39.019\n",
      "  -> New best Val MAE: 39.019. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/500 | Train Loss: 38.4393 | Val Loss: 38.2090 | Val MAE: 38.209\n",
      "  -> New best Val MAE: 38.209. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/500 | Train Loss: 38.0817 | Val Loss: 38.0172 | Val MAE: 38.017\n",
      "  -> New best Val MAE: 38.017. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/500 | Train Loss: 37.5893 | Val Loss: 37.5810 | Val MAE: 37.581\n",
      "  -> New best Val MAE: 37.581. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/500 | Train Loss: 37.1697 | Val Loss: 37.3500 | Val MAE: 37.350\n",
      "  -> New best Val MAE: 37.350. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/500 | Train Loss: 36.8667 | Val Loss: 36.4018 | Val MAE: 36.402\n",
      "  -> New best Val MAE: 36.402. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/500 | Train Loss: 36.3615 | Val Loss: 36.0007 | Val MAE: 36.001\n",
      "  -> New best Val MAE: 36.001. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/500 | Train Loss: 36.0545 | Val Loss: 35.2553 | Val MAE: 35.255\n",
      "  -> New best Val MAE: 35.255. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/500 | Train Loss: 35.6027 | Val Loss: 34.8591 | Val MAE: 34.859\n",
      "  -> New best Val MAE: 34.859. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/500 | Train Loss: 35.1319 | Val Loss: 34.6616 | Val MAE: 34.662\n",
      "  -> New best Val MAE: 34.662. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/500 | Train Loss: 34.7332 | Val Loss: 34.4560 | Val MAE: 34.456\n",
      "  -> New best Val MAE: 34.456. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/500 | Train Loss: 34.2823 | Val Loss: 33.9753 | Val MAE: 33.975\n",
      "  -> New best Val MAE: 33.975. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/500 | Train Loss: 33.7968 | Val Loss: 32.7612 | Val MAE: 32.761\n",
      "  -> New best Val MAE: 32.761. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500 | Train Loss: 33.3087 | Val Loss: 33.0136 | Val MAE: 33.014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500 | Train Loss: 33.0148 | Val Loss: 32.4875 | Val MAE: 32.488\n",
      "  -> New best Val MAE: 32.488. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500 | Train Loss: 32.3593 | Val Loss: 32.1971 | Val MAE: 32.197\n",
      "  -> New best Val MAE: 32.197. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500 | Train Loss: 31.8568 | Val Loss: 30.8787 | Val MAE: 30.879\n",
      "  -> New best Val MAE: 30.879. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500 | Train Loss: 31.5493 | Val Loss: 31.2089 | Val MAE: 31.209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500 | Train Loss: 30.9200 | Val Loss: 31.0007 | Val MAE: 31.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500 | Train Loss: 30.6956 | Val Loss: 29.9315 | Val MAE: 29.932\n",
      "  -> New best Val MAE: 29.932. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500 | Train Loss: 30.0242 | Val Loss: 29.1914 | Val MAE: 29.191\n",
      "  -> New best Val MAE: 29.191. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500 | Train Loss: 29.5182 | Val Loss: 29.5569 | Val MAE: 29.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500 | Train Loss: 29.1720 | Val Loss: 28.9956 | Val MAE: 28.996\n",
      "  -> New best Val MAE: 28.996. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500 | Train Loss: 28.6142 | Val Loss: 28.0000 | Val MAE: 28.000\n",
      "  -> New best Val MAE: 28.000. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500 | Train Loss: 28.0696 | Val Loss: 27.6230 | Val MAE: 27.623\n",
      "  -> New best Val MAE: 27.623. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/500 | Train Loss: 27.7484 | Val Loss: 27.7554 | Val MAE: 27.755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/500 | Train Loss: 27.3016 | Val Loss: 26.6312 | Val MAE: 26.631\n",
      "  -> New best Val MAE: 26.631. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/500 | Train Loss: 26.7378 | Val Loss: 24.9484 | Val MAE: 24.948\n",
      "  -> New best Val MAE: 24.948. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/500 | Train Loss: 26.3313 | Val Loss: 23.6669 | Val MAE: 23.667\n",
      "  -> New best Val MAE: 23.667. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/500 | Train Loss: 25.7375 | Val Loss: 25.3459 | Val MAE: 25.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/500 | Train Loss: 25.3498 | Val Loss: 24.0378 | Val MAE: 24.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/500 | Train Loss: 24.8870 | Val Loss: 23.6777 | Val MAE: 23.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/500 | Train Loss: 24.2999 | Val Loss: 22.8534 | Val MAE: 22.853\n",
      "  -> New best Val MAE: 22.853. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/500 | Train Loss: 24.0714 | Val Loss: 23.3730 | Val MAE: 23.373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/500 | Train Loss: 23.3747 | Val Loss: 21.7609 | Val MAE: 21.761\n",
      "  -> New best Val MAE: 21.761. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/500 | Train Loss: 22.8993 | Val Loss: 21.5396 | Val MAE: 21.540\n",
      "  -> New best Val MAE: 21.540. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/500 | Train Loss: 22.6974 | Val Loss: 20.6382 | Val MAE: 20.638\n",
      "  -> New best Val MAE: 20.638. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/500 | Train Loss: 22.0128 | Val Loss: 21.3130 | Val MAE: 21.313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/500 | Train Loss: 21.3560 | Val Loss: 22.0422 | Val MAE: 22.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/500 | Train Loss: 20.9322 | Val Loss: 19.7850 | Val MAE: 19.785\n",
      "  -> New best Val MAE: 19.785. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/500 | Train Loss: 20.4687 | Val Loss: 20.5792 | Val MAE: 20.579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/500 | Train Loss: 19.8896 | Val Loss: 18.9932 | Val MAE: 18.993\n",
      "  -> New best Val MAE: 18.993. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500 | Train Loss: 19.5454 | Val Loss: 18.9610 | Val MAE: 18.961\n",
      "  -> New best Val MAE: 18.961. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500 | Train Loss: 18.8247 | Val Loss: 17.3572 | Val MAE: 17.357\n",
      "  -> New best Val MAE: 17.357. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/500 | Train Loss: 18.5210 | Val Loss: 16.6927 | Val MAE: 16.693\n",
      "  -> New best Val MAE: 16.693. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500 | Train Loss: 17.9580 | Val Loss: 18.1808 | Val MAE: 18.181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500 | Train Loss: 17.4119 | Val Loss: 16.0821 | Val MAE: 16.082\n",
      "  -> New best Val MAE: 16.082. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/500 | Train Loss: 16.9178 | Val Loss: 16.9096 | Val MAE: 16.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500 | Train Loss: 16.5804 | Val Loss: 16.1407 | Val MAE: 16.141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500 | Train Loss: 16.2098 | Val Loss: 16.0093 | Val MAE: 16.009\n",
      "  -> New best Val MAE: 16.009. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500 | Train Loss: 15.8606 | Val Loss: 16.2306 | Val MAE: 16.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500 | Train Loss: 15.4427 | Val Loss: 15.2010 | Val MAE: 15.201\n",
      "  -> New best Val MAE: 15.201. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500 | Train Loss: 15.0767 | Val Loss: 13.7068 | Val MAE: 13.707\n",
      "  -> New best Val MAE: 13.707. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/500 | Train Loss: 14.9888 | Val Loss: 14.0311 | Val MAE: 14.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/500 | Train Loss: 14.2975 | Val Loss: 13.2899 | Val MAE: 13.290\n",
      "  -> New best Val MAE: 13.290. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/500 | Train Loss: 14.1800 | Val Loss: 14.0533 | Val MAE: 14.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/500 | Train Loss: 13.9522 | Val Loss: 11.8511 | Val MAE: 11.851\n",
      "  -> New best Val MAE: 11.851. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/500 | Train Loss: 13.3630 | Val Loss: 11.9772 | Val MAE: 11.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/500 | Train Loss: 13.1851 | Val Loss: 12.5152 | Val MAE: 12.515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/500 | Train Loss: 12.7852 | Val Loss: 11.9503 | Val MAE: 11.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/500 | Train Loss: 12.8144 | Val Loss: 12.1763 | Val MAE: 12.176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/500 | Train Loss: 12.2688 | Val Loss: 11.4118 | Val MAE: 11.412\n",
      "  -> New best Val MAE: 11.412. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/500 | Train Loss: 11.9156 | Val Loss: 10.7049 | Val MAE: 10.705\n",
      "  -> New best Val MAE: 10.705. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/500 | Train Loss: 12.1394 | Val Loss: 10.6405 | Val MAE: 10.640\n",
      "  -> New best Val MAE: 10.640. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/500 | Train Loss: 11.7996 | Val Loss: 9.5095 | Val MAE: 9.509\n",
      "  -> New best Val MAE: 9.509. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/500 | Train Loss: 11.4969 | Val Loss: 10.7671 | Val MAE: 10.767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/500 | Train Loss: 11.4571 | Val Loss: 9.5920 | Val MAE: 9.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/500 | Train Loss: 11.1453 | Val Loss: 10.6038 | Val MAE: 10.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500 | Train Loss: 10.9424 | Val Loss: 9.3645 | Val MAE: 9.365\n",
      "  -> New best Val MAE: 9.365. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500 | Train Loss: 10.7760 | Val Loss: 9.8142 | Val MAE: 9.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/500 | Train Loss: 11.0362 | Val Loss: 9.1374 | Val MAE: 9.137\n",
      "  -> New best Val MAE: 9.137. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500 | Train Loss: 10.6597 | Val Loss: 8.8777 | Val MAE: 8.878\n",
      "  -> New best Val MAE: 8.878. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/500 | Train Loss: 10.4194 | Val Loss: 10.3979 | Val MAE: 10.398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/500 | Train Loss: 10.3621 | Val Loss: 8.5180 | Val MAE: 8.518\n",
      "  -> New best Val MAE: 8.518. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/500 | Train Loss: 10.5719 | Val Loss: 8.3304 | Val MAE: 8.330\n",
      "  -> New best Val MAE: 8.330. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/500 | Train Loss: 10.3023 | Val Loss: 7.8517 | Val MAE: 7.852\n",
      "  -> New best Val MAE: 7.852. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/500 | Train Loss: 10.4329 | Val Loss: 7.9605 | Val MAE: 7.961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/500 | Train Loss: 10.2939 | Val Loss: 7.6513 | Val MAE: 7.651\n",
      "  -> New best Val MAE: 7.651. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/500 | Train Loss: 10.0490 | Val Loss: 7.9325 | Val MAE: 7.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/500 | Train Loss: 10.0486 | Val Loss: 8.2821 | Val MAE: 8.282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/500 | Train Loss: 9.9898 | Val Loss: 8.8429 | Val MAE: 8.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/500 | Train Loss: 9.8826 | Val Loss: 7.7119 | Val MAE: 7.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/500 | Train Loss: 9.7971 | Val Loss: 7.5083 | Val MAE: 7.508\n",
      "  -> New best Val MAE: 7.508. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/500 | Train Loss: 9.8522 | Val Loss: 8.6738 | Val MAE: 8.674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/500 | Train Loss: 9.8264 | Val Loss: 7.7783 | Val MAE: 7.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/500 | Train Loss: 9.6885 | Val Loss: 7.7470 | Val MAE: 7.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/500 | Train Loss: 9.9866 | Val Loss: 7.4814 | Val MAE: 7.481\n",
      "  -> New best Val MAE: 7.481. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/500 | Train Loss: 9.5816 | Val Loss: 7.3299 | Val MAE: 7.330\n",
      "  -> New best Val MAE: 7.330. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/500 | Train Loss: 9.8610 | Val Loss: 7.6957 | Val MAE: 7.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/500 | Train Loss: 9.5754 | Val Loss: 7.3769 | Val MAE: 7.377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/500 | Train Loss: 9.3983 | Val Loss: 7.6800 | Val MAE: 7.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/500 | Train Loss: 9.8741 | Val Loss: 7.3386 | Val MAE: 7.339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/500 | Train Loss: 9.5094 | Val Loss: 7.4059 | Val MAE: 7.406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/500 | Train Loss: 9.5052 | Val Loss: 8.0821 | Val MAE: 8.082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/500 | Train Loss: 9.4526 | Val Loss: 7.6788 | Val MAE: 7.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/500 | Train Loss: 9.9306 | Val Loss: 7.4401 | Val MAE: 7.440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/500 | Train Loss: 9.6209 | Val Loss: 7.3518 | Val MAE: 7.352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/500 | Train Loss: 9.8077 | Val Loss: 7.2820 | Val MAE: 7.282\n",
      "  -> New best Val MAE: 7.282. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/500 | Train Loss: 9.3995 | Val Loss: 7.4187 | Val MAE: 7.419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/500 | Train Loss: 9.5777 | Val Loss: 7.4919 | Val MAE: 7.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/500 | Train Loss: 9.3840 | Val Loss: 7.2244 | Val MAE: 7.224\n",
      "  -> New best Val MAE: 7.224. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/500 | Train Loss: 9.9735 | Val Loss: 7.3761 | Val MAE: 7.376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/500 | Train Loss: 9.6386 | Val Loss: 7.3061 | Val MAE: 7.306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/500 | Train Loss: 10.0165 | Val Loss: 6.9860 | Val MAE: 6.986\n",
      "  -> New best Val MAE: 6.986. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/500 | Train Loss: 9.3355 | Val Loss: 7.5021 | Val MAE: 7.502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/500 | Train Loss: 9.3350 | Val Loss: 7.2044 | Val MAE: 7.204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/500 | Train Loss: 9.5110 | Val Loss: 7.3677 | Val MAE: 7.368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/500 | Train Loss: 9.3733 | Val Loss: 7.5166 | Val MAE: 7.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/500 | Train Loss: 9.6272 | Val Loss: 7.1386 | Val MAE: 7.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/500 | Train Loss: 9.5017 | Val Loss: 7.4890 | Val MAE: 7.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/500 | Train Loss: 9.4376 | Val Loss: 6.9857 | Val MAE: 6.986\n",
      "  -> New best Val MAE: 6.986. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/500 | Train Loss: 9.3915 | Val Loss: 7.0737 | Val MAE: 7.074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500 | Train Loss: 9.5902 | Val Loss: 7.1989 | Val MAE: 7.199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/500 | Train Loss: 9.1375 | Val Loss: 6.9915 | Val MAE: 6.992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/500 | Train Loss: 9.3452 | Val Loss: 6.9271 | Val MAE: 6.927\n",
      "  -> New best Val MAE: 6.927. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/500 | Train Loss: 9.4251 | Val Loss: 6.9337 | Val MAE: 6.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/500 | Train Loss: 9.5191 | Val Loss: 7.3349 | Val MAE: 7.335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/500 | Train Loss: 9.4230 | Val Loss: 6.8787 | Val MAE: 6.879\n",
      "  -> New best Val MAE: 6.879. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/500 | Train Loss: 9.2206 | Val Loss: 6.9510 | Val MAE: 6.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/500 | Train Loss: 9.3509 | Val Loss: 7.1393 | Val MAE: 7.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/500 | Train Loss: 9.4592 | Val Loss: 7.2990 | Val MAE: 7.299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/500 | Train Loss: 9.6816 | Val Loss: 7.0267 | Val MAE: 7.027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/500 | Train Loss: 8.9257 | Val Loss: 7.0624 | Val MAE: 7.062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/500 | Train Loss: 9.4929 | Val Loss: 7.2032 | Val MAE: 7.203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187/500 | Train Loss: 9.1691 | Val Loss: 7.0979 | Val MAE: 7.098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/500 | Train Loss: 9.3250 | Val Loss: 7.0467 | Val MAE: 7.047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/500 | Train Loss: 9.3788 | Val Loss: 6.9255 | Val MAE: 6.926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/500 | Train Loss: 9.4041 | Val Loss: 6.7827 | Val MAE: 6.783\n",
      "  -> New best Val MAE: 6.783. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/500 | Train Loss: 9.2505 | Val Loss: 6.9986 | Val MAE: 6.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/500 | Train Loss: 9.3126 | Val Loss: 6.6925 | Val MAE: 6.692\n",
      "  -> New best Val MAE: 6.692. Saved model to fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/500 | Train Loss: 9.2313 | Val Loss: 6.8866 | Val MAE: 6.887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/500 | Train Loss: 9.0571 | Val Loss: 6.9718 | Val MAE: 6.972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/500 | Train Loss: 9.4948 | Val Loss: 7.1458 | Val MAE: 7.146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/500 | Train Loss: 9.2252 | Val Loss: 6.8835 | Val MAE: 6.884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/500 | Train Loss: 9.0976 | Val Loss: 6.8495 | Val MAE: 6.850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/500 | Train Loss: 9.2518 | Val Loss: 6.9405 | Val MAE: 6.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/500 | Train Loss: 9.2620 | Val Loss: 6.8251 | Val MAE: 6.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/500 | Train Loss: 9.1508 | Val Loss: 7.0214 | Val MAE: 7.021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/500 | Train Loss: 9.0995 | Val Loss: 6.8459 | Val MAE: 6.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/500 | Train Loss: 9.2542 | Val Loss: 6.9494 | Val MAE: 6.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/500 | Train Loss: 9.4390 | Val Loss: 6.8511 | Val MAE: 6.851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204/500 | Train Loss: 8.9835 | Val Loss: 6.7988 | Val MAE: 6.799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205/500 | Train Loss: 9.4374 | Val Loss: 6.9409 | Val MAE: 6.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206/500 | Train Loss: 9.1314 | Val Loss: 6.8150 | Val MAE: 6.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207/500 | Train Loss: 9.1699 | Val Loss: 7.0878 | Val MAE: 7.088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208/500 | Train Loss: 9.3498 | Val Loss: 6.8732 | Val MAE: 6.873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209/500 | Train Loss: 9.1226 | Val Loss: 6.9563 | Val MAE: 6.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210/500 | Train Loss: 9.3128 | Val Loss: 7.0457 | Val MAE: 7.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211/500 | Train Loss: 9.0309 | Val Loss: 6.8429 | Val MAE: 6.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212/500 | Train Loss: 9.1723 | Val Loss: 7.0807 | Val MAE: 7.081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/500 | Train Loss: 9.0588 | Val Loss: 6.9640 | Val MAE: 6.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214/500 | Train Loss: 9.1037 | Val Loss: 7.0310 | Val MAE: 7.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215/500 | Train Loss: 9.2086 | Val Loss: 6.8893 | Val MAE: 6.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216/500 | Train Loss: 9.3317 | Val Loss: 6.8486 | Val MAE: 6.849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/500 | Train Loss: 9.0031 | Val Loss: 6.9251 | Val MAE: 6.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218/500 | Train Loss: 8.9669 | Val Loss: 6.9444 | Val MAE: 6.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219/500 | Train Loss: 9.0458 | Val Loss: 6.8939 | Val MAE: 6.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/500 | Train Loss: 9.1502 | Val Loss: 6.9259 | Val MAE: 6.926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/500 | Train Loss: 9.1316 | Val Loss: 7.0416 | Val MAE: 7.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/500 | Train Loss: 8.8570 | Val Loss: 6.7551 | Val MAE: 6.755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/500 | Train Loss: 9.1416 | Val Loss: 6.9232 | Val MAE: 6.923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/500 | Train Loss: 9.1907 | Val Loss: 7.0286 | Val MAE: 7.029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/500 | Train Loss: 9.1011 | Val Loss: 6.9183 | Val MAE: 6.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/500 | Train Loss: 9.0350 | Val Loss: 6.9535 | Val MAE: 6.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/500 | Train Loss: 9.1556 | Val Loss: 6.9345 | Val MAE: 6.935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/500 | Train Loss: 9.1376 | Val Loss: 6.8911 | Val MAE: 6.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/500 | Train Loss: 8.8160 | Val Loss: 7.0914 | Val MAE: 7.091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/500 | Train Loss: 9.0500 | Val Loss: 7.3193 | Val MAE: 7.319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/500 | Train Loss: 9.1384 | Val Loss: 6.9956 | Val MAE: 6.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/500 | Train Loss: 9.0564 | Val Loss: 7.0950 | Val MAE: 7.095\n",
      "\n",
      "Early stopping triggered after 40 epochs without improvement.\n",
      "\n",
      "--- Training Finished ---\n",
      "Best Validation MAE: 6.692 achieved at epoch 192\n",
      "Best model saved to fused_mlp_model_avg_pool_coronal_120_130.pth\n",
      "\n",
      "--- Evaluating on Test Set using Best Model ---\n",
      "Loaded best model weights from fused_mlp_model_avg_pool_coronal_120_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 6.9427 | Test MAE: 6.943\n",
      "\n",
      "Feature fusion (average pooling) MLP training script for Coronal [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130] finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Point to the directory containing the coronal 120-130 slice features\n",
    "FEATURE_ROOT = Path(\"/data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_coronal_120_130\")\n",
    "CSV_PATH = Path(\"/data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\")\n",
    "MODEL_SAVE_PATH = Path(\"./fused_mlp_model_avg_pool_coronal_120_130.pth\") # Path to save the best model\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "SPLITS = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "# Define the target coronal slices to load and fuse\n",
    "CORONAL_SLICE_CENTER = 125\n",
    "CORONAL_SLICE_RANGE = 5\n",
    "TARGET_CORONAL_INDICES = list(range(CORONAL_SLICE_CENTER - CORONAL_SLICE_RANGE,\n",
    "                                    CORONAL_SLICE_CENTER + CORONAL_SLICE_RANGE + 1)) # 120 to 130\n",
    "# Convert indices to strings to match the dataset logic expecting identifiers\n",
    "TARGET_SLICE_IDENTIFIERS = [str(idx) for idx in TARGET_CORONAL_INDICES]\n",
    "\n",
    "NUM_SLICES_TO_FUSE = len(TARGET_SLICE_IDENTIFIERS)\n",
    "FEATURE_DIM_PER_SLICE = 256 # Dimension after Global Average Pooling in preprocessing\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "EPOCHS = 500\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3 # Start similar to single-slice models, may need tuning\n",
    "SCHEDULER_PATIENCE_PERCENT = 0.10\n",
    "EARLY_STOPPING_PATIENCE = 40\n",
    "NUM_WORKERS = 4\n",
    "FUSION_METHOD = 'average' # 'average' or 'max'\n",
    "\n",
    "# --- Dataset Definition (Loads and Fuses Features) ---\n",
    "class FusedSliceDataset(Dataset):\n",
    "    def __init__(self, feature_root, csv_path, split, target_slice_identifiers):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_root (Path): Base directory containing split folders with subject subdirs.\n",
    "            csv_path (Path): Path to the CSV file with metadata.\n",
    "            split (str): The dataset split ('train', 'validation', or 'test').\n",
    "            target_slice_identifiers (list[str]): List of slice identifiers (e.g., '120', '121').\n",
    "        \"\"\"\n",
    "        self.feature_root = Path(feature_root)\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.split = split\n",
    "        self.target_slice_identifiers = sorted(target_slice_identifiers) # Ensure consistent order\n",
    "        self.num_slices_to_fuse = len(self.target_slice_identifiers)\n",
    "        self.split_dir = self.feature_root / self.split\n",
    "\n",
    "        print(f\"\\n[Dataset Init] Split: {self.split}, Fusing Coronal Slices: {self.target_slice_identifiers}\")\n",
    "        print(f\"Loading features from base: {self.feature_root}\")\n",
    "        print(f\"Loading metadata from: {self.csv_path}\")\n",
    "\n",
    "        if not self.split_dir.is_dir():\n",
    "            raise FileNotFoundError(f\"Split directory not found: {self.split_dir}\")\n",
    "        if not self.csv_path.is_file():\n",
    "            raise FileNotFoundError(f\"CSV file not found: {self.csv_path}\")\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(self.csv_path)\n",
    "            self.meta_dict = df.set_index('filename')['age'].to_dict()\n",
    "            self.subject_id_to_filename = {\n",
    "                fname.replace(\".nii.gz\", \"\").replace(\".nii\", \"\"): fname\n",
    "                for fname in self.meta_dict.keys()\n",
    "            }\n",
    "            print(f\"Loaded metadata for {len(self.meta_dict)} subjects from CSV.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading or processing CSV {self.csv_path}: {e}\")\n",
    "\n",
    "        all_subject_dirs = [d for d in self.split_dir.iterdir() if d.is_dir()]\n",
    "        self.valid_subject_dirs = []\n",
    "        missing_meta_count = 0\n",
    "        incomplete_slice_count = 0\n",
    "\n",
    "        print(f\"Scanning {len(all_subject_dirs)} potential subject directories...\")\n",
    "\n",
    "        for subject_dir in all_subject_dirs:\n",
    "            subject_id = subject_dir.name\n",
    "            original_filename = self.subject_id_to_filename.get(subject_id)\n",
    "\n",
    "            if original_filename and original_filename in self.meta_dict:\n",
    "                all_slices_present = True\n",
    "                for slice_id in self.target_slice_identifiers:\n",
    "                    # Construct filename based on the coronal slice index\n",
    "                    slice_filename = f\"slice_{slice_id}.npy\"\n",
    "                    expected_slice_path = subject_dir / slice_filename\n",
    "                    if not expected_slice_path.is_file():\n",
    "                        all_slices_present = False\n",
    "                        break\n",
    "\n",
    "                if all_slices_present:\n",
    "                    self.valid_subject_dirs.append(subject_dir)\n",
    "                else:\n",
    "                    incomplete_slice_count += 1\n",
    "            else:\n",
    "                missing_meta_count += 1\n",
    "\n",
    "        if missing_meta_count > 0:\n",
    "            print(f\"Warning: {missing_meta_count} subject directories did not have corresponding metadata.\")\n",
    "        if incomplete_slice_count > 0:\n",
    "            print(f\"Warning: {incomplete_slice_count} subjects with metadata were missing one or more required slices.\")\n",
    "\n",
    "        if not self.valid_subject_dirs:\n",
    "             raise RuntimeError(f\"No subjects found in {self.split_dir} with metadata and all required slices {self.target_slice_identifiers}.\")\n",
    "\n",
    "        print(f\"Found {len(self.valid_subject_dirs)} valid subjects for split {self.split}.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_subject_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_dir = self.valid_subject_dirs[idx]\n",
    "        subject_id = subject_dir.name\n",
    "        original_filename = self.subject_id_to_filename.get(subject_id)\n",
    "\n",
    "        if not original_filename:\n",
    "             raise ValueError(f\"Could not find original filename for subject ID {subject_id}\")\n",
    "\n",
    "        slice_features = []\n",
    "        try:\n",
    "            for slice_id in self.target_slice_identifiers:\n",
    "                slice_filename = f\"slice_{slice_id}.npy\"\n",
    "                slice_path = subject_dir / slice_filename\n",
    "                embedding = np.load(slice_path)\n",
    "                embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "\n",
    "                # Apply Global Average Pooling (GAP) if not already done\n",
    "                if embedding_tensor.ndim == 4 and embedding_tensor.shape[0] == 1 and embedding_tensor.shape[1] == FEATURE_DIM_PER_SLICE:\n",
    "                    pooled_embedding = F.adaptive_avg_pool2d(embedding_tensor, (1, 1)).squeeze()\n",
    "                elif embedding_tensor.ndim == 3 and embedding_tensor.shape[0] == FEATURE_DIM_PER_SLICE:\n",
    "                     pooled_embedding = F.adaptive_avg_pool2d(embedding_tensor.unsqueeze(0), (1, 1)).squeeze()\n",
    "                elif embedding_tensor.ndim == 1 and embedding_tensor.shape[0] == FEATURE_DIM_PER_SLICE:\n",
    "                     pooled_embedding = embedding_tensor\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected embedding shape {embedding_tensor.shape} for {slice_path}.\")\n",
    "\n",
    "                if pooled_embedding.shape[0] != FEATURE_DIM_PER_SLICE:\n",
    "                    raise ValueError(f\"Pooled embedding shape is not {FEATURE_DIM_PER_SLICE} for {slice_path}: {pooled_embedding.shape}\")\n",
    "\n",
    "                slice_features.append(pooled_embedding)\n",
    "\n",
    "            stacked_features = torch.stack(slice_features, dim=0)\n",
    "\n",
    "            if FUSION_METHOD == 'average':\n",
    "                fused_feature_vector = torch.mean(stacked_features, dim=0)\n",
    "            # elif FUSION_METHOD == 'max':\n",
    "            #     fused_feature_vector = torch.max(stacked_features, dim=0)[0]\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown fusion method: {FUSION_METHOD}\")\n",
    "\n",
    "            age = self.meta_dict[original_filename]\n",
    "            age_tensor = torch.tensor(age, dtype=torch.float32)\n",
    "\n",
    "            return fused_feature_vector, age_tensor\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing slices for subject {subject_dir}: {e}\")\n",
    "            raise e\n",
    "\n",
    "# --- Model Definition (Takes FUSED features as input) ---\n",
    "# Input dim is FEATURE_DIM_PER_SLICE (256 after fusion)\n",
    "class FusedAgeMLP(nn.Module):\n",
    "    def __init__(self, input_dim=FEATURE_DIM_PER_SLICE, hidden_dim1=128, hidden_dim2=64, hidden_dim3=32, dropout_rate=DROPOUT_RATE):\n",
    "        super(FusedAgeMLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1), nn.BatchNorm1d(hidden_dim1), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2), nn.BatchNorm1d(hidden_dim2), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3), nn.BatchNorm1d(hidden_dim3), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x).squeeze(-1)\n",
    "\n",
    "# --- Training and Evaluation Functions (Identical) ---\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for features, ages in pbar:\n",
    "        features, ages = features.to(device), ages.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, ages)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * features.size(0)\n",
    "        num_samples += features.size(0)\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "    return total_loss / num_samples\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    num_samples = 0\n",
    "    pbar = tqdm(loader, desc=\"Evaluating\", leave=False)\n",
    "    for features, ages in pbar:\n",
    "        features, ages = features.to(device), ages.to(device)\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, ages)\n",
    "        total_loss += loss.item() * features.size(0)\n",
    "        mae = F.l1_loss(predictions, ages, reduction='sum')\n",
    "        total_mae += mae.item()\n",
    "        num_samples += features.size(0)\n",
    "        pbar.set_postfix(loss=loss.item(), mae=(total_mae/num_samples if num_samples > 0 else 0))\n",
    "\n",
    "    avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "    avg_mae = total_mae / num_samples if num_samples > 0 else 0\n",
    "    return avg_loss, avg_mae\n",
    "\n",
    "# --- Main Training Script ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting feature fusion ({FUSION_METHOD} pooling) MLP training for Coronal slices {TARGET_CORONAL_INDICES}...\")\n",
    "    print(f\"Target Slice Identifiers for Fusion: {TARGET_SLICE_IDENTIFIERS}\")\n",
    "    print(f\"Fused Feature Dim: {FEATURE_DIM_PER_SLICE}\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"Feature Root: {FEATURE_ROOT}\")\n",
    "    print(f\"CSV Path: {CSV_PATH}\")\n",
    "    print(f\"Best model will be saved to: {MODEL_SAVE_PATH}\")\n",
    "    print(f\"Hyperparameters: Epochs={EPOCHS}, LR={LEARNING_RATE}, Batch={BATCH_SIZE}, Dropout={DROPOUT_RATE}, ES_Patience={EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "    # --- Setup Datasets and DataLoaders ---\n",
    "    try:\n",
    "        print(\"Setting up datasets...\")\n",
    "        train_dataset = FusedSliceDataset(FEATURE_ROOT, CSV_PATH, 'train', TARGET_SLICE_IDENTIFIERS)\n",
    "        val_dataset = FusedSliceDataset(FEATURE_ROOT, CSV_PATH, 'validation', TARGET_SLICE_IDENTIFIERS)\n",
    "        test_dataset = FusedSliceDataset(FEATURE_ROOT, CSV_PATH, 'test', TARGET_SLICE_IDENTIFIERS)\n",
    "\n",
    "        print(\"Setting up dataloaders...\")\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    except (FileNotFoundError, ValueError, RuntimeError) as e:\n",
    "        print(f\"Error initializing datasets/loaders: {e}\")\n",
    "        raise SystemExit(\"Dataset/Loader initialization failed.\")\n",
    "\n",
    "    # --- Setup Model, Loss, Optimizer, Scheduler ---\n",
    "    print(\"Initializing model, optimizer, scheduler...\")\n",
    "    model = FusedAgeMLP(input_dim=FEATURE_DIM_PER_SLICE, dropout_rate=DROPOUT_RATE).to(DEVICE)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler_patience_epochs = max(1, int(EPOCHS * SCHEDULER_PATIENCE_PERCENT))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=scheduler_patience_epochs, factor=0.5, verbose=True)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    best_val_mae = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_epoch = -1\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_mae': []}\n",
    "\n",
    "    print(f\"\\n--- Starting Training ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        val_loss, val_mae = evaluate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_mae'].append(val_mae)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val MAE: {val_mae:.3f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_mae < best_val_mae:\n",
    "            best_val_mae = val_mae\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_no_improve = 0\n",
    "            try:\n",
    "                torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "                print(f\"  -> New best Val MAE: {best_val_mae:.3f}. Saved model to {MODEL_SAVE_PATH}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  -> Error saving model: {e}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered after {EARLY_STOPPING_PATIENCE} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "             gc.collect()\n",
    "             if DEVICE.startswith('cuda'): torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\n--- Training Finished ---\")\n",
    "    if best_epoch != -1:\n",
    "         print(f\"Best Validation MAE: {best_val_mae:.3f} achieved at epoch {best_epoch}\")\n",
    "         print(f\"Best model saved to {MODEL_SAVE_PATH}\")\n",
    "    else:\n",
    "         print(\"No improvement found during training or model saving failed.\")\n",
    "\n",
    "    # --- Evaluate on Test Set ---\n",
    "    if MODEL_SAVE_PATH.exists() and best_epoch != -1:\n",
    "        print(f\"\\n--- Evaluating on Test Set using Best Model ---\")\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "            print(f\"Loaded best model weights from {MODEL_SAVE_PATH}\")\n",
    "            test_loss, test_mae = evaluate(model, test_loader, criterion, DEVICE)\n",
    "            print(f\"Test Loss: {test_loss:.4f} | Test MAE: {test_mae:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during test set evaluation: {e}\")\n",
    "    elif not MODEL_SAVE_PATH.exists():\n",
    "         print(f\"\\nSkipping test set evaluation: Best model file not found at {MODEL_SAVE_PATH}\")\n",
    "    else:\n",
    "         print(\"\\nSkipping test set evaluation: No best model was saved during training.\")\n",
    "\n",
    "    print(f\"\\nFeature fusion ({FUSION_METHOD} pooling) MLP training script for Coronal {TARGET_CORONAL_INDICES} finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
