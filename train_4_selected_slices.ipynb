{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6baafa9c",
   "metadata": {},
   "source": [
    "Train one MLP for each selected slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38643345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting specific slice MLP training from multi-slice directory...\n",
      "Target Slices:\n",
      "  - Orientation: sagittal, Index: 80\n",
      "  - Orientation: sagittal, Index: 125\n",
      "  - Orientation: coronal, Index: 125\n",
      "  - Orientation: axial, Index: 80\n",
      "Using device: cuda:1\n",
      "Feature Root: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice\n",
      "CSV Path: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Models will be saved to: specific_slice_models_multi_dir\n",
      "Hyperparameters: Epochs=500, LR=0.0001, Batch=32, ES_Patience=50\n",
      "\n",
      "==================== Training for Slice: sagittal 80 ====================\n",
      "Using feature root: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice\n",
      "Setting up datasets...\n",
      "\n",
      "[Dataset Init] Split: train, Orientation: sagittal, Slice: 80\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/train\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 2274 subject directories in 'train' to CSV entries.\n",
      "Warning: Could not map 1 directories to CSV.\n",
      "Scanning 2275 potential subject directories for slice sagittal_80...\n",
      "Info: 1 subject directories were skipped as they couldn't be mapped to metadata.\n",
      "Found 2274 valid files for sagittal slice 80 in split train.\n",
      "\n",
      "[Dataset Init] Split: validation, Orientation: sagittal, Slice: 80\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/validation\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 280 subject directories in 'validation' to CSV entries.\n",
      "Scanning 280 potential subject directories for slice sagittal_80...\n",
      "Found 280 valid files for sagittal slice 80 in split validation.\n",
      "Setting up dataloaders...\n",
      "Initializing model, optimizer, scheduler...\n",
      "\n",
      "--- Starting Training for sagittal Slice 80 ---\n",
      "Epoch 1/500 | Train Loss: 53.8999 | Val Loss: 53.2547 | Val MAE: 53.255 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.255. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 2/500 | Train Loss: 53.7463 | Val Loss: 53.2251 | Val MAE: 53.225 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.225. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 3/500 | Train Loss: 53.6729 | Val Loss: 53.2182 | Val MAE: 53.218 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.218. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 4/500 | Train Loss: 53.5358 | Val Loss: 53.1554 | Val MAE: 53.155 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.155. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 5/500 | Train Loss: 53.4159 | Val Loss: 52.9514 | Val MAE: 52.951 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.951. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 6/500 | Train Loss: 53.2622 | Val Loss: 52.7269 | Val MAE: 52.727 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.727. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 7/500 | Train Loss: 53.1431 | Val Loss: 52.6647 | Val MAE: 52.665 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.665. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 8/500 | Train Loss: 53.0330 | Val Loss: 52.6469 | Val MAE: 52.647 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.647. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 9/500 | Train Loss: 52.8932 | Val Loss: 52.4058 | Val MAE: 52.406 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.406. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 10/500 | Train Loss: 52.7447 | Val Loss: 52.3758 | Val MAE: 52.376 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.376. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 11/500 | Train Loss: 52.5724 | Val Loss: 52.1415 | Val MAE: 52.142 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.142. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 12/500 | Train Loss: 52.4170 | Val Loss: 52.1085 | Val MAE: 52.109 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.109. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 13/500 | Train Loss: 52.2463 | Val Loss: 51.8010 | Val MAE: 51.801 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.801. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 14/500 | Train Loss: 52.1073 | Val Loss: 51.7050 | Val MAE: 51.705 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.705. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 15/500 | Train Loss: 51.9340 | Val Loss: 51.6214 | Val MAE: 51.621 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.621. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 16/500 | Train Loss: 51.7422 | Val Loss: 51.4295 | Val MAE: 51.429 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.429. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 17/500 | Train Loss: 51.5263 | Val Loss: 51.1611 | Val MAE: 51.161 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.161. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 18/500 | Train Loss: 51.3652 | Val Loss: 51.0061 | Val MAE: 51.006 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.006. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 19/500 | Train Loss: 51.1429 | Val Loss: 50.8512 | Val MAE: 50.851 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.851. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 20/500 | Train Loss: 50.9880 | Val Loss: 50.5298 | Val MAE: 50.530 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.530. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 21/500 | Train Loss: 50.7763 | Val Loss: 50.4280 | Val MAE: 50.428 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.428. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 22/500 | Train Loss: 50.5334 | Val Loss: 50.1843 | Val MAE: 50.184 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.184. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 23/500 | Train Loss: 50.3221 | Val Loss: 49.8354 | Val MAE: 49.835 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.835. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 24/500 | Train Loss: 50.0567 | Val Loss: 49.5329 | Val MAE: 49.533 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.533. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 25/500 | Train Loss: 49.8082 | Val Loss: 49.4435 | Val MAE: 49.444 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.444. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 26/500 | Train Loss: 49.5966 | Val Loss: 49.1744 | Val MAE: 49.174 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.174. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 27/500 | Train Loss: 49.3421 | Val Loss: 48.9822 | Val MAE: 48.982 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.982. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 28/500 | Train Loss: 49.1083 | Val Loss: 48.4225 | Val MAE: 48.423 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.423. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 29/500 | Train Loss: 48.8164 | Val Loss: 48.2873 | Val MAE: 48.287 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.287. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 30/500 | Train Loss: 48.5279 | Val Loss: 47.9758 | Val MAE: 47.976 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.976. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 31/500 | Train Loss: 48.2489 | Val Loss: 47.8386 | Val MAE: 47.839 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.839. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 32/500 | Train Loss: 47.9403 | Val Loss: 47.4565 | Val MAE: 47.457 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.457. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 33/500 | Train Loss: 47.6744 | Val Loss: 46.9226 | Val MAE: 46.923 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 46.923. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 34/500 | Train Loss: 47.3600 | Val Loss: 46.8070 | Val MAE: 46.807 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 46.807. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 35/500 | Train Loss: 47.0004 | Val Loss: 46.2122 | Val MAE: 46.212 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 46.212. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 36/500 | Train Loss: 46.7202 | Val Loss: 46.2608 | Val MAE: 46.261 | LR: 1.0e-04\n",
      "Epoch 37/500 | Train Loss: 46.3525 | Val Loss: 45.8022 | Val MAE: 45.802 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.802. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 38/500 | Train Loss: 46.0890 | Val Loss: 45.2569 | Val MAE: 45.257 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.257. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 39/500 | Train Loss: 45.7479 | Val Loss: 45.3194 | Val MAE: 45.319 | LR: 1.0e-04\n",
      "Epoch 40/500 | Train Loss: 45.3943 | Val Loss: 45.0338 | Val MAE: 45.034 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.034. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 41/500 | Train Loss: 45.0720 | Val Loss: 44.9455 | Val MAE: 44.946 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 44.946. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 42/500 | Train Loss: 44.7626 | Val Loss: 44.0591 | Val MAE: 44.059 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 44.059. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 43/500 | Train Loss: 44.4150 | Val Loss: 44.1423 | Val MAE: 44.142 | LR: 1.0e-04\n",
      "Epoch 44/500 | Train Loss: 44.1171 | Val Loss: 44.0136 | Val MAE: 44.014 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 44.014. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 45/500 | Train Loss: 43.6876 | Val Loss: 43.1393 | Val MAE: 43.139 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 43.139. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 46/500 | Train Loss: 43.2489 | Val Loss: 42.4957 | Val MAE: 42.496 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 42.496. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 47/500 | Train Loss: 42.9292 | Val Loss: 42.6470 | Val MAE: 42.647 | LR: 1.0e-04\n",
      "Epoch 48/500 | Train Loss: 42.5070 | Val Loss: 42.3241 | Val MAE: 42.324 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 42.324. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 49/500 | Train Loss: 42.1144 | Val Loss: 41.5782 | Val MAE: 41.578 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 41.578. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 50/500 | Train Loss: 41.7295 | Val Loss: 41.6712 | Val MAE: 41.671 | LR: 1.0e-04\n",
      "Epoch 51/500 | Train Loss: 41.3389 | Val Loss: 40.9736 | Val MAE: 40.974 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.974. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 52/500 | Train Loss: 41.0398 | Val Loss: 40.5995 | Val MAE: 40.599 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.599. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 53/500 | Train Loss: 40.6402 | Val Loss: 40.1979 | Val MAE: 40.198 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.198. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 54/500 | Train Loss: 40.1705 | Val Loss: 39.5687 | Val MAE: 39.569 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 39.569. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 55/500 | Train Loss: 39.7795 | Val Loss: 39.4448 | Val MAE: 39.445 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 39.445. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 56/500 | Train Loss: 39.3899 | Val Loss: 39.8749 | Val MAE: 39.875 | LR: 1.0e-04\n",
      "Epoch 57/500 | Train Loss: 38.9258 | Val Loss: 39.6224 | Val MAE: 39.622 | LR: 1.0e-04\n",
      "Epoch 58/500 | Train Loss: 38.5742 | Val Loss: 38.6512 | Val MAE: 38.651 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 38.651. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 59/500 | Train Loss: 38.1989 | Val Loss: 38.6201 | Val MAE: 38.620 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 38.620. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 60/500 | Train Loss: 37.6613 | Val Loss: 36.8671 | Val MAE: 36.867 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 36.867. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 61/500 | Train Loss: 37.2569 | Val Loss: 38.1386 | Val MAE: 38.139 | LR: 1.0e-04\n",
      "Epoch 62/500 | Train Loss: 36.8639 | Val Loss: 36.5448 | Val MAE: 36.545 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 36.545. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 63/500 | Train Loss: 36.3551 | Val Loss: 36.3457 | Val MAE: 36.346 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 36.346. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 64/500 | Train Loss: 35.7641 | Val Loss: 37.4982 | Val MAE: 37.498 | LR: 1.0e-04\n",
      "Epoch 65/500 | Train Loss: 35.4361 | Val Loss: 35.2930 | Val MAE: 35.293 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 35.293. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 66/500 | Train Loss: 35.0535 | Val Loss: 35.3566 | Val MAE: 35.357 | LR: 1.0e-04\n",
      "Epoch 67/500 | Train Loss: 34.6353 | Val Loss: 33.5768 | Val MAE: 33.577 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 33.577. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 68/500 | Train Loss: 34.2349 | Val Loss: 34.6193 | Val MAE: 34.619 | LR: 1.0e-04\n",
      "Epoch 69/500 | Train Loss: 33.7676 | Val Loss: 33.2208 | Val MAE: 33.221 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 33.221. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 70/500 | Train Loss: 33.2522 | Val Loss: 33.1681 | Val MAE: 33.168 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 33.168. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 71/500 | Train Loss: 32.7793 | Val Loss: 31.3436 | Val MAE: 31.344 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 31.344. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 72/500 | Train Loss: 32.3403 | Val Loss: 33.5731 | Val MAE: 33.573 | LR: 1.0e-04\n",
      "Epoch 73/500 | Train Loss: 32.0212 | Val Loss: 36.1912 | Val MAE: 36.191 | LR: 1.0e-04\n",
      "Epoch 74/500 | Train Loss: 31.4306 | Val Loss: 31.6325 | Val MAE: 31.633 | LR: 1.0e-04\n",
      "Epoch 75/500 | Train Loss: 31.0826 | Val Loss: 29.2411 | Val MAE: 29.241 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 29.241. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 76/500 | Train Loss: 30.4550 | Val Loss: 29.8437 | Val MAE: 29.844 | LR: 1.0e-04\n",
      "Epoch 77/500 | Train Loss: 30.0460 | Val Loss: 29.9363 | Val MAE: 29.936 | LR: 1.0e-04\n",
      "Epoch 78/500 | Train Loss: 29.6013 | Val Loss: 28.1482 | Val MAE: 28.148 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 28.148. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 79/500 | Train Loss: 28.9962 | Val Loss: 31.6695 | Val MAE: 31.669 | LR: 1.0e-04\n",
      "Epoch 80/500 | Train Loss: 28.7296 | Val Loss: 26.6909 | Val MAE: 26.691 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 26.691. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 81/500 | Train Loss: 28.3530 | Val Loss: 30.2678 | Val MAE: 30.268 | LR: 1.0e-04\n",
      "Epoch 82/500 | Train Loss: 27.7530 | Val Loss: 30.6909 | Val MAE: 30.691 | LR: 1.0e-04\n",
      "Epoch 83/500 | Train Loss: 27.1994 | Val Loss: 29.6634 | Val MAE: 29.663 | LR: 1.0e-04\n",
      "Epoch 84/500 | Train Loss: 26.8085 | Val Loss: 26.6305 | Val MAE: 26.630 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 26.630. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 85/500 | Train Loss: 26.4901 | Val Loss: 27.7447 | Val MAE: 27.745 | LR: 1.0e-04\n",
      "Epoch 86/500 | Train Loss: 25.7277 | Val Loss: 30.7211 | Val MAE: 30.721 | LR: 1.0e-04\n",
      "Epoch 87/500 | Train Loss: 25.3888 | Val Loss: 24.1809 | Val MAE: 24.181 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 24.181. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 88/500 | Train Loss: 24.8404 | Val Loss: 26.1778 | Val MAE: 26.178 | LR: 1.0e-04\n",
      "Epoch 89/500 | Train Loss: 24.5878 | Val Loss: 24.1483 | Val MAE: 24.148 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 24.148. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 90/500 | Train Loss: 24.2968 | Val Loss: 25.5556 | Val MAE: 25.556 | LR: 1.0e-04\n",
      "Epoch 91/500 | Train Loss: 23.6411 | Val Loss: 22.6649 | Val MAE: 22.665 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 22.665. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 92/500 | Train Loss: 23.1082 | Val Loss: 20.9529 | Val MAE: 20.953 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 20.953. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 93/500 | Train Loss: 22.8746 | Val Loss: 32.3136 | Val MAE: 32.314 | LR: 1.0e-04\n",
      "Epoch 94/500 | Train Loss: 22.1028 | Val Loss: 21.7850 | Val MAE: 21.785 | LR: 1.0e-04\n",
      "Epoch 95/500 | Train Loss: 21.8451 | Val Loss: 24.7917 | Val MAE: 24.792 | LR: 1.0e-04\n",
      "Epoch 96/500 | Train Loss: 21.3834 | Val Loss: 22.4390 | Val MAE: 22.439 | LR: 1.0e-04\n",
      "Epoch 97/500 | Train Loss: 20.8793 | Val Loss: 26.4418 | Val MAE: 26.442 | LR: 1.0e-04\n",
      "Epoch 98/500 | Train Loss: 20.5859 | Val Loss: 20.8052 | Val MAE: 20.805 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 20.805. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 99/500 | Train Loss: 20.1242 | Val Loss: 17.5133 | Val MAE: 17.513 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 17.513. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 100/500 | Train Loss: 19.8495 | Val Loss: 15.0523 | Val MAE: 15.052 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 15.052. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 101/500 | Train Loss: 19.4032 | Val Loss: 19.4154 | Val MAE: 19.415 | LR: 1.0e-04\n",
      "Epoch 102/500 | Train Loss: 18.8972 | Val Loss: 14.7157 | Val MAE: 14.716 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 14.716. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 103/500 | Train Loss: 18.6389 | Val Loss: 15.8207 | Val MAE: 15.821 | LR: 1.0e-04\n",
      "Epoch 104/500 | Train Loss: 18.1322 | Val Loss: 21.9138 | Val MAE: 21.914 | LR: 1.0e-04\n",
      "Epoch 105/500 | Train Loss: 17.8227 | Val Loss: 20.8910 | Val MAE: 20.891 | LR: 1.0e-04\n",
      "Epoch 106/500 | Train Loss: 17.4588 | Val Loss: 18.8964 | Val MAE: 18.896 | LR: 1.0e-04\n",
      "Epoch 107/500 | Train Loss: 16.8519 | Val Loss: 18.8598 | Val MAE: 18.860 | LR: 1.0e-04\n",
      "Epoch 108/500 | Train Loss: 16.8799 | Val Loss: 17.7517 | Val MAE: 17.752 | LR: 1.0e-04\n",
      "Epoch 109/500 | Train Loss: 16.3141 | Val Loss: 12.5083 | Val MAE: 12.508 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 12.508. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 110/500 | Train Loss: 16.0087 | Val Loss: 13.4649 | Val MAE: 13.465 | LR: 1.0e-04\n",
      "Epoch 111/500 | Train Loss: 15.9027 | Val Loss: 13.7148 | Val MAE: 13.715 | LR: 1.0e-04\n",
      "Epoch 112/500 | Train Loss: 15.2136 | Val Loss: 13.6540 | Val MAE: 13.654 | LR: 1.0e-04\n",
      "Epoch 113/500 | Train Loss: 15.2838 | Val Loss: 11.3408 | Val MAE: 11.341 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 11.341. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 114/500 | Train Loss: 14.8822 | Val Loss: 13.4626 | Val MAE: 13.463 | LR: 1.0e-04\n",
      "Epoch 115/500 | Train Loss: 14.5707 | Val Loss: 12.4826 | Val MAE: 12.483 | LR: 1.0e-04\n",
      "Epoch 116/500 | Train Loss: 14.2634 | Val Loss: 10.0718 | Val MAE: 10.072 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 10.072. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 117/500 | Train Loss: 14.1786 | Val Loss: 20.0801 | Val MAE: 20.080 | LR: 1.0e-04\n",
      "Epoch 118/500 | Train Loss: 13.6971 | Val Loss: 22.3712 | Val MAE: 22.371 | LR: 1.0e-04\n",
      "Epoch 119/500 | Train Loss: 13.4264 | Val Loss: 15.9199 | Val MAE: 15.920 | LR: 1.0e-04\n",
      "Epoch 120/500 | Train Loss: 13.3461 | Val Loss: 10.5837 | Val MAE: 10.584 | LR: 1.0e-04\n",
      "Epoch 121/500 | Train Loss: 12.8796 | Val Loss: 15.5894 | Val MAE: 15.589 | LR: 1.0e-04\n",
      "Epoch 122/500 | Train Loss: 12.7776 | Val Loss: 9.8267 | Val MAE: 9.827 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 9.827. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 123/500 | Train Loss: 12.5333 | Val Loss: 11.7069 | Val MAE: 11.707 | LR: 1.0e-04\n",
      "Epoch 124/500 | Train Loss: 12.4097 | Val Loss: 11.7256 | Val MAE: 11.726 | LR: 1.0e-04\n",
      "Epoch 125/500 | Train Loss: 12.4933 | Val Loss: 9.5358 | Val MAE: 9.536 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 9.536. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 126/500 | Train Loss: 12.4891 | Val Loss: 14.7964 | Val MAE: 14.796 | LR: 1.0e-04\n",
      "Epoch 127/500 | Train Loss: 11.9174 | Val Loss: 10.0454 | Val MAE: 10.045 | LR: 1.0e-04\n",
      "Epoch 128/500 | Train Loss: 11.8424 | Val Loss: 9.6648 | Val MAE: 9.665 | LR: 1.0e-04\n",
      "Epoch 129/500 | Train Loss: 11.6372 | Val Loss: 11.9396 | Val MAE: 11.940 | LR: 1.0e-04\n",
      "Epoch 130/500 | Train Loss: 11.6440 | Val Loss: 13.1751 | Val MAE: 13.175 | LR: 1.0e-04\n",
      "Epoch 131/500 | Train Loss: 11.4066 | Val Loss: 9.3718 | Val MAE: 9.372 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 9.372. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 132/500 | Train Loss: 11.4447 | Val Loss: 11.3311 | Val MAE: 11.331 | LR: 1.0e-04\n",
      "Epoch 133/500 | Train Loss: 11.4127 | Val Loss: 8.6431 | Val MAE: 8.643 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.643. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 134/500 | Train Loss: 11.2663 | Val Loss: 11.9685 | Val MAE: 11.968 | LR: 1.0e-04\n",
      "Epoch 135/500 | Train Loss: 11.0314 | Val Loss: 9.9173 | Val MAE: 9.917 | LR: 1.0e-04\n",
      "Epoch 136/500 | Train Loss: 11.0655 | Val Loss: 9.1730 | Val MAE: 9.173 | LR: 1.0e-04\n",
      "Epoch 137/500 | Train Loss: 10.9036 | Val Loss: 10.0256 | Val MAE: 10.026 | LR: 1.0e-04\n",
      "Epoch 138/500 | Train Loss: 10.7987 | Val Loss: 9.9626 | Val MAE: 9.963 | LR: 1.0e-04\n",
      "Epoch 139/500 | Train Loss: 10.9353 | Val Loss: 10.4684 | Val MAE: 10.468 | LR: 1.0e-04\n",
      "Epoch 140/500 | Train Loss: 10.7011 | Val Loss: 11.8933 | Val MAE: 11.893 | LR: 1.0e-04\n",
      "Epoch 141/500 | Train Loss: 10.6887 | Val Loss: 14.5247 | Val MAE: 14.525 | LR: 1.0e-04\n",
      "Epoch 142/500 | Train Loss: 10.6769 | Val Loss: 8.6673 | Val MAE: 8.667 | LR: 1.0e-04\n",
      "Epoch 143/500 | Train Loss: 10.5600 | Val Loss: 9.1789 | Val MAE: 9.179 | LR: 1.0e-04\n",
      "Epoch 144/500 | Train Loss: 10.3529 | Val Loss: 8.9308 | Val MAE: 8.931 | LR: 1.0e-04\n",
      "Epoch 145/500 | Train Loss: 10.3410 | Val Loss: 13.2146 | Val MAE: 13.215 | LR: 1.0e-04\n",
      "Epoch 146/500 | Train Loss: 10.4993 | Val Loss: 9.0989 | Val MAE: 9.099 | LR: 1.0e-04\n",
      "Epoch 147/500 | Train Loss: 10.6373 | Val Loss: 8.7198 | Val MAE: 8.720 | LR: 1.0e-04\n",
      "Epoch 148/500 | Train Loss: 10.4833 | Val Loss: 9.9735 | Val MAE: 9.974 | LR: 1.0e-04\n",
      "Epoch 149/500 | Train Loss: 10.2478 | Val Loss: 9.8691 | Val MAE: 9.869 | LR: 1.0e-04\n",
      "Epoch 150/500 | Train Loss: 10.6508 | Val Loss: 8.4885 | Val MAE: 8.489 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.489. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 151/500 | Train Loss: 10.2917 | Val Loss: 10.3869 | Val MAE: 10.387 | LR: 1.0e-04\n",
      "Epoch 152/500 | Train Loss: 10.6187 | Val Loss: 15.3421 | Val MAE: 15.342 | LR: 1.0e-04\n",
      "Epoch 153/500 | Train Loss: 10.4910 | Val Loss: 9.7833 | Val MAE: 9.783 | LR: 1.0e-04\n",
      "Epoch 154/500 | Train Loss: 10.6692 | Val Loss: 8.7861 | Val MAE: 8.786 | LR: 1.0e-04\n",
      "Epoch 155/500 | Train Loss: 10.2255 | Val Loss: 9.0773 | Val MAE: 9.077 | LR: 1.0e-04\n",
      "Epoch 156/500 | Train Loss: 10.1426 | Val Loss: 9.0719 | Val MAE: 9.072 | LR: 1.0e-04\n",
      "Epoch 157/500 | Train Loss: 10.2188 | Val Loss: 10.0204 | Val MAE: 10.020 | LR: 1.0e-04\n",
      "Epoch 158/500 | Train Loss: 10.2995 | Val Loss: 8.6864 | Val MAE: 8.686 | LR: 1.0e-04\n",
      "Epoch 159/500 | Train Loss: 10.2393 | Val Loss: 10.2992 | Val MAE: 10.299 | LR: 1.0e-04\n",
      "Epoch 160/500 | Train Loss: 9.8701 | Val Loss: 10.1709 | Val MAE: 10.171 | LR: 1.0e-04\n",
      "Epoch 161/500 | Train Loss: 10.1384 | Val Loss: 10.9632 | Val MAE: 10.963 | LR: 1.0e-04\n",
      "Epoch 162/500 | Train Loss: 9.9420 | Val Loss: 9.4207 | Val MAE: 9.421 | LR: 1.0e-04\n",
      "Epoch 163/500 | Train Loss: 10.1400 | Val Loss: 9.0688 | Val MAE: 9.069 | LR: 1.0e-04\n",
      "Epoch 164/500 | Train Loss: 10.0171 | Val Loss: 8.7799 | Val MAE: 8.780 | LR: 1.0e-04\n",
      "Epoch 165/500 | Train Loss: 10.1267 | Val Loss: 9.6259 | Val MAE: 9.626 | LR: 1.0e-04\n",
      "Epoch 166/500 | Train Loss: 10.0224 | Val Loss: 8.5024 | Val MAE: 8.502 | LR: 1.0e-04\n",
      "Epoch 167/500 | Train Loss: 10.1797 | Val Loss: 9.0165 | Val MAE: 9.017 | LR: 1.0e-04\n",
      "Epoch 168/500 | Train Loss: 10.2352 | Val Loss: 8.9099 | Val MAE: 8.910 | LR: 1.0e-04\n",
      "Epoch 169/500 | Train Loss: 10.0027 | Val Loss: 8.9679 | Val MAE: 8.968 | LR: 1.0e-04\n",
      "Epoch 170/500 | Train Loss: 10.0848 | Val Loss: 8.5680 | Val MAE: 8.568 | LR: 1.0e-04\n",
      "Epoch 171/500 | Train Loss: 9.9461 | Val Loss: 9.8795 | Val MAE: 9.880 | LR: 1.0e-04\n",
      "Epoch 172/500 | Train Loss: 9.9470 | Val Loss: 12.4810 | Val MAE: 12.481 | LR: 1.0e-04\n",
      "Epoch 173/500 | Train Loss: 10.0115 | Val Loss: 12.6573 | Val MAE: 12.657 | LR: 1.0e-04\n",
      "Epoch 174/500 | Train Loss: 9.8494 | Val Loss: 14.3513 | Val MAE: 14.351 | LR: 1.0e-04\n",
      "Epoch 175/500 | Train Loss: 9.9525 | Val Loss: 10.1061 | Val MAE: 10.106 | LR: 1.0e-04\n",
      "Epoch 176/500 | Train Loss: 10.0752 | Val Loss: 9.3586 | Val MAE: 9.359 | LR: 1.0e-04\n",
      "Epoch 177/500 | Train Loss: 9.6687 | Val Loss: 12.5299 | Val MAE: 12.530 | LR: 1.0e-04\n",
      "Epoch 178/500 | Train Loss: 9.9015 | Val Loss: 8.9001 | Val MAE: 8.900 | LR: 1.0e-04\n",
      "Epoch 179/500 | Train Loss: 9.8658 | Val Loss: 10.3792 | Val MAE: 10.379 | LR: 1.0e-04\n",
      "Epoch 180/500 | Train Loss: 10.0785 | Val Loss: 8.6526 | Val MAE: 8.653 | LR: 1.0e-04\n",
      "Epoch 181/500 | Train Loss: 9.5888 | Val Loss: 12.7133 | Val MAE: 12.713 | LR: 1.0e-04\n",
      "Epoch 182/500 | Train Loss: 10.0074 | Val Loss: 9.5507 | Val MAE: 9.551 | LR: 1.0e-04\n",
      "Epoch 183/500 | Train Loss: 10.0611 | Val Loss: 8.9422 | Val MAE: 8.942 | LR: 1.0e-04\n",
      "Epoch 184/500 | Train Loss: 10.0084 | Val Loss: 9.2761 | Val MAE: 9.276 | LR: 1.0e-04\n",
      "Epoch 185/500 | Train Loss: 9.9883 | Val Loss: 10.6534 | Val MAE: 10.653 | LR: 1.0e-04\n",
      "Epoch 186/500 | Train Loss: 9.7423 | Val Loss: 9.8520 | Val MAE: 9.852 | LR: 1.0e-04\n",
      "Epoch 187/500 | Train Loss: 9.7990 | Val Loss: 8.5115 | Val MAE: 8.512 | LR: 1.0e-04\n",
      "Epoch 188/500 | Train Loss: 10.0897 | Val Loss: 10.3903 | Val MAE: 10.390 | LR: 1.0e-04\n",
      "Epoch 189/500 | Train Loss: 9.8067 | Val Loss: 8.3685 | Val MAE: 8.368 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.368. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 190/500 | Train Loss: 9.6593 | Val Loss: 20.9654 | Val MAE: 20.965 | LR: 1.0e-04\n",
      "Epoch 191/500 | Train Loss: 9.7173 | Val Loss: 8.7029 | Val MAE: 8.703 | LR: 1.0e-04\n",
      "Epoch 192/500 | Train Loss: 9.8725 | Val Loss: 15.5292 | Val MAE: 15.529 | LR: 1.0e-04\n",
      "Epoch 193/500 | Train Loss: 9.6025 | Val Loss: 9.1592 | Val MAE: 9.159 | LR: 1.0e-04\n",
      "Epoch 194/500 | Train Loss: 9.7651 | Val Loss: 9.8954 | Val MAE: 9.895 | LR: 1.0e-04\n",
      "Epoch 195/500 | Train Loss: 9.8860 | Val Loss: 10.3934 | Val MAE: 10.393 | LR: 1.0e-04\n",
      "Epoch 196/500 | Train Loss: 9.9098 | Val Loss: 8.9651 | Val MAE: 8.965 | LR: 1.0e-04\n",
      "Epoch 197/500 | Train Loss: 9.5136 | Val Loss: 9.7337 | Val MAE: 9.734 | LR: 1.0e-04\n",
      "Epoch 198/500 | Train Loss: 9.8650 | Val Loss: 8.9852 | Val MAE: 8.985 | LR: 1.0e-04\n",
      "Epoch 199/500 | Train Loss: 9.5213 | Val Loss: 9.3083 | Val MAE: 9.308 | LR: 1.0e-04\n",
      "Epoch 200/500 | Train Loss: 10.0466 | Val Loss: 15.3593 | Val MAE: 15.359 | LR: 1.0e-04\n",
      "Epoch 201/500 | Train Loss: 9.7757 | Val Loss: 8.9001 | Val MAE: 8.900 | LR: 1.0e-04\n",
      "Epoch 202/500 | Train Loss: 9.8790 | Val Loss: 9.5700 | Val MAE: 9.570 | LR: 1.0e-04\n",
      "Epoch 203/500 | Train Loss: 9.5523 | Val Loss: 10.2908 | Val MAE: 10.291 | LR: 1.0e-04\n",
      "Epoch 204/500 | Train Loss: 9.7435 | Val Loss: 10.5571 | Val MAE: 10.557 | LR: 1.0e-04\n",
      "Epoch 205/500 | Train Loss: 9.6460 | Val Loss: 8.8608 | Val MAE: 8.861 | LR: 1.0e-04\n",
      "Epoch 206/500 | Train Loss: 9.4202 | Val Loss: 10.2790 | Val MAE: 10.279 | LR: 1.0e-04\n",
      "Epoch 207/500 | Train Loss: 9.7318 | Val Loss: 8.5049 | Val MAE: 8.505 | LR: 1.0e-04\n",
      "Epoch 208/500 | Train Loss: 9.8677 | Val Loss: 8.6901 | Val MAE: 8.690 | LR: 1.0e-04\n",
      "Epoch 209/500 | Train Loss: 9.7113 | Val Loss: 8.4308 | Val MAE: 8.431 | LR: 1.0e-04\n",
      "Epoch 210/500 | Train Loss: 9.6380 | Val Loss: 8.4528 | Val MAE: 8.453 | LR: 1.0e-04\n",
      "Epoch 211/500 | Train Loss: 9.5957 | Val Loss: 8.8063 | Val MAE: 8.806 | LR: 1.0e-04\n",
      "Epoch 212/500 | Train Loss: 9.7528 | Val Loss: 8.4979 | Val MAE: 8.498 | LR: 1.0e-04\n",
      "Epoch 213/500 | Train Loss: 9.8332 | Val Loss: 8.6461 | Val MAE: 8.646 | LR: 1.0e-04\n",
      "Epoch 214/500 | Train Loss: 9.7804 | Val Loss: 11.4235 | Val MAE: 11.424 | LR: 1.0e-04\n",
      "Epoch 215/500 | Train Loss: 9.6892 | Val Loss: 11.5769 | Val MAE: 11.577 | LR: 1.0e-04\n",
      "Epoch 216/500 | Train Loss: 9.7011 | Val Loss: 8.9823 | Val MAE: 8.982 | LR: 1.0e-04\n",
      "Epoch 217/500 | Train Loss: 9.5653 | Val Loss: 11.5993 | Val MAE: 11.599 | LR: 1.0e-04\n",
      "Epoch 218/500 | Train Loss: 9.6595 | Val Loss: 8.7426 | Val MAE: 8.743 | LR: 1.0e-04\n",
      "Epoch 219/500 | Train Loss: 9.8030 | Val Loss: 9.4787 | Val MAE: 9.479 | LR: 1.0e-04\n",
      "Epoch 220/500 | Train Loss: 9.7048 | Val Loss: 9.0000 | Val MAE: 9.000 | LR: 1.0e-04\n",
      "Epoch 221/500 | Train Loss: 9.4167 | Val Loss: 11.2730 | Val MAE: 11.273 | LR: 1.0e-04\n",
      "Epoch 222/500 | Train Loss: 9.6630 | Val Loss: 9.2865 | Val MAE: 9.286 | LR: 1.0e-04\n",
      "Epoch 223/500 | Train Loss: 9.4735 | Val Loss: 9.6802 | Val MAE: 9.680 | LR: 1.0e-04\n",
      "Epoch 224/500 | Train Loss: 9.7808 | Val Loss: 9.2949 | Val MAE: 9.295 | LR: 1.0e-04\n",
      "Epoch 225/500 | Train Loss: 9.2119 | Val Loss: 8.1287 | Val MAE: 8.129 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.129. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth\n",
      "Epoch 226/500 | Train Loss: 9.5927 | Val Loss: 9.8046 | Val MAE: 9.805 | LR: 1.0e-04\n",
      "Epoch 227/500 | Train Loss: 9.8005 | Val Loss: 8.7332 | Val MAE: 8.733 | LR: 1.0e-04\n",
      "Epoch 228/500 | Train Loss: 9.4828 | Val Loss: 8.5710 | Val MAE: 8.571 | LR: 1.0e-04\n",
      "Epoch 229/500 | Train Loss: 9.8162 | Val Loss: 11.1443 | Val MAE: 11.144 | LR: 1.0e-04\n",
      "Epoch 230/500 | Train Loss: 9.3574 | Val Loss: 8.8849 | Val MAE: 8.885 | LR: 1.0e-04\n",
      "Epoch 231/500 | Train Loss: 9.4790 | Val Loss: 8.4119 | Val MAE: 8.412 | LR: 1.0e-04\n",
      "Epoch 232/500 | Train Loss: 9.3411 | Val Loss: 8.2930 | Val MAE: 8.293 | LR: 1.0e-04\n",
      "Epoch 233/500 | Train Loss: 9.7215 | Val Loss: 10.3007 | Val MAE: 10.301 | LR: 1.0e-04\n",
      "Epoch 234/500 | Train Loss: 9.5826 | Val Loss: 8.6549 | Val MAE: 8.655 | LR: 1.0e-04\n",
      "Epoch 235/500 | Train Loss: 9.6051 | Val Loss: 8.4057 | Val MAE: 8.406 | LR: 1.0e-04\n",
      "Epoch 236/500 | Train Loss: 9.5129 | Val Loss: 9.8617 | Val MAE: 9.862 | LR: 1.0e-04\n",
      "Epoch 237/500 | Train Loss: 9.5177 | Val Loss: 10.0947 | Val MAE: 10.095 | LR: 1.0e-04\n",
      "Epoch 238/500 | Train Loss: 9.4744 | Val Loss: 9.1335 | Val MAE: 9.134 | LR: 1.0e-04\n",
      "Epoch 239/500 | Train Loss: 9.4713 | Val Loss: 9.1439 | Val MAE: 9.144 | LR: 1.0e-04\n",
      "Epoch 240/500 | Train Loss: 9.8296 | Val Loss: 9.1846 | Val MAE: 9.185 | LR: 1.0e-04\n",
      "Epoch 241/500 | Train Loss: 9.4918 | Val Loss: 9.1842 | Val MAE: 9.184 | LR: 1.0e-04\n",
      "Epoch 242/500 | Train Loss: 9.4955 | Val Loss: 8.9042 | Val MAE: 8.904 | LR: 1.0e-04\n",
      "Epoch 243/500 | Train Loss: 9.5408 | Val Loss: 8.7599 | Val MAE: 8.760 | LR: 1.0e-04\n",
      "Epoch 244/500 | Train Loss: 9.5927 | Val Loss: 10.0323 | Val MAE: 10.032 | LR: 1.0e-04\n",
      "Epoch 245/500 | Train Loss: 9.5292 | Val Loss: 9.2102 | Val MAE: 9.210 | LR: 1.0e-04\n",
      "Epoch 246/500 | Train Loss: 9.3228 | Val Loss: 9.9105 | Val MAE: 9.911 | LR: 1.0e-04\n",
      "Epoch 247/500 | Train Loss: 9.4860 | Val Loss: 9.0143 | Val MAE: 9.014 | LR: 1.0e-04\n",
      "Epoch 248/500 | Train Loss: 9.6547 | Val Loss: 9.8710 | Val MAE: 9.871 | LR: 1.0e-04\n",
      "Epoch 249/500 | Train Loss: 9.5733 | Val Loss: 8.6295 | Val MAE: 8.630 | LR: 1.0e-04\n",
      "Epoch 250/500 | Train Loss: 9.4959 | Val Loss: 8.5147 | Val MAE: 8.515 | LR: 1.0e-04\n",
      "Epoch 251/500 | Train Loss: 9.3239 | Val Loss: 8.8745 | Val MAE: 8.874 | LR: 1.0e-04\n",
      "Epoch 252/500 | Train Loss: 9.5718 | Val Loss: 9.1882 | Val MAE: 9.188 | LR: 1.0e-04\n",
      "Epoch 253/500 | Train Loss: 9.5750 | Val Loss: 8.8723 | Val MAE: 8.872 | LR: 1.0e-04\n",
      "Epoch 254/500 | Train Loss: 9.5522 | Val Loss: 8.9814 | Val MAE: 8.981 | LR: 1.0e-04\n",
      "Epoch 255/500 | Train Loss: 9.3501 | Val Loss: 11.0097 | Val MAE: 11.010 | LR: 1.0e-04\n",
      "Epoch 256/500 | Train Loss: 9.5504 | Val Loss: 8.3730 | Val MAE: 8.373 | LR: 1.0e-04\n",
      "Epoch 257/500 | Train Loss: 9.3775 | Val Loss: 8.2847 | Val MAE: 8.285 | LR: 1.0e-04\n",
      "Epoch 258/500 | Train Loss: 9.2607 | Val Loss: 9.5240 | Val MAE: 9.524 | LR: 1.0e-04\n",
      "Epoch 259/500 | Train Loss: 9.3919 | Val Loss: 11.3544 | Val MAE: 11.354 | LR: 1.0e-04\n",
      "Epoch 260/500 | Train Loss: 9.4101 | Val Loss: 9.0991 | Val MAE: 9.099 | LR: 1.0e-04\n",
      "Epoch 261/500 | Train Loss: 9.3057 | Val Loss: 8.6279 | Val MAE: 8.628 | LR: 1.0e-04\n",
      "Epoch 262/500 | Train Loss: 9.4389 | Val Loss: 10.1139 | Val MAE: 10.114 | LR: 1.0e-04\n",
      "Epoch 263/500 | Train Loss: 9.5398 | Val Loss: 8.5927 | Val MAE: 8.593 | LR: 1.0e-04\n",
      "Epoch 264/500 | Train Loss: 9.6766 | Val Loss: 8.5646 | Val MAE: 8.565 | LR: 1.0e-04\n",
      "Epoch 265/500 | Train Loss: 9.1819 | Val Loss: 8.5924 | Val MAE: 8.592 | LR: 1.0e-04\n",
      "Epoch 266/500 | Train Loss: 9.4151 | Val Loss: 9.0686 | Val MAE: 9.069 | LR: 1.0e-04\n",
      "Epoch 267/500 | Train Loss: 9.3761 | Val Loss: 10.5358 | Val MAE: 10.536 | LR: 1.0e-04\n",
      "Epoch 268/500 | Train Loss: 9.3056 | Val Loss: 8.4564 | Val MAE: 8.456 | LR: 1.0e-04\n",
      "Epoch 269/500 | Train Loss: 9.2959 | Val Loss: 10.7716 | Val MAE: 10.772 | LR: 1.0e-04\n",
      "Epoch 270/500 | Train Loss: 9.4881 | Val Loss: 9.0955 | Val MAE: 9.096 | LR: 1.0e-04\n",
      "Epoch 271/500 | Train Loss: 9.5081 | Val Loss: 10.3824 | Val MAE: 10.382 | LR: 1.0e-04\n",
      "Epoch 272/500 | Train Loss: 9.2865 | Val Loss: 8.4824 | Val MAE: 8.482 | LR: 1.0e-04\n",
      "Epoch 273/500 | Train Loss: 9.5486 | Val Loss: 12.7100 | Val MAE: 12.710 | LR: 1.0e-04\n",
      "Epoch 274/500 | Train Loss: 9.3421 | Val Loss: 8.3970 | Val MAE: 8.397 | LR: 1.0e-04\n",
      "Epoch 275/500 | Train Loss: 9.3952 | Val Loss: 11.4018 | Val MAE: 11.402 | LR: 1.0e-04\n",
      "\n",
      "Early stopping triggered for sagittal slice 80 after 50 epochs without improvement.\n",
      "\n",
      "--- Training Finished for sagittal Slice 80 ---\n",
      "Best Validation MAE: 8.129 achieved at epoch 225\n",
      "\n",
      "==================== Training for Slice: sagittal 125 ====================\n",
      "Using feature root: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice\n",
      "Setting up datasets...\n",
      "\n",
      "[Dataset Init] Split: train, Orientation: sagittal, Slice: 125\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/train\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 2274 subject directories in 'train' to CSV entries.\n",
      "Warning: Could not map 1 directories to CSV.\n",
      "Scanning 2275 potential subject directories for slice sagittal_125...\n",
      "Info: 1 subject directories were skipped as they couldn't be mapped to metadata.\n",
      "Found 2274 valid files for sagittal slice 125 in split train.\n",
      "\n",
      "[Dataset Init] Split: validation, Orientation: sagittal, Slice: 125\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/validation\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 280 subject directories in 'validation' to CSV entries.\n",
      "Scanning 280 potential subject directories for slice sagittal_125...\n",
      "Found 280 valid files for sagittal slice 125 in split validation.\n",
      "Setting up dataloaders...\n",
      "Initializing model, optimizer, scheduler...\n",
      "\n",
      "--- Starting Training for sagittal Slice 125 ---\n",
      "Epoch 1/500 | Train Loss: 54.2534 | Val Loss: 53.4086 | Val MAE: 53.409 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.409. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 2/500 | Train Loss: 54.1151 | Val Loss: 53.3412 | Val MAE: 53.341 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.341. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 3/500 | Train Loss: 54.0391 | Val Loss: 53.2265 | Val MAE: 53.227 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.227. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 4/500 | Train Loss: 53.9058 | Val Loss: 53.1975 | Val MAE: 53.198 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.198. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 5/500 | Train Loss: 53.7651 | Val Loss: 53.0833 | Val MAE: 53.083 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.083. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 6/500 | Train Loss: 53.6596 | Val Loss: 53.0349 | Val MAE: 53.035 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.035. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 7/500 | Train Loss: 53.5351 | Val Loss: 52.8780 | Val MAE: 52.878 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.878. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 8/500 | Train Loss: 53.4293 | Val Loss: 52.7871 | Val MAE: 52.787 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.787. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 9/500 | Train Loss: 53.2789 | Val Loss: 52.7583 | Val MAE: 52.758 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.758. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 10/500 | Train Loss: 53.1629 | Val Loss: 52.5658 | Val MAE: 52.566 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.566. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 11/500 | Train Loss: 53.0327 | Val Loss: 52.4724 | Val MAE: 52.472 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.472. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 12/500 | Train Loss: 52.9063 | Val Loss: 52.3598 | Val MAE: 52.360 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.360. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 13/500 | Train Loss: 52.7400 | Val Loss: 52.1679 | Val MAE: 52.168 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.168. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 14/500 | Train Loss: 52.5792 | Val Loss: 52.0775 | Val MAE: 52.077 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.077. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 15/500 | Train Loss: 52.4747 | Val Loss: 51.9799 | Val MAE: 51.980 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.980. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 16/500 | Train Loss: 52.3303 | Val Loss: 51.7469 | Val MAE: 51.747 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.747. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 17/500 | Train Loss: 52.1725 | Val Loss: 51.6622 | Val MAE: 51.662 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.662. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 18/500 | Train Loss: 51.9567 | Val Loss: 51.5516 | Val MAE: 51.552 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.552. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 19/500 | Train Loss: 51.8260 | Val Loss: 51.3386 | Val MAE: 51.339 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.339. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 20/500 | Train Loss: 51.6534 | Val Loss: 51.1832 | Val MAE: 51.183 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.183. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 21/500 | Train Loss: 51.4461 | Val Loss: 50.9891 | Val MAE: 50.989 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.989. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 22/500 | Train Loss: 51.2991 | Val Loss: 50.8894 | Val MAE: 50.889 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.889. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 23/500 | Train Loss: 51.0919 | Val Loss: 50.4995 | Val MAE: 50.499 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.499. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 24/500 | Train Loss: 50.8660 | Val Loss: 50.2942 | Val MAE: 50.294 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.294. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 25/500 | Train Loss: 50.6829 | Val Loss: 50.2797 | Val MAE: 50.280 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.280. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 26/500 | Train Loss: 50.5003 | Val Loss: 50.1443 | Val MAE: 50.144 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.144. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 27/500 | Train Loss: 50.2951 | Val Loss: 49.9220 | Val MAE: 49.922 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.922. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 28/500 | Train Loss: 50.0888 | Val Loss: 49.5200 | Val MAE: 49.520 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.520. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 29/500 | Train Loss: 49.8742 | Val Loss: 49.5251 | Val MAE: 49.525 | LR: 1.0e-04\n",
      "Epoch 30/500 | Train Loss: 49.6294 | Val Loss: 48.9453 | Val MAE: 48.945 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.945. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 31/500 | Train Loss: 49.4060 | Val Loss: 48.9893 | Val MAE: 48.989 | LR: 1.0e-04\n",
      "Epoch 32/500 | Train Loss: 49.1401 | Val Loss: 48.6543 | Val MAE: 48.654 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.654. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 33/500 | Train Loss: 48.9030 | Val Loss: 48.3508 | Val MAE: 48.351 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.351. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 34/500 | Train Loss: 48.6497 | Val Loss: 48.3989 | Val MAE: 48.399 | LR: 1.0e-04\n",
      "Epoch 35/500 | Train Loss: 48.4132 | Val Loss: 47.6089 | Val MAE: 47.609 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.609. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 36/500 | Train Loss: 48.0982 | Val Loss: 47.6921 | Val MAE: 47.692 | LR: 1.0e-04\n",
      "Epoch 37/500 | Train Loss: 47.8120 | Val Loss: 47.4125 | Val MAE: 47.413 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.413. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 38/500 | Train Loss: 47.5683 | Val Loss: 47.0825 | Val MAE: 47.082 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.082. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 39/500 | Train Loss: 47.2971 | Val Loss: 47.0035 | Val MAE: 47.004 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.004. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 40/500 | Train Loss: 46.8959 | Val Loss: 46.4015 | Val MAE: 46.402 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 46.402. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 41/500 | Train Loss: 46.6365 | Val Loss: 46.1207 | Val MAE: 46.121 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 46.121. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 42/500 | Train Loss: 46.2859 | Val Loss: 46.0754 | Val MAE: 46.075 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 46.075. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 43/500 | Train Loss: 45.9787 | Val Loss: 45.6866 | Val MAE: 45.687 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.687. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 44/500 | Train Loss: 45.6690 | Val Loss: 45.7309 | Val MAE: 45.731 | LR: 1.0e-04\n",
      "Epoch 45/500 | Train Loss: 45.3149 | Val Loss: 44.9901 | Val MAE: 44.990 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 44.990. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 46/500 | Train Loss: 44.9746 | Val Loss: 45.4005 | Val MAE: 45.401 | LR: 1.0e-04\n",
      "Epoch 47/500 | Train Loss: 44.6491 | Val Loss: 42.7479 | Val MAE: 42.748 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 42.748. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 48/500 | Train Loss: 44.2996 | Val Loss: 43.6006 | Val MAE: 43.601 | LR: 1.0e-04\n",
      "Epoch 49/500 | Train Loss: 43.9374 | Val Loss: 43.3963 | Val MAE: 43.396 | LR: 1.0e-04\n",
      "Epoch 50/500 | Train Loss: 43.5775 | Val Loss: 42.9839 | Val MAE: 42.984 | LR: 1.0e-04\n",
      "Epoch 51/500 | Train Loss: 43.2239 | Val Loss: 42.8211 | Val MAE: 42.821 | LR: 1.0e-04\n",
      "Epoch 52/500 | Train Loss: 42.8093 | Val Loss: 41.1237 | Val MAE: 41.124 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 41.124. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 53/500 | Train Loss: 42.4075 | Val Loss: 41.3783 | Val MAE: 41.378 | LR: 1.0e-04\n",
      "Epoch 54/500 | Train Loss: 42.0836 | Val Loss: 41.9866 | Val MAE: 41.987 | LR: 1.0e-04\n",
      "Epoch 55/500 | Train Loss: 41.6367 | Val Loss: 41.2673 | Val MAE: 41.267 | LR: 1.0e-04\n",
      "Epoch 56/500 | Train Loss: 41.3363 | Val Loss: 40.4365 | Val MAE: 40.436 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.436. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 57/500 | Train Loss: 41.0085 | Val Loss: 40.9959 | Val MAE: 40.996 | LR: 1.0e-04\n",
      "Epoch 58/500 | Train Loss: 40.5497 | Val Loss: 40.6994 | Val MAE: 40.699 | LR: 1.0e-04\n",
      "Epoch 59/500 | Train Loss: 40.2234 | Val Loss: 40.3060 | Val MAE: 40.306 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.306. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 60/500 | Train Loss: 39.8269 | Val Loss: 39.4874 | Val MAE: 39.487 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 39.487. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 61/500 | Train Loss: 39.3917 | Val Loss: 39.9433 | Val MAE: 39.943 | LR: 1.0e-04\n",
      "Epoch 62/500 | Train Loss: 39.1039 | Val Loss: 39.0369 | Val MAE: 39.037 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 39.037. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 63/500 | Train Loss: 38.6788 | Val Loss: 39.4505 | Val MAE: 39.450 | LR: 1.0e-04\n",
      "Epoch 64/500 | Train Loss: 38.3450 | Val Loss: 38.0697 | Val MAE: 38.070 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 38.070. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 65/500 | Train Loss: 37.9339 | Val Loss: 36.6482 | Val MAE: 36.648 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 36.648. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 66/500 | Train Loss: 37.4970 | Val Loss: 38.3991 | Val MAE: 38.399 | LR: 1.0e-04\n",
      "Epoch 67/500 | Train Loss: 37.0082 | Val Loss: 37.5280 | Val MAE: 37.528 | LR: 1.0e-04\n",
      "Epoch 68/500 | Train Loss: 36.6646 | Val Loss: 36.0439 | Val MAE: 36.044 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 36.044. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 69/500 | Train Loss: 36.2829 | Val Loss: 36.0704 | Val MAE: 36.070 | LR: 1.0e-04\n",
      "Epoch 70/500 | Train Loss: 35.7787 | Val Loss: 39.0682 | Val MAE: 39.068 | LR: 1.0e-04\n",
      "Epoch 71/500 | Train Loss: 35.4562 | Val Loss: 35.5113 | Val MAE: 35.511 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 35.511. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 72/500 | Train Loss: 35.0829 | Val Loss: 34.5121 | Val MAE: 34.512 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 34.512. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 73/500 | Train Loss: 34.6199 | Val Loss: 36.7270 | Val MAE: 36.727 | LR: 1.0e-04\n",
      "Epoch 74/500 | Train Loss: 34.1339 | Val Loss: 33.4106 | Val MAE: 33.411 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 33.411. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 75/500 | Train Loss: 33.5392 | Val Loss: 32.1010 | Val MAE: 32.101 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 32.101. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 76/500 | Train Loss: 33.2015 | Val Loss: 33.6189 | Val MAE: 33.619 | LR: 1.0e-04\n",
      "Epoch 77/500 | Train Loss: 32.7195 | Val Loss: 31.0867 | Val MAE: 31.087 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 31.087. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 78/500 | Train Loss: 32.2763 | Val Loss: 31.0291 | Val MAE: 31.029 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 31.029. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 79/500 | Train Loss: 31.7876 | Val Loss: 30.7883 | Val MAE: 30.788 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 30.788. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 80/500 | Train Loss: 31.4098 | Val Loss: 33.7413 | Val MAE: 33.741 | LR: 1.0e-04\n",
      "Epoch 81/500 | Train Loss: 30.9699 | Val Loss: 26.4058 | Val MAE: 26.406 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 26.406. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 82/500 | Train Loss: 30.6528 | Val Loss: 25.5019 | Val MAE: 25.502 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 25.502. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 83/500 | Train Loss: 30.1184 | Val Loss: 32.2405 | Val MAE: 32.241 | LR: 1.0e-04\n",
      "Epoch 84/500 | Train Loss: 29.8167 | Val Loss: 26.3983 | Val MAE: 26.398 | LR: 1.0e-04\n",
      "Epoch 85/500 | Train Loss: 29.1892 | Val Loss: 29.8812 | Val MAE: 29.881 | LR: 1.0e-04\n",
      "Epoch 86/500 | Train Loss: 28.8452 | Val Loss: 31.6449 | Val MAE: 31.645 | LR: 1.0e-04\n",
      "Epoch 87/500 | Train Loss: 28.4662 | Val Loss: 29.7965 | Val MAE: 29.796 | LR: 1.0e-04\n",
      "Epoch 88/500 | Train Loss: 27.8049 | Val Loss: 30.3702 | Val MAE: 30.370 | LR: 1.0e-04\n",
      "Epoch 89/500 | Train Loss: 27.1631 | Val Loss: 26.0669 | Val MAE: 26.067 | LR: 1.0e-04\n",
      "Epoch 90/500 | Train Loss: 26.8875 | Val Loss: 23.9922 | Val MAE: 23.992 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 23.992. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 91/500 | Train Loss: 26.3124 | Val Loss: 23.1390 | Val MAE: 23.139 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 23.139. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 92/500 | Train Loss: 26.0951 | Val Loss: 26.3254 | Val MAE: 26.325 | LR: 1.0e-04\n",
      "Epoch 93/500 | Train Loss: 25.4797 | Val Loss: 28.6383 | Val MAE: 28.638 | LR: 1.0e-04\n",
      "Epoch 94/500 | Train Loss: 24.9971 | Val Loss: 26.5193 | Val MAE: 26.519 | LR: 1.0e-04\n",
      "Epoch 95/500 | Train Loss: 24.5191 | Val Loss: 27.2924 | Val MAE: 27.292 | LR: 1.0e-04\n",
      "Epoch 96/500 | Train Loss: 24.0742 | Val Loss: 31.1428 | Val MAE: 31.143 | LR: 1.0e-04\n",
      "Epoch 97/500 | Train Loss: 23.6415 | Val Loss: 23.7492 | Val MAE: 23.749 | LR: 1.0e-04\n",
      "Epoch 98/500 | Train Loss: 23.2475 | Val Loss: 20.2193 | Val MAE: 20.219 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 20.219. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 99/500 | Train Loss: 22.7825 | Val Loss: 20.4509 | Val MAE: 20.451 | LR: 1.0e-04\n",
      "Epoch 100/500 | Train Loss: 22.3336 | Val Loss: 17.0802 | Val MAE: 17.080 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 17.080. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 101/500 | Train Loss: 21.7246 | Val Loss: 19.9301 | Val MAE: 19.930 | LR: 1.0e-04\n",
      "Epoch 102/500 | Train Loss: 21.2930 | Val Loss: 16.5907 | Val MAE: 16.591 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 16.591. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 103/500 | Train Loss: 21.0294 | Val Loss: 23.6899 | Val MAE: 23.690 | LR: 1.0e-04\n",
      "Epoch 104/500 | Train Loss: 20.6001 | Val Loss: 27.9430 | Val MAE: 27.943 | LR: 1.0e-04\n",
      "Epoch 105/500 | Train Loss: 20.1572 | Val Loss: 20.0766 | Val MAE: 20.077 | LR: 1.0e-04\n",
      "Epoch 106/500 | Train Loss: 19.7108 | Val Loss: 13.9951 | Val MAE: 13.995 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 13.995. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 107/500 | Train Loss: 19.1918 | Val Loss: 24.5569 | Val MAE: 24.557 | LR: 1.0e-04\n",
      "Epoch 108/500 | Train Loss: 18.9112 | Val Loss: 21.6804 | Val MAE: 21.680 | LR: 1.0e-04\n",
      "Epoch 109/500 | Train Loss: 18.3649 | Val Loss: 17.9368 | Val MAE: 17.937 | LR: 1.0e-04\n",
      "Epoch 110/500 | Train Loss: 17.8981 | Val Loss: 18.0528 | Val MAE: 18.053 | LR: 1.0e-04\n",
      "Epoch 111/500 | Train Loss: 17.5682 | Val Loss: 23.6287 | Val MAE: 23.629 | LR: 1.0e-04\n",
      "Epoch 112/500 | Train Loss: 17.4081 | Val Loss: 22.5301 | Val MAE: 22.530 | LR: 1.0e-04\n",
      "Epoch 113/500 | Train Loss: 16.9944 | Val Loss: 22.7891 | Val MAE: 22.789 | LR: 1.0e-04\n",
      "Epoch 114/500 | Train Loss: 16.8249 | Val Loss: 18.7894 | Val MAE: 18.789 | LR: 1.0e-04\n",
      "Epoch 115/500 | Train Loss: 16.4179 | Val Loss: 22.3692 | Val MAE: 22.369 | LR: 1.0e-04\n",
      "Epoch 116/500 | Train Loss: 16.0659 | Val Loss: 10.6697 | Val MAE: 10.670 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 10.670. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 117/500 | Train Loss: 15.7203 | Val Loss: 11.9321 | Val MAE: 11.932 | LR: 1.0e-04\n",
      "Epoch 118/500 | Train Loss: 15.5445 | Val Loss: 11.6086 | Val MAE: 11.609 | LR: 1.0e-04\n",
      "Epoch 119/500 | Train Loss: 15.0396 | Val Loss: 14.6125 | Val MAE: 14.613 | LR: 1.0e-04\n",
      "Epoch 120/500 | Train Loss: 14.7402 | Val Loss: 11.4061 | Val MAE: 11.406 | LR: 1.0e-04\n",
      "Epoch 121/500 | Train Loss: 14.3004 | Val Loss: 10.0869 | Val MAE: 10.087 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 10.087. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 122/500 | Train Loss: 14.4663 | Val Loss: 9.9936 | Val MAE: 9.994 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 9.994. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 123/500 | Train Loss: 14.0254 | Val Loss: 12.5280 | Val MAE: 12.528 | LR: 1.0e-04\n",
      "Epoch 124/500 | Train Loss: 13.6238 | Val Loss: 19.6970 | Val MAE: 19.697 | LR: 1.0e-04\n",
      "Epoch 125/500 | Train Loss: 13.4786 | Val Loss: 19.4865 | Val MAE: 19.486 | LR: 1.0e-04\n",
      "Epoch 126/500 | Train Loss: 13.4507 | Val Loss: 15.4735 | Val MAE: 15.473 | LR: 1.0e-04\n",
      "Epoch 127/500 | Train Loss: 13.0779 | Val Loss: 13.8997 | Val MAE: 13.900 | LR: 1.0e-04\n",
      "Epoch 128/500 | Train Loss: 12.8685 | Val Loss: 10.5677 | Val MAE: 10.568 | LR: 1.0e-04\n",
      "Epoch 129/500 | Train Loss: 12.6521 | Val Loss: 10.2171 | Val MAE: 10.217 | LR: 1.0e-04\n",
      "Epoch 130/500 | Train Loss: 12.2549 | Val Loss: 9.9144 | Val MAE: 9.914 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 9.914. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 131/500 | Train Loss: 12.5546 | Val Loss: 18.8021 | Val MAE: 18.802 | LR: 1.0e-04\n",
      "Epoch 132/500 | Train Loss: 12.0572 | Val Loss: 15.2829 | Val MAE: 15.283 | LR: 1.0e-04\n",
      "Epoch 133/500 | Train Loss: 11.8527 | Val Loss: 9.4847 | Val MAE: 9.485 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 9.485. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 134/500 | Train Loss: 11.8903 | Val Loss: 12.0527 | Val MAE: 12.053 | LR: 1.0e-04\n",
      "Epoch 135/500 | Train Loss: 11.6915 | Val Loss: 19.3814 | Val MAE: 19.381 | LR: 1.0e-04\n",
      "Epoch 136/500 | Train Loss: 11.9021 | Val Loss: 11.0159 | Val MAE: 11.016 | LR: 1.0e-04\n",
      "Epoch 137/500 | Train Loss: 11.4408 | Val Loss: 12.8073 | Val MAE: 12.807 | LR: 1.0e-04\n",
      "Epoch 138/500 | Train Loss: 11.1706 | Val Loss: 8.7728 | Val MAE: 8.773 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.773. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 139/500 | Train Loss: 11.3782 | Val Loss: 16.3175 | Val MAE: 16.317 | LR: 1.0e-04\n",
      "Epoch 140/500 | Train Loss: 11.2651 | Val Loss: 13.4054 | Val MAE: 13.405 | LR: 1.0e-04\n",
      "Epoch 141/500 | Train Loss: 11.1707 | Val Loss: 11.6269 | Val MAE: 11.627 | LR: 1.0e-04\n",
      "Epoch 142/500 | Train Loss: 11.2456 | Val Loss: 11.4837 | Val MAE: 11.484 | LR: 1.0e-04\n",
      "Epoch 143/500 | Train Loss: 11.1090 | Val Loss: 8.9096 | Val MAE: 8.910 | LR: 1.0e-04\n",
      "Epoch 144/500 | Train Loss: 11.1704 | Val Loss: 9.5520 | Val MAE: 9.552 | LR: 1.0e-04\n",
      "Epoch 145/500 | Train Loss: 11.1753 | Val Loss: 10.8542 | Val MAE: 10.854 | LR: 1.0e-04\n",
      "Epoch 146/500 | Train Loss: 10.6461 | Val Loss: 9.6908 | Val MAE: 9.691 | LR: 1.0e-04\n",
      "Epoch 147/500 | Train Loss: 10.7415 | Val Loss: 15.7837 | Val MAE: 15.784 | LR: 1.0e-04\n",
      "Epoch 148/500 | Train Loss: 10.5823 | Val Loss: 8.5683 | Val MAE: 8.568 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.568. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 149/500 | Train Loss: 10.8854 | Val Loss: 11.8031 | Val MAE: 11.803 | LR: 1.0e-04\n",
      "Epoch 150/500 | Train Loss: 10.8425 | Val Loss: 8.5064 | Val MAE: 8.506 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.506. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 151/500 | Train Loss: 10.7720 | Val Loss: 10.0945 | Val MAE: 10.094 | LR: 1.0e-04\n",
      "Epoch 152/500 | Train Loss: 10.7003 | Val Loss: 8.6514 | Val MAE: 8.651 | LR: 1.0e-04\n",
      "Epoch 153/500 | Train Loss: 10.6261 | Val Loss: 9.9474 | Val MAE: 9.947 | LR: 1.0e-04\n",
      "Epoch 154/500 | Train Loss: 10.6604 | Val Loss: 10.2467 | Val MAE: 10.247 | LR: 1.0e-04\n",
      "Epoch 155/500 | Train Loss: 10.8543 | Val Loss: 8.9629 | Val MAE: 8.963 | LR: 1.0e-04\n",
      "Epoch 156/500 | Train Loss: 10.5173 | Val Loss: 9.8001 | Val MAE: 9.800 | LR: 1.0e-04\n",
      "Epoch 157/500 | Train Loss: 10.6236 | Val Loss: 11.9712 | Val MAE: 11.971 | LR: 1.0e-04\n",
      "Epoch 158/500 | Train Loss: 10.5707 | Val Loss: 7.7787 | Val MAE: 7.779 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.779. Saved model to specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth\n",
      "Epoch 159/500 | Train Loss: 10.4428 | Val Loss: 13.7471 | Val MAE: 13.747 | LR: 1.0e-04\n",
      "Epoch 160/500 | Train Loss: 10.7572 | Val Loss: 10.7451 | Val MAE: 10.745 | LR: 1.0e-04\n",
      "Epoch 161/500 | Train Loss: 10.6719 | Val Loss: 8.5710 | Val MAE: 8.571 | LR: 1.0e-04\n",
      "Epoch 162/500 | Train Loss: 10.3729 | Val Loss: 8.2533 | Val MAE: 8.253 | LR: 1.0e-04\n",
      "Epoch 163/500 | Train Loss: 10.6049 | Val Loss: 10.9408 | Val MAE: 10.941 | LR: 1.0e-04\n",
      "Epoch 164/500 | Train Loss: 10.6375 | Val Loss: 9.4176 | Val MAE: 9.418 | LR: 1.0e-04\n",
      "Epoch 165/500 | Train Loss: 10.4264 | Val Loss: 8.7503 | Val MAE: 8.750 | LR: 1.0e-04\n",
      "Epoch 166/500 | Train Loss: 10.1610 | Val Loss: 12.2474 | Val MAE: 12.247 | LR: 1.0e-04\n",
      "Epoch 167/500 | Train Loss: 10.4598 | Val Loss: 7.8522 | Val MAE: 7.852 | LR: 1.0e-04\n",
      "Epoch 168/500 | Train Loss: 10.2788 | Val Loss: 8.0703 | Val MAE: 8.070 | LR: 1.0e-04\n",
      "Epoch 169/500 | Train Loss: 10.3526 | Val Loss: 9.4326 | Val MAE: 9.433 | LR: 1.0e-04\n",
      "Epoch 170/500 | Train Loss: 10.3665 | Val Loss: 8.2935 | Val MAE: 8.293 | LR: 1.0e-04\n",
      "Epoch 171/500 | Train Loss: 10.4845 | Val Loss: 9.5717 | Val MAE: 9.572 | LR: 1.0e-04\n",
      "Epoch 172/500 | Train Loss: 10.3235 | Val Loss: 11.0741 | Val MAE: 11.074 | LR: 1.0e-04\n",
      "Epoch 173/500 | Train Loss: 10.2217 | Val Loss: 15.3044 | Val MAE: 15.304 | LR: 1.0e-04\n",
      "Epoch 174/500 | Train Loss: 10.4433 | Val Loss: 9.5397 | Val MAE: 9.540 | LR: 1.0e-04\n",
      "Epoch 175/500 | Train Loss: 10.3233 | Val Loss: 7.9438 | Val MAE: 7.944 | LR: 1.0e-04\n",
      "Epoch 176/500 | Train Loss: 10.0388 | Val Loss: 8.5247 | Val MAE: 8.525 | LR: 1.0e-04\n",
      "Epoch 177/500 | Train Loss: 10.1064 | Val Loss: 9.0302 | Val MAE: 9.030 | LR: 1.0e-04\n",
      "Epoch 178/500 | Train Loss: 10.5223 | Val Loss: 8.3511 | Val MAE: 8.351 | LR: 1.0e-04\n",
      "Epoch 179/500 | Train Loss: 10.1760 | Val Loss: 9.3484 | Val MAE: 9.348 | LR: 1.0e-04\n",
      "Epoch 180/500 | Train Loss: 10.3350 | Val Loss: 10.5434 | Val MAE: 10.543 | LR: 1.0e-04\n",
      "Epoch 181/500 | Train Loss: 10.2338 | Val Loss: 8.8452 | Val MAE: 8.845 | LR: 1.0e-04\n",
      "Epoch 182/500 | Train Loss: 10.0883 | Val Loss: 8.6704 | Val MAE: 8.670 | LR: 1.0e-04\n",
      "Epoch 183/500 | Train Loss: 10.3523 | Val Loss: 8.7107 | Val MAE: 8.711 | LR: 1.0e-04\n",
      "Epoch 184/500 | Train Loss: 10.1143 | Val Loss: 8.9123 | Val MAE: 8.912 | LR: 1.0e-04\n",
      "Epoch 185/500 | Train Loss: 10.0483 | Val Loss: 10.7711 | Val MAE: 10.771 | LR: 1.0e-04\n",
      "Epoch 186/500 | Train Loss: 9.8436 | Val Loss: 11.8680 | Val MAE: 11.868 | LR: 1.0e-04\n",
      "Epoch 187/500 | Train Loss: 10.2419 | Val Loss: 14.3292 | Val MAE: 14.329 | LR: 1.0e-04\n",
      "Epoch 188/500 | Train Loss: 10.2089 | Val Loss: 7.9610 | Val MAE: 7.961 | LR: 1.0e-04\n",
      "Epoch 189/500 | Train Loss: 10.2915 | Val Loss: 10.5353 | Val MAE: 10.535 | LR: 1.0e-04\n",
      "Epoch 190/500 | Train Loss: 10.3183 | Val Loss: 8.9824 | Val MAE: 8.982 | LR: 1.0e-04\n",
      "Epoch 191/500 | Train Loss: 10.1518 | Val Loss: 11.2633 | Val MAE: 11.263 | LR: 1.0e-04\n",
      "Epoch 192/500 | Train Loss: 10.0833 | Val Loss: 7.9259 | Val MAE: 7.926 | LR: 1.0e-04\n",
      "Epoch 193/500 | Train Loss: 9.9622 | Val Loss: 7.9866 | Val MAE: 7.987 | LR: 1.0e-04\n",
      "Epoch 194/500 | Train Loss: 9.9956 | Val Loss: 10.3674 | Val MAE: 10.367 | LR: 1.0e-04\n",
      "Epoch 195/500 | Train Loss: 9.8875 | Val Loss: 17.0978 | Val MAE: 17.098 | LR: 1.0e-04\n",
      "Epoch 196/500 | Train Loss: 9.9940 | Val Loss: 7.8061 | Val MAE: 7.806 | LR: 1.0e-04\n",
      "Epoch 197/500 | Train Loss: 10.1123 | Val Loss: 12.2615 | Val MAE: 12.262 | LR: 1.0e-04\n",
      "Epoch 198/500 | Train Loss: 10.1042 | Val Loss: 8.6906 | Val MAE: 8.691 | LR: 1.0e-04\n",
      "Epoch 199/500 | Train Loss: 10.0959 | Val Loss: 9.8566 | Val MAE: 9.857 | LR: 1.0e-04\n",
      "Epoch 200/500 | Train Loss: 9.8611 | Val Loss: 7.9390 | Val MAE: 7.939 | LR: 1.0e-04\n",
      "Epoch 201/500 | Train Loss: 10.3549 | Val Loss: 13.3558 | Val MAE: 13.356 | LR: 1.0e-04\n",
      "Epoch 202/500 | Train Loss: 9.9160 | Val Loss: 8.6507 | Val MAE: 8.651 | LR: 1.0e-04\n",
      "Epoch 203/500 | Train Loss: 10.2080 | Val Loss: 9.3409 | Val MAE: 9.341 | LR: 1.0e-04\n",
      "Epoch 204/500 | Train Loss: 10.0209 | Val Loss: 8.3648 | Val MAE: 8.365 | LR: 1.0e-04\n",
      "Epoch 205/500 | Train Loss: 10.0205 | Val Loss: 10.4193 | Val MAE: 10.419 | LR: 1.0e-04\n",
      "Epoch 206/500 | Train Loss: 9.9165 | Val Loss: 12.2112 | Val MAE: 12.211 | LR: 1.0e-04\n",
      "Epoch 207/500 | Train Loss: 10.0779 | Val Loss: 8.8846 | Val MAE: 8.885 | LR: 1.0e-04\n",
      "Epoch 208/500 | Train Loss: 9.5792 | Val Loss: 7.7880 | Val MAE: 7.788 | LR: 1.0e-04\n",
      "\n",
      "Early stopping triggered for sagittal slice 125 after 50 epochs without improvement.\n",
      "\n",
      "--- Training Finished for sagittal Slice 125 ---\n",
      "Best Validation MAE: 7.779 achieved at epoch 158\n",
      "\n",
      "==================== Training for Slice: coronal 125 ====================\n",
      "Using feature root: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice\n",
      "Setting up datasets...\n",
      "\n",
      "[Dataset Init] Split: train, Orientation: coronal, Slice: 125\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/train\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 2274 subject directories in 'train' to CSV entries.\n",
      "Warning: Could not map 1 directories to CSV.\n",
      "Scanning 2275 potential subject directories for slice coronal_125...\n",
      "Info: 1 subject directories were skipped as they couldn't be mapped to metadata.\n",
      "Found 2274 valid files for coronal slice 125 in split train.\n",
      "\n",
      "[Dataset Init] Split: validation, Orientation: coronal, Slice: 125\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/validation\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 280 subject directories in 'validation' to CSV entries.\n",
      "Scanning 280 potential subject directories for slice coronal_125...\n",
      "Found 280 valid files for coronal slice 125 in split validation.\n",
      "Setting up dataloaders...\n",
      "Initializing model, optimizer, scheduler...\n",
      "\n",
      "--- Starting Training for coronal Slice 125 ---\n",
      "Epoch 1/500 | Train Loss: 54.1163 | Val Loss: 53.2767 | Val MAE: 53.277 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.277. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 2/500 | Train Loss: 53.9792 | Val Loss: 53.2039 | Val MAE: 53.204 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.204. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 3/500 | Train Loss: 53.8636 | Val Loss: 53.1756 | Val MAE: 53.176 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.176. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 4/500 | Train Loss: 53.7720 | Val Loss: 53.0758 | Val MAE: 53.076 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.076. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 5/500 | Train Loss: 53.6321 | Val Loss: 53.0220 | Val MAE: 53.022 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.022. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 6/500 | Train Loss: 53.4736 | Val Loss: 52.9341 | Val MAE: 52.934 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.934. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 7/500 | Train Loss: 53.3783 | Val Loss: 52.7681 | Val MAE: 52.768 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.768. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 8/500 | Train Loss: 53.2560 | Val Loss: 52.7277 | Val MAE: 52.728 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.728. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 9/500 | Train Loss: 53.1222 | Val Loss: 52.5731 | Val MAE: 52.573 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.573. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 10/500 | Train Loss: 52.9789 | Val Loss: 52.4799 | Val MAE: 52.480 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.480. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 11/500 | Train Loss: 52.8677 | Val Loss: 52.3491 | Val MAE: 52.349 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.349. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 12/500 | Train Loss: 52.7385 | Val Loss: 52.2484 | Val MAE: 52.248 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.248. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 13/500 | Train Loss: 52.5679 | Val Loss: 52.0897 | Val MAE: 52.090 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.090. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 14/500 | Train Loss: 52.3990 | Val Loss: 52.0990 | Val MAE: 52.099 | LR: 1.0e-04\n",
      "Epoch 15/500 | Train Loss: 52.3292 | Val Loss: 51.9009 | Val MAE: 51.901 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.901. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 16/500 | Train Loss: 52.1284 | Val Loss: 51.7447 | Val MAE: 51.745 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.745. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 17/500 | Train Loss: 51.9903 | Val Loss: 51.6572 | Val MAE: 51.657 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.657. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 18/500 | Train Loss: 51.8467 | Val Loss: 51.3797 | Val MAE: 51.380 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.380. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 19/500 | Train Loss: 51.6239 | Val Loss: 51.1944 | Val MAE: 51.194 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.194. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 20/500 | Train Loss: 51.4586 | Val Loss: 50.9903 | Val MAE: 50.990 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.990. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 21/500 | Train Loss: 51.2871 | Val Loss: 50.8182 | Val MAE: 50.818 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.818. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 22/500 | Train Loss: 51.0935 | Val Loss: 50.5757 | Val MAE: 50.576 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.576. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 23/500 | Train Loss: 50.9316 | Val Loss: 50.4591 | Val MAE: 50.459 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.459. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 24/500 | Train Loss: 50.6576 | Val Loss: 50.2902 | Val MAE: 50.290 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.290. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 25/500 | Train Loss: 50.4925 | Val Loss: 49.9749 | Val MAE: 49.975 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.975. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 26/500 | Train Loss: 50.2885 | Val Loss: 49.8627 | Val MAE: 49.863 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.863. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 27/500 | Train Loss: 50.0658 | Val Loss: 49.5971 | Val MAE: 49.597 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.597. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 28/500 | Train Loss: 49.7832 | Val Loss: 49.4676 | Val MAE: 49.468 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.468. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 29/500 | Train Loss: 49.5629 | Val Loss: 49.2256 | Val MAE: 49.226 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.226. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 30/500 | Train Loss: 49.3777 | Val Loss: 48.8316 | Val MAE: 48.832 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.832. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 31/500 | Train Loss: 49.1274 | Val Loss: 48.7975 | Val MAE: 48.798 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.798. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 32/500 | Train Loss: 48.8808 | Val Loss: 47.8764 | Val MAE: 47.876 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.876. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 33/500 | Train Loss: 48.5823 | Val Loss: 48.1159 | Val MAE: 48.116 | LR: 1.0e-04\n",
      "Epoch 34/500 | Train Loss: 48.3518 | Val Loss: 47.6613 | Val MAE: 47.661 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.661. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 35/500 | Train Loss: 48.0659 | Val Loss: 47.7971 | Val MAE: 47.797 | LR: 1.0e-04\n",
      "Epoch 36/500 | Train Loss: 47.7281 | Val Loss: 47.4814 | Val MAE: 47.481 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.481. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 37/500 | Train Loss: 47.4971 | Val Loss: 47.0011 | Val MAE: 47.001 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.001. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 38/500 | Train Loss: 47.2012 | Val Loss: 46.9635 | Val MAE: 46.963 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 46.963. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 39/500 | Train Loss: 46.9202 | Val Loss: 46.2094 | Val MAE: 46.209 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 46.209. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 40/500 | Train Loss: 46.5596 | Val Loss: 45.8329 | Val MAE: 45.833 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.833. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 41/500 | Train Loss: 46.2720 | Val Loss: 45.5381 | Val MAE: 45.538 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.538. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 42/500 | Train Loss: 45.9083 | Val Loss: 45.6901 | Val MAE: 45.690 | LR: 1.0e-04\n",
      "Epoch 43/500 | Train Loss: 45.6470 | Val Loss: 44.5053 | Val MAE: 44.505 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 44.505. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 44/500 | Train Loss: 45.3002 | Val Loss: 45.8196 | Val MAE: 45.820 | LR: 1.0e-04\n",
      "Epoch 45/500 | Train Loss: 45.0261 | Val Loss: 44.4916 | Val MAE: 44.492 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 44.492. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 46/500 | Train Loss: 44.5632 | Val Loss: 44.0184 | Val MAE: 44.018 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 44.018. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 47/500 | Train Loss: 44.2521 | Val Loss: 43.6076 | Val MAE: 43.608 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 43.608. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 48/500 | Train Loss: 43.8892 | Val Loss: 42.9227 | Val MAE: 42.923 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 42.923. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 49/500 | Train Loss: 43.5737 | Val Loss: 43.2123 | Val MAE: 43.212 | LR: 1.0e-04\n",
      "Epoch 50/500 | Train Loss: 43.1744 | Val Loss: 42.3031 | Val MAE: 42.303 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 42.303. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 51/500 | Train Loss: 42.8218 | Val Loss: 42.7976 | Val MAE: 42.798 | LR: 1.0e-04\n",
      "Epoch 52/500 | Train Loss: 42.4528 | Val Loss: 41.9914 | Val MAE: 41.991 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 41.991. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 53/500 | Train Loss: 42.0413 | Val Loss: 41.4157 | Val MAE: 41.416 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 41.416. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 54/500 | Train Loss: 41.7611 | Val Loss: 40.4338 | Val MAE: 40.434 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.434. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 55/500 | Train Loss: 41.4262 | Val Loss: 40.4650 | Val MAE: 40.465 | LR: 1.0e-04\n",
      "Epoch 56/500 | Train Loss: 41.0611 | Val Loss: 40.7898 | Val MAE: 40.790 | LR: 1.0e-04\n",
      "Epoch 57/500 | Train Loss: 40.6668 | Val Loss: 40.2344 | Val MAE: 40.234 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.234. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 58/500 | Train Loss: 40.3095 | Val Loss: 39.7110 | Val MAE: 39.711 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 39.711. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 59/500 | Train Loss: 39.8992 | Val Loss: 39.2862 | Val MAE: 39.286 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 39.286. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 60/500 | Train Loss: 39.5989 | Val Loss: 39.3748 | Val MAE: 39.375 | LR: 1.0e-04\n",
      "Epoch 61/500 | Train Loss: 39.0767 | Val Loss: 37.8381 | Val MAE: 37.838 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 37.838. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 62/500 | Train Loss: 38.7651 | Val Loss: 38.3177 | Val MAE: 38.318 | LR: 1.0e-04\n",
      "Epoch 63/500 | Train Loss: 38.3351 | Val Loss: 37.6407 | Val MAE: 37.641 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 37.641. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 64/500 | Train Loss: 37.9750 | Val Loss: 36.5890 | Val MAE: 36.589 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 36.589. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 65/500 | Train Loss: 37.6197 | Val Loss: 38.4002 | Val MAE: 38.400 | LR: 1.0e-04\n",
      "Epoch 66/500 | Train Loss: 37.1816 | Val Loss: 37.3567 | Val MAE: 37.357 | LR: 1.0e-04\n",
      "Epoch 67/500 | Train Loss: 36.7717 | Val Loss: 36.8658 | Val MAE: 36.866 | LR: 1.0e-04\n",
      "Epoch 68/500 | Train Loss: 36.3475 | Val Loss: 36.0807 | Val MAE: 36.081 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 36.081. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 69/500 | Train Loss: 35.9556 | Val Loss: 35.2493 | Val MAE: 35.249 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 35.249. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 70/500 | Train Loss: 35.4850 | Val Loss: 36.8577 | Val MAE: 36.858 | LR: 1.0e-04\n",
      "Epoch 71/500 | Train Loss: 34.9960 | Val Loss: 36.2003 | Val MAE: 36.200 | LR: 1.0e-04\n",
      "Epoch 72/500 | Train Loss: 34.6717 | Val Loss: 33.2356 | Val MAE: 33.236 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 33.236. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 73/500 | Train Loss: 34.2153 | Val Loss: 32.7881 | Val MAE: 32.788 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 32.788. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 74/500 | Train Loss: 33.7700 | Val Loss: 36.4361 | Val MAE: 36.436 | LR: 1.0e-04\n",
      "Epoch 75/500 | Train Loss: 33.4344 | Val Loss: 32.9609 | Val MAE: 32.961 | LR: 1.0e-04\n",
      "Epoch 76/500 | Train Loss: 32.8864 | Val Loss: 33.4429 | Val MAE: 33.443 | LR: 1.0e-04\n",
      "Epoch 77/500 | Train Loss: 32.5437 | Val Loss: 30.1533 | Val MAE: 30.153 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 30.153. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 78/500 | Train Loss: 32.0863 | Val Loss: 28.7803 | Val MAE: 28.780 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 28.780. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 79/500 | Train Loss: 31.5361 | Val Loss: 27.6169 | Val MAE: 27.617 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 27.617. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 80/500 | Train Loss: 31.2380 | Val Loss: 32.3367 | Val MAE: 32.337 | LR: 1.0e-04\n",
      "Epoch 81/500 | Train Loss: 30.6505 | Val Loss: 33.6234 | Val MAE: 33.623 | LR: 1.0e-04\n",
      "Epoch 82/500 | Train Loss: 30.1247 | Val Loss: 35.3841 | Val MAE: 35.384 | LR: 1.0e-04\n",
      "Epoch 83/500 | Train Loss: 29.7729 | Val Loss: 25.0509 | Val MAE: 25.051 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 25.051. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 84/500 | Train Loss: 29.2543 | Val Loss: 27.1477 | Val MAE: 27.148 | LR: 1.0e-04\n",
      "Epoch 85/500 | Train Loss: 28.6896 | Val Loss: 32.8010 | Val MAE: 32.801 | LR: 1.0e-04\n",
      "Epoch 86/500 | Train Loss: 28.2537 | Val Loss: 26.7247 | Val MAE: 26.725 | LR: 1.0e-04\n",
      "Epoch 87/500 | Train Loss: 27.9075 | Val Loss: 23.1705 | Val MAE: 23.170 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 23.170. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 88/500 | Train Loss: 27.2556 | Val Loss: 28.1336 | Val MAE: 28.134 | LR: 1.0e-04\n",
      "Epoch 89/500 | Train Loss: 26.8881 | Val Loss: 27.4302 | Val MAE: 27.430 | LR: 1.0e-04\n",
      "Epoch 90/500 | Train Loss: 26.5467 | Val Loss: 24.1226 | Val MAE: 24.123 | LR: 1.0e-04\n",
      "Epoch 91/500 | Train Loss: 26.1465 | Val Loss: 22.0047 | Val MAE: 22.005 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 22.005. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 92/500 | Train Loss: 25.7249 | Val Loss: 22.2752 | Val MAE: 22.275 | LR: 1.0e-04\n",
      "Epoch 93/500 | Train Loss: 25.1708 | Val Loss: 27.9203 | Val MAE: 27.920 | LR: 1.0e-04\n",
      "Epoch 94/500 | Train Loss: 24.6331 | Val Loss: 23.6065 | Val MAE: 23.606 | LR: 1.0e-04\n",
      "Epoch 95/500 | Train Loss: 24.3565 | Val Loss: 25.2253 | Val MAE: 25.225 | LR: 1.0e-04\n",
      "Epoch 96/500 | Train Loss: 23.7383 | Val Loss: 22.5118 | Val MAE: 22.512 | LR: 1.0e-04\n",
      "Epoch 97/500 | Train Loss: 23.2894 | Val Loss: 23.8654 | Val MAE: 23.865 | LR: 1.0e-04\n",
      "Epoch 98/500 | Train Loss: 22.9219 | Val Loss: 19.4779 | Val MAE: 19.478 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 19.478. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 99/500 | Train Loss: 22.4974 | Val Loss: 25.0865 | Val MAE: 25.087 | LR: 1.0e-04\n",
      "Epoch 100/500 | Train Loss: 21.9497 | Val Loss: 20.8464 | Val MAE: 20.846 | LR: 1.0e-04\n",
      "Epoch 101/500 | Train Loss: 21.6445 | Val Loss: 19.3001 | Val MAE: 19.300 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 19.300. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 102/500 | Train Loss: 21.3529 | Val Loss: 21.1950 | Val MAE: 21.195 | LR: 1.0e-04\n",
      "Epoch 103/500 | Train Loss: 20.7384 | Val Loss: 20.4563 | Val MAE: 20.456 | LR: 1.0e-04\n",
      "Epoch 104/500 | Train Loss: 20.1110 | Val Loss: 13.0308 | Val MAE: 13.031 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 13.031. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 105/500 | Train Loss: 20.0321 | Val Loss: 21.9064 | Val MAE: 21.906 | LR: 1.0e-04\n",
      "Epoch 106/500 | Train Loss: 19.4625 | Val Loss: 18.1826 | Val MAE: 18.183 | LR: 1.0e-04\n",
      "Epoch 107/500 | Train Loss: 19.0537 | Val Loss: 21.6353 | Val MAE: 21.635 | LR: 1.0e-04\n",
      "Epoch 108/500 | Train Loss: 18.4739 | Val Loss: 14.1476 | Val MAE: 14.148 | LR: 1.0e-04\n",
      "Epoch 109/500 | Train Loss: 18.1543 | Val Loss: 14.3531 | Val MAE: 14.353 | LR: 1.0e-04\n",
      "Epoch 110/500 | Train Loss: 17.6922 | Val Loss: 21.3113 | Val MAE: 21.311 | LR: 1.0e-04\n",
      "Epoch 111/500 | Train Loss: 17.5177 | Val Loss: 23.7860 | Val MAE: 23.786 | LR: 1.0e-04\n",
      "Epoch 112/500 | Train Loss: 16.9970 | Val Loss: 14.6056 | Val MAE: 14.606 | LR: 1.0e-04\n",
      "Epoch 113/500 | Train Loss: 16.4220 | Val Loss: 14.6052 | Val MAE: 14.605 | LR: 1.0e-04\n",
      "Epoch 114/500 | Train Loss: 16.1131 | Val Loss: 18.1155 | Val MAE: 18.115 | LR: 1.0e-04\n",
      "Epoch 115/500 | Train Loss: 15.9874 | Val Loss: 14.3983 | Val MAE: 14.398 | LR: 1.0e-04\n",
      "Epoch 116/500 | Train Loss: 15.6078 | Val Loss: 15.6073 | Val MAE: 15.607 | LR: 1.0e-04\n",
      "Epoch 117/500 | Train Loss: 15.1930 | Val Loss: 13.2198 | Val MAE: 13.220 | LR: 1.0e-04\n",
      "Epoch 118/500 | Train Loss: 15.0212 | Val Loss: 10.7907 | Val MAE: 10.791 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 10.791. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 119/500 | Train Loss: 14.6098 | Val Loss: 22.2590 | Val MAE: 22.259 | LR: 1.0e-04\n",
      "Epoch 120/500 | Train Loss: 14.3440 | Val Loss: 10.1633 | Val MAE: 10.163 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 10.163. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 121/500 | Train Loss: 14.1811 | Val Loss: 24.1368 | Val MAE: 24.137 | LR: 1.0e-04\n",
      "Epoch 122/500 | Train Loss: 13.6937 | Val Loss: 11.7742 | Val MAE: 11.774 | LR: 1.0e-04\n",
      "Epoch 123/500 | Train Loss: 13.6885 | Val Loss: 12.5849 | Val MAE: 12.585 | LR: 1.0e-04\n",
      "Epoch 124/500 | Train Loss: 13.1849 | Val Loss: 17.1513 | Val MAE: 17.151 | LR: 1.0e-04\n",
      "Epoch 125/500 | Train Loss: 13.0716 | Val Loss: 12.2580 | Val MAE: 12.258 | LR: 1.0e-04\n",
      "Epoch 126/500 | Train Loss: 12.8228 | Val Loss: 18.2460 | Val MAE: 18.246 | LR: 1.0e-04\n",
      "Epoch 127/500 | Train Loss: 12.7986 | Val Loss: 14.4938 | Val MAE: 14.494 | LR: 1.0e-04\n",
      "Epoch 128/500 | Train Loss: 12.3768 | Val Loss: 15.3908 | Val MAE: 15.391 | LR: 1.0e-04\n",
      "Epoch 129/500 | Train Loss: 12.0884 | Val Loss: 9.1225 | Val MAE: 9.123 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 9.123. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 130/500 | Train Loss: 11.7510 | Val Loss: 11.7783 | Val MAE: 11.778 | LR: 1.0e-04\n",
      "Epoch 131/500 | Train Loss: 11.8447 | Val Loss: 9.0267 | Val MAE: 9.027 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 9.027. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 132/500 | Train Loss: 11.5822 | Val Loss: 9.8349 | Val MAE: 9.835 | LR: 1.0e-04\n",
      "Epoch 133/500 | Train Loss: 11.6686 | Val Loss: 9.7372 | Val MAE: 9.737 | LR: 1.0e-04\n",
      "Epoch 134/500 | Train Loss: 11.5860 | Val Loss: 11.4581 | Val MAE: 11.458 | LR: 1.0e-04\n",
      "Epoch 135/500 | Train Loss: 11.3792 | Val Loss: 10.2743 | Val MAE: 10.274 | LR: 1.0e-04\n",
      "Epoch 136/500 | Train Loss: 11.2920 | Val Loss: 10.6726 | Val MAE: 10.673 | LR: 1.0e-04\n",
      "Epoch 137/500 | Train Loss: 11.2396 | Val Loss: 12.9442 | Val MAE: 12.944 | LR: 1.0e-04\n",
      "Epoch 138/500 | Train Loss: 11.1708 | Val Loss: 8.7445 | Val MAE: 8.744 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.744. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 139/500 | Train Loss: 10.9426 | Val Loss: 8.3848 | Val MAE: 8.385 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.385. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 140/500 | Train Loss: 10.9539 | Val Loss: 10.0033 | Val MAE: 10.003 | LR: 1.0e-04\n",
      "Epoch 141/500 | Train Loss: 10.7446 | Val Loss: 8.1186 | Val MAE: 8.119 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.119. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 142/500 | Train Loss: 10.4245 | Val Loss: 15.9001 | Val MAE: 15.900 | LR: 1.0e-04\n",
      "Epoch 143/500 | Train Loss: 10.7555 | Val Loss: 8.5285 | Val MAE: 8.528 | LR: 1.0e-04\n",
      "Epoch 144/500 | Train Loss: 10.5750 | Val Loss: 10.9106 | Val MAE: 10.911 | LR: 1.0e-04\n",
      "Epoch 145/500 | Train Loss: 10.4505 | Val Loss: 10.7798 | Val MAE: 10.780 | LR: 1.0e-04\n",
      "Epoch 146/500 | Train Loss: 10.4506 | Val Loss: 8.1699 | Val MAE: 8.170 | LR: 1.0e-04\n",
      "Epoch 147/500 | Train Loss: 10.4361 | Val Loss: 10.6706 | Val MAE: 10.671 | LR: 1.0e-04\n",
      "Epoch 148/500 | Train Loss: 10.2886 | Val Loss: 8.7018 | Val MAE: 8.702 | LR: 1.0e-04\n",
      "Epoch 149/500 | Train Loss: 10.5394 | Val Loss: 8.9028 | Val MAE: 8.903 | LR: 1.0e-04\n",
      "Epoch 150/500 | Train Loss: 10.2001 | Val Loss: 8.4628 | Val MAE: 8.463 | LR: 1.0e-04\n",
      "Epoch 151/500 | Train Loss: 10.3083 | Val Loss: 10.3414 | Val MAE: 10.341 | LR: 1.0e-04\n",
      "Epoch 152/500 | Train Loss: 10.2601 | Val Loss: 8.5501 | Val MAE: 8.550 | LR: 1.0e-04\n",
      "Epoch 153/500 | Train Loss: 10.1524 | Val Loss: 8.7662 | Val MAE: 8.766 | LR: 1.0e-04\n",
      "Epoch 154/500 | Train Loss: 10.2177 | Val Loss: 8.9947 | Val MAE: 8.995 | LR: 1.0e-04\n",
      "Epoch 155/500 | Train Loss: 10.2835 | Val Loss: 9.5898 | Val MAE: 9.590 | LR: 1.0e-04\n",
      "Epoch 156/500 | Train Loss: 9.8375 | Val Loss: 7.9053 | Val MAE: 7.905 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.905. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 157/500 | Train Loss: 10.0389 | Val Loss: 8.2827 | Val MAE: 8.283 | LR: 1.0e-04\n",
      "Epoch 158/500 | Train Loss: 9.8853 | Val Loss: 9.9779 | Val MAE: 9.978 | LR: 1.0e-04\n",
      "Epoch 159/500 | Train Loss: 10.0279 | Val Loss: 9.8652 | Val MAE: 9.865 | LR: 1.0e-04\n",
      "Epoch 160/500 | Train Loss: 9.8735 | Val Loss: 8.8604 | Val MAE: 8.860 | LR: 1.0e-04\n",
      "Epoch 161/500 | Train Loss: 9.6410 | Val Loss: 8.0388 | Val MAE: 8.039 | LR: 1.0e-04\n",
      "Epoch 162/500 | Train Loss: 9.8375 | Val Loss: 7.9737 | Val MAE: 7.974 | LR: 1.0e-04\n",
      "Epoch 163/500 | Train Loss: 9.9381 | Val Loss: 7.8775 | Val MAE: 7.878 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.878. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 164/500 | Train Loss: 10.1988 | Val Loss: 11.0813 | Val MAE: 11.081 | LR: 1.0e-04\n",
      "Epoch 165/500 | Train Loss: 9.8836 | Val Loss: 9.0595 | Val MAE: 9.060 | LR: 1.0e-04\n",
      "Epoch 166/500 | Train Loss: 9.9945 | Val Loss: 7.7542 | Val MAE: 7.754 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.754. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 167/500 | Train Loss: 10.0001 | Val Loss: 7.4744 | Val MAE: 7.474 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.474. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 168/500 | Train Loss: 9.9104 | Val Loss: 7.7027 | Val MAE: 7.703 | LR: 1.0e-04\n",
      "Epoch 169/500 | Train Loss: 9.9741 | Val Loss: 8.5321 | Val MAE: 8.532 | LR: 1.0e-04\n",
      "Epoch 170/500 | Train Loss: 9.6458 | Val Loss: 8.4791 | Val MAE: 8.479 | LR: 1.0e-04\n",
      "Epoch 171/500 | Train Loss: 9.8239 | Val Loss: 8.4507 | Val MAE: 8.451 | LR: 1.0e-04\n",
      "Epoch 172/500 | Train Loss: 9.9760 | Val Loss: 8.1385 | Val MAE: 8.138 | LR: 1.0e-04\n",
      "Epoch 173/500 | Train Loss: 9.4102 | Val Loss: 7.8392 | Val MAE: 7.839 | LR: 1.0e-04\n",
      "Epoch 174/500 | Train Loss: 9.8531 | Val Loss: 9.6571 | Val MAE: 9.657 | LR: 1.0e-04\n",
      "Epoch 175/500 | Train Loss: 9.4933 | Val Loss: 8.0051 | Val MAE: 8.005 | LR: 1.0e-04\n",
      "Epoch 176/500 | Train Loss: 9.5260 | Val Loss: 11.1342 | Val MAE: 11.134 | LR: 1.0e-04\n",
      "Epoch 177/500 | Train Loss: 9.9302 | Val Loss: 7.5048 | Val MAE: 7.505 | LR: 1.0e-04\n",
      "Epoch 178/500 | Train Loss: 10.0341 | Val Loss: 7.9685 | Val MAE: 7.968 | LR: 1.0e-04\n",
      "Epoch 179/500 | Train Loss: 9.8040 | Val Loss: 8.4544 | Val MAE: 8.454 | LR: 1.0e-04\n",
      "Epoch 180/500 | Train Loss: 9.6751 | Val Loss: 7.8208 | Val MAE: 7.821 | LR: 1.0e-04\n",
      "Epoch 181/500 | Train Loss: 9.7665 | Val Loss: 7.8281 | Val MAE: 7.828 | LR: 1.0e-04\n",
      "Epoch 182/500 | Train Loss: 9.6649 | Val Loss: 7.5826 | Val MAE: 7.583 | LR: 1.0e-04\n",
      "Epoch 183/500 | Train Loss: 9.5687 | Val Loss: 7.4870 | Val MAE: 7.487 | LR: 1.0e-04\n",
      "Epoch 184/500 | Train Loss: 9.5927 | Val Loss: 13.3415 | Val MAE: 13.341 | LR: 1.0e-04\n",
      "Epoch 185/500 | Train Loss: 9.7129 | Val Loss: 10.6986 | Val MAE: 10.699 | LR: 1.0e-04\n",
      "Epoch 186/500 | Train Loss: 9.6600 | Val Loss: 8.1933 | Val MAE: 8.193 | LR: 1.0e-04\n",
      "Epoch 187/500 | Train Loss: 9.6158 | Val Loss: 11.7958 | Val MAE: 11.796 | LR: 1.0e-04\n",
      "Epoch 188/500 | Train Loss: 9.4102 | Val Loss: 8.2759 | Val MAE: 8.276 | LR: 1.0e-04\n",
      "Epoch 189/500 | Train Loss: 9.6034 | Val Loss: 10.9489 | Val MAE: 10.949 | LR: 1.0e-04\n",
      "Epoch 190/500 | Train Loss: 9.7661 | Val Loss: 9.1973 | Val MAE: 9.197 | LR: 1.0e-04\n",
      "Epoch 191/500 | Train Loss: 9.9055 | Val Loss: 8.6112 | Val MAE: 8.611 | LR: 1.0e-04\n",
      "Epoch 192/500 | Train Loss: 9.2810 | Val Loss: 8.5279 | Val MAE: 8.528 | LR: 1.0e-04\n",
      "Epoch 193/500 | Train Loss: 9.6408 | Val Loss: 7.8179 | Val MAE: 7.818 | LR: 1.0e-04\n",
      "Epoch 194/500 | Train Loss: 9.7202 | Val Loss: 9.7330 | Val MAE: 9.733 | LR: 1.0e-04\n",
      "Epoch 195/500 | Train Loss: 9.5889 | Val Loss: 13.0820 | Val MAE: 13.082 | LR: 1.0e-04\n",
      "Epoch 196/500 | Train Loss: 9.6677 | Val Loss: 8.1978 | Val MAE: 8.198 | LR: 1.0e-04\n",
      "Epoch 197/500 | Train Loss: 9.5337 | Val Loss: 8.9237 | Val MAE: 8.924 | LR: 1.0e-04\n",
      "Epoch 198/500 | Train Loss: 9.6862 | Val Loss: 8.8915 | Val MAE: 8.891 | LR: 1.0e-04\n",
      "Epoch 199/500 | Train Loss: 9.7132 | Val Loss: 7.8082 | Val MAE: 7.808 | LR: 1.0e-04\n",
      "Epoch 200/500 | Train Loss: 9.3662 | Val Loss: 8.6680 | Val MAE: 8.668 | LR: 1.0e-04\n",
      "Epoch 201/500 | Train Loss: 9.3941 | Val Loss: 11.0110 | Val MAE: 11.011 | LR: 1.0e-04\n",
      "Epoch 202/500 | Train Loss: 9.5549 | Val Loss: 7.5667 | Val MAE: 7.567 | LR: 1.0e-04\n",
      "Epoch 203/500 | Train Loss: 9.5571 | Val Loss: 7.8370 | Val MAE: 7.837 | LR: 1.0e-04\n",
      "Epoch 204/500 | Train Loss: 9.7408 | Val Loss: 7.8489 | Val MAE: 7.849 | LR: 1.0e-04\n",
      "Epoch 205/500 | Train Loss: 9.4298 | Val Loss: 7.5139 | Val MAE: 7.514 | LR: 1.0e-04\n",
      "Epoch 206/500 | Train Loss: 9.6252 | Val Loss: 8.5191 | Val MAE: 8.519 | LR: 1.0e-04\n",
      "Epoch 207/500 | Train Loss: 9.4454 | Val Loss: 7.4342 | Val MAE: 7.434 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.434. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 208/500 | Train Loss: 9.1915 | Val Loss: 9.6308 | Val MAE: 9.631 | LR: 1.0e-04\n",
      "Epoch 209/500 | Train Loss: 9.5503 | Val Loss: 9.8458 | Val MAE: 9.846 | LR: 1.0e-04\n",
      "Epoch 210/500 | Train Loss: 9.4899 | Val Loss: 7.9420 | Val MAE: 7.942 | LR: 1.0e-04\n",
      "Epoch 211/500 | Train Loss: 9.0974 | Val Loss: 8.0301 | Val MAE: 8.030 | LR: 1.0e-04\n",
      "Epoch 212/500 | Train Loss: 9.6062 | Val Loss: 9.1742 | Val MAE: 9.174 | LR: 1.0e-04\n",
      "Epoch 213/500 | Train Loss: 9.4317 | Val Loss: 7.7511 | Val MAE: 7.751 | LR: 1.0e-04\n",
      "Epoch 214/500 | Train Loss: 9.2484 | Val Loss: 7.8690 | Val MAE: 7.869 | LR: 1.0e-04\n",
      "Epoch 215/500 | Train Loss: 9.2898 | Val Loss: 7.9086 | Val MAE: 7.909 | LR: 1.0e-04\n",
      "Epoch 216/500 | Train Loss: 9.4063 | Val Loss: 8.8059 | Val MAE: 8.806 | LR: 1.0e-04\n",
      "Epoch 217/500 | Train Loss: 9.4669 | Val Loss: 8.7476 | Val MAE: 8.748 | LR: 1.0e-04\n",
      "Epoch 218/500 | Train Loss: 9.4604 | Val Loss: 8.2463 | Val MAE: 8.246 | LR: 1.0e-04\n",
      "Epoch 219/500 | Train Loss: 9.6958 | Val Loss: 7.9215 | Val MAE: 7.921 | LR: 1.0e-04\n",
      "Epoch 220/500 | Train Loss: 9.2123 | Val Loss: 7.3019 | Val MAE: 7.302 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.302. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 221/500 | Train Loss: 9.5087 | Val Loss: 7.9253 | Val MAE: 7.925 | LR: 1.0e-04\n",
      "Epoch 222/500 | Train Loss: 9.2077 | Val Loss: 7.5286 | Val MAE: 7.529 | LR: 1.0e-04\n",
      "Epoch 223/500 | Train Loss: 9.1803 | Val Loss: 8.8149 | Val MAE: 8.815 | LR: 1.0e-04\n",
      "Epoch 224/500 | Train Loss: 9.3931 | Val Loss: 7.3045 | Val MAE: 7.304 | LR: 1.0e-04\n",
      "Epoch 225/500 | Train Loss: 9.1391 | Val Loss: 7.5432 | Val MAE: 7.543 | LR: 1.0e-04\n",
      "Epoch 226/500 | Train Loss: 9.2043 | Val Loss: 7.2703 | Val MAE: 7.270 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.270. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 227/500 | Train Loss: 9.5023 | Val Loss: 9.7234 | Val MAE: 9.723 | LR: 1.0e-04\n",
      "Epoch 228/500 | Train Loss: 9.2866 | Val Loss: 7.6880 | Val MAE: 7.688 | LR: 1.0e-04\n",
      "Epoch 229/500 | Train Loss: 9.2448 | Val Loss: 9.1697 | Val MAE: 9.170 | LR: 1.0e-04\n",
      "Epoch 230/500 | Train Loss: 8.9712 | Val Loss: 8.2209 | Val MAE: 8.221 | LR: 1.0e-04\n",
      "Epoch 231/500 | Train Loss: 9.2162 | Val Loss: 7.5469 | Val MAE: 7.547 | LR: 1.0e-04\n",
      "Epoch 232/500 | Train Loss: 9.2379 | Val Loss: 8.3004 | Val MAE: 8.300 | LR: 1.0e-04\n",
      "Epoch 233/500 | Train Loss: 9.1823 | Val Loss: 7.9087 | Val MAE: 7.909 | LR: 1.0e-04\n",
      "Epoch 234/500 | Train Loss: 9.0820 | Val Loss: 7.2805 | Val MAE: 7.280 | LR: 1.0e-04\n",
      "Epoch 235/500 | Train Loss: 9.2230 | Val Loss: 8.2468 | Val MAE: 8.247 | LR: 1.0e-04\n",
      "Epoch 236/500 | Train Loss: 9.2046 | Val Loss: 7.3493 | Val MAE: 7.349 | LR: 1.0e-04\n",
      "Epoch 237/500 | Train Loss: 9.4337 | Val Loss: 8.0713 | Val MAE: 8.071 | LR: 1.0e-04\n",
      "Epoch 238/500 | Train Loss: 9.4327 | Val Loss: 7.3262 | Val MAE: 7.326 | LR: 1.0e-04\n",
      "Epoch 239/500 | Train Loss: 9.3493 | Val Loss: 7.6082 | Val MAE: 7.608 | LR: 1.0e-04\n",
      "Epoch 240/500 | Train Loss: 8.7985 | Val Loss: 8.1391 | Val MAE: 8.139 | LR: 1.0e-04\n",
      "Epoch 241/500 | Train Loss: 9.1749 | Val Loss: 7.3327 | Val MAE: 7.333 | LR: 1.0e-04\n",
      "Epoch 242/500 | Train Loss: 9.0167 | Val Loss: 7.0996 | Val MAE: 7.100 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.100. Saved model to specific_slice_models_multi_dir/best_model_coronal_slice_125.pth\n",
      "Epoch 243/500 | Train Loss: 9.5228 | Val Loss: 10.9895 | Val MAE: 10.990 | LR: 1.0e-04\n",
      "Epoch 244/500 | Train Loss: 9.0006 | Val Loss: 7.5318 | Val MAE: 7.532 | LR: 1.0e-04\n",
      "Epoch 245/500 | Train Loss: 9.1495 | Val Loss: 7.8891 | Val MAE: 7.889 | LR: 1.0e-04\n",
      "Epoch 246/500 | Train Loss: 9.0735 | Val Loss: 8.9679 | Val MAE: 8.968 | LR: 1.0e-04\n",
      "Epoch 247/500 | Train Loss: 9.1171 | Val Loss: 7.3029 | Val MAE: 7.303 | LR: 1.0e-04\n",
      "Epoch 248/500 | Train Loss: 9.4284 | Val Loss: 7.6682 | Val MAE: 7.668 | LR: 1.0e-04\n",
      "Epoch 249/500 | Train Loss: 9.4056 | Val Loss: 7.8540 | Val MAE: 7.854 | LR: 1.0e-04\n",
      "Epoch 250/500 | Train Loss: 9.1921 | Val Loss: 9.4866 | Val MAE: 9.487 | LR: 1.0e-04\n",
      "Epoch 251/500 | Train Loss: 9.2560 | Val Loss: 7.4737 | Val MAE: 7.474 | LR: 1.0e-04\n",
      "Epoch 252/500 | Train Loss: 9.0323 | Val Loss: 8.5479 | Val MAE: 8.548 | LR: 1.0e-04\n",
      "Epoch 253/500 | Train Loss: 9.0317 | Val Loss: 11.6331 | Val MAE: 11.633 | LR: 1.0e-04\n",
      "Epoch 254/500 | Train Loss: 9.0870 | Val Loss: 9.8542 | Val MAE: 9.854 | LR: 1.0e-04\n",
      "Epoch 255/500 | Train Loss: 9.0489 | Val Loss: 7.1822 | Val MAE: 7.182 | LR: 1.0e-04\n",
      "Epoch 256/500 | Train Loss: 9.0894 | Val Loss: 7.3547 | Val MAE: 7.355 | LR: 1.0e-04\n",
      "Epoch 257/500 | Train Loss: 8.9433 | Val Loss: 7.7294 | Val MAE: 7.729 | LR: 1.0e-04\n",
      "Epoch 258/500 | Train Loss: 9.0851 | Val Loss: 7.8330 | Val MAE: 7.833 | LR: 1.0e-04\n",
      "Epoch 259/500 | Train Loss: 9.1367 | Val Loss: 11.4840 | Val MAE: 11.484 | LR: 1.0e-04\n",
      "Epoch 260/500 | Train Loss: 8.7905 | Val Loss: 7.2191 | Val MAE: 7.219 | LR: 1.0e-04\n",
      "Epoch 261/500 | Train Loss: 9.3311 | Val Loss: 16.2753 | Val MAE: 16.275 | LR: 1.0e-04\n",
      "Epoch 262/500 | Train Loss: 9.3964 | Val Loss: 7.6930 | Val MAE: 7.693 | LR: 1.0e-04\n",
      "Epoch 263/500 | Train Loss: 9.3584 | Val Loss: 8.2277 | Val MAE: 8.228 | LR: 1.0e-04\n",
      "Epoch 264/500 | Train Loss: 9.2026 | Val Loss: 7.3395 | Val MAE: 7.340 | LR: 1.0e-04\n",
      "Epoch 265/500 | Train Loss: 9.1743 | Val Loss: 7.1044 | Val MAE: 7.104 | LR: 1.0e-04\n",
      "Epoch 266/500 | Train Loss: 9.3225 | Val Loss: 9.1762 | Val MAE: 9.176 | LR: 1.0e-04\n",
      "Epoch 267/500 | Train Loss: 9.1234 | Val Loss: 9.2613 | Val MAE: 9.261 | LR: 1.0e-04\n",
      "Epoch 268/500 | Train Loss: 9.1201 | Val Loss: 7.3124 | Val MAE: 7.312 | LR: 1.0e-04\n",
      "Epoch 269/500 | Train Loss: 9.2462 | Val Loss: 8.2927 | Val MAE: 8.293 | LR: 1.0e-04\n",
      "Epoch 270/500 | Train Loss: 9.2269 | Val Loss: 7.4893 | Val MAE: 7.489 | LR: 1.0e-04\n",
      "Epoch 271/500 | Train Loss: 8.8782 | Val Loss: 7.9226 | Val MAE: 7.923 | LR: 1.0e-04\n",
      "Epoch 272/500 | Train Loss: 8.9967 | Val Loss: 7.4762 | Val MAE: 7.476 | LR: 1.0e-04\n",
      "Epoch 273/500 | Train Loss: 8.8419 | Val Loss: 8.2905 | Val MAE: 8.290 | LR: 1.0e-04\n",
      "Epoch 274/500 | Train Loss: 9.2807 | Val Loss: 7.7331 | Val MAE: 7.733 | LR: 1.0e-04\n",
      "Epoch 275/500 | Train Loss: 9.2505 | Val Loss: 9.6311 | Val MAE: 9.631 | LR: 1.0e-04\n",
      "Epoch 276/500 | Train Loss: 9.3231 | Val Loss: 10.7217 | Val MAE: 10.722 | LR: 1.0e-04\n",
      "Epoch 277/500 | Train Loss: 8.9810 | Val Loss: 8.2709 | Val MAE: 8.271 | LR: 1.0e-04\n",
      "Epoch 278/500 | Train Loss: 8.9350 | Val Loss: 11.6542 | Val MAE: 11.654 | LR: 1.0e-04\n",
      "Epoch 279/500 | Train Loss: 9.3135 | Val Loss: 8.4139 | Val MAE: 8.414 | LR: 1.0e-04\n",
      "Epoch 280/500 | Train Loss: 9.1199 | Val Loss: 7.2246 | Val MAE: 7.225 | LR: 1.0e-04\n",
      "Epoch 281/500 | Train Loss: 8.9884 | Val Loss: 8.7223 | Val MAE: 8.722 | LR: 1.0e-04\n",
      "Epoch 282/500 | Train Loss: 8.9860 | Val Loss: 8.1344 | Val MAE: 8.134 | LR: 1.0e-04\n",
      "Epoch 283/500 | Train Loss: 8.7702 | Val Loss: 7.2272 | Val MAE: 7.227 | LR: 1.0e-04\n",
      "Epoch 284/500 | Train Loss: 9.0505 | Val Loss: 12.9028 | Val MAE: 12.903 | LR: 1.0e-04\n",
      "Epoch 285/500 | Train Loss: 9.0723 | Val Loss: 11.1469 | Val MAE: 11.147 | LR: 1.0e-04\n",
      "Epoch 286/500 | Train Loss: 8.9735 | Val Loss: 8.8156 | Val MAE: 8.816 | LR: 1.0e-04\n",
      "Epoch 287/500 | Train Loss: 9.2367 | Val Loss: 10.4082 | Val MAE: 10.408 | LR: 1.0e-04\n",
      "Epoch 288/500 | Train Loss: 8.8390 | Val Loss: 7.6181 | Val MAE: 7.618 | LR: 1.0e-04\n",
      "Epoch 289/500 | Train Loss: 8.9795 | Val Loss: 15.2276 | Val MAE: 15.228 | LR: 1.0e-04\n",
      "Epoch 290/500 | Train Loss: 9.0769 | Val Loss: 7.9765 | Val MAE: 7.976 | LR: 1.0e-04\n",
      "Epoch 291/500 | Train Loss: 9.1729 | Val Loss: 8.6906 | Val MAE: 8.691 | LR: 1.0e-04\n",
      "Epoch 292/500 | Train Loss: 9.0292 | Val Loss: 7.2445 | Val MAE: 7.244 | LR: 1.0e-04\n",
      "\n",
      "Early stopping triggered for coronal slice 125 after 50 epochs without improvement.\n",
      "\n",
      "--- Training Finished for coronal Slice 125 ---\n",
      "Best Validation MAE: 7.100 achieved at epoch 242\n",
      "\n",
      "==================== Training for Slice: axial 80 ====================\n",
      "Using feature root: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice\n",
      "Setting up datasets...\n",
      "\n",
      "[Dataset Init] Split: train, Orientation: axial, Slice: 80\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/train\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 2274 subject directories in 'train' to CSV entries.\n",
      "Warning: Could not map 1 directories to CSV.\n",
      "Scanning 2275 potential subject directories for slice axial_80...\n",
      "Info: 1 subject directories were skipped as they couldn't be mapped to metadata.\n",
      "Found 2274 valid files for axial slice 80 in split train.\n",
      "\n",
      "[Dataset Init] Split: validation, Orientation: axial, Slice: 80\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/validation\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 280 subject directories in 'validation' to CSV entries.\n",
      "Scanning 280 potential subject directories for slice axial_80...\n",
      "Found 280 valid files for axial slice 80 in split validation.\n",
      "Setting up dataloaders...\n",
      "Initializing model, optimizer, scheduler...\n",
      "\n",
      "--- Starting Training for axial Slice 80 ---\n",
      "Epoch 1/500 | Train Loss: 54.3854 | Val Loss: 53.4183 | Val MAE: 53.418 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.418. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 2/500 | Train Loss: 54.2781 | Val Loss: 53.4817 | Val MAE: 53.482 | LR: 1.0e-04\n",
      "Epoch 3/500 | Train Loss: 54.1524 | Val Loss: 53.4079 | Val MAE: 53.408 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.408. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 4/500 | Train Loss: 54.0218 | Val Loss: 53.3787 | Val MAE: 53.379 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.379. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 5/500 | Train Loss: 53.9437 | Val Loss: 53.2843 | Val MAE: 53.284 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.284. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 6/500 | Train Loss: 53.8506 | Val Loss: 53.2175 | Val MAE: 53.217 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.217. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 7/500 | Train Loss: 53.6987 | Val Loss: 53.1378 | Val MAE: 53.138 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.138. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 8/500 | Train Loss: 53.6204 | Val Loss: 53.0454 | Val MAE: 53.045 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.045. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 9/500 | Train Loss: 53.4841 | Val Loss: 52.9034 | Val MAE: 52.903 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.903. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 10/500 | Train Loss: 53.3891 | Val Loss: 52.8331 | Val MAE: 52.833 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.833. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 11/500 | Train Loss: 53.2445 | Val Loss: 52.7117 | Val MAE: 52.712 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.712. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 12/500 | Train Loss: 53.1083 | Val Loss: 52.6861 | Val MAE: 52.686 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.686. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 13/500 | Train Loss: 52.9749 | Val Loss: 52.6042 | Val MAE: 52.604 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.604. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 14/500 | Train Loss: 52.8316 | Val Loss: 52.4418 | Val MAE: 52.442 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.442. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 15/500 | Train Loss: 52.6954 | Val Loss: 52.3116 | Val MAE: 52.312 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.312. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 16/500 | Train Loss: 52.5250 | Val Loss: 52.2197 | Val MAE: 52.220 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.220. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 17/500 | Train Loss: 52.3609 | Val Loss: 51.9814 | Val MAE: 51.981 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.981. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 18/500 | Train Loss: 52.2611 | Val Loss: 51.7938 | Val MAE: 51.794 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.794. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 19/500 | Train Loss: 52.0731 | Val Loss: 51.7034 | Val MAE: 51.703 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.703. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 20/500 | Train Loss: 51.8801 | Val Loss: 51.7216 | Val MAE: 51.722 | LR: 1.0e-04\n",
      "Epoch 21/500 | Train Loss: 51.7514 | Val Loss: 51.4527 | Val MAE: 51.453 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.453. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 22/500 | Train Loss: 51.5456 | Val Loss: 51.1371 | Val MAE: 51.137 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.137. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 23/500 | Train Loss: 51.3598 | Val Loss: 51.1910 | Val MAE: 51.191 | LR: 1.0e-04\n",
      "Epoch 24/500 | Train Loss: 51.1767 | Val Loss: 50.9336 | Val MAE: 50.934 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.934. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 25/500 | Train Loss: 50.9992 | Val Loss: 50.8044 | Val MAE: 50.804 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.804. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 26/500 | Train Loss: 50.8141 | Val Loss: 50.5336 | Val MAE: 50.534 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.534. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 27/500 | Train Loss: 50.5368 | Val Loss: 50.3171 | Val MAE: 50.317 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.317. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 28/500 | Train Loss: 50.3636 | Val Loss: 50.0161 | Val MAE: 50.016 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.016. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 29/500 | Train Loss: 50.1213 | Val Loss: 49.9717 | Val MAE: 49.972 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.972. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 30/500 | Train Loss: 49.9103 | Val Loss: 49.7803 | Val MAE: 49.780 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.780. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 31/500 | Train Loss: 49.6902 | Val Loss: 49.4428 | Val MAE: 49.443 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.443. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 32/500 | Train Loss: 49.4539 | Val Loss: 48.9962 | Val MAE: 48.996 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.996. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 33/500 | Train Loss: 49.1379 | Val Loss: 48.9369 | Val MAE: 48.937 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.937. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 34/500 | Train Loss: 48.9289 | Val Loss: 48.7381 | Val MAE: 48.738 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.738. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 35/500 | Train Loss: 48.7339 | Val Loss: 48.3826 | Val MAE: 48.383 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.383. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 36/500 | Train Loss: 48.3946 | Val Loss: 48.2406 | Val MAE: 48.241 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.241. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 37/500 | Train Loss: 48.1169 | Val Loss: 47.6958 | Val MAE: 47.696 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.696. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 38/500 | Train Loss: 47.8615 | Val Loss: 47.6118 | Val MAE: 47.612 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.612. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 39/500 | Train Loss: 47.5367 | Val Loss: 47.3068 | Val MAE: 47.307 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.307. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 40/500 | Train Loss: 47.2559 | Val Loss: 47.0288 | Val MAE: 47.029 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.029. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 41/500 | Train Loss: 46.9764 | Val Loss: 46.8064 | Val MAE: 46.806 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 46.806. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 42/500 | Train Loss: 46.6720 | Val Loss: 46.4704 | Val MAE: 46.470 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 46.470. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 43/500 | Train Loss: 46.3357 | Val Loss: 46.2216 | Val MAE: 46.222 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 46.222. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 44/500 | Train Loss: 45.9624 | Val Loss: 45.9080 | Val MAE: 45.908 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.908. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 45/500 | Train Loss: 45.7247 | Val Loss: 45.4316 | Val MAE: 45.432 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.432. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 46/500 | Train Loss: 45.3261 | Val Loss: 45.3357 | Val MAE: 45.336 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.336. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 47/500 | Train Loss: 45.0119 | Val Loss: 44.7659 | Val MAE: 44.766 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 44.766. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 48/500 | Train Loss: 44.6415 | Val Loss: 44.3245 | Val MAE: 44.325 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 44.325. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 49/500 | Train Loss: 44.2843 | Val Loss: 44.1134 | Val MAE: 44.113 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 44.113. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 50/500 | Train Loss: 44.0005 | Val Loss: 43.6169 | Val MAE: 43.617 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 43.617. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 51/500 | Train Loss: 43.5897 | Val Loss: 43.4210 | Val MAE: 43.421 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 43.421. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 52/500 | Train Loss: 43.1525 | Val Loss: 43.2908 | Val MAE: 43.291 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 43.291. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 53/500 | Train Loss: 42.8670 | Val Loss: 42.5093 | Val MAE: 42.509 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 42.509. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 54/500 | Train Loss: 42.5301 | Val Loss: 42.1126 | Val MAE: 42.113 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 42.113. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 55/500 | Train Loss: 42.1174 | Val Loss: 41.7525 | Val MAE: 41.752 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 41.752. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 56/500 | Train Loss: 41.6373 | Val Loss: 41.4678 | Val MAE: 41.468 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 41.468. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 57/500 | Train Loss: 41.2612 | Val Loss: 41.0182 | Val MAE: 41.018 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 41.018. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 58/500 | Train Loss: 40.9358 | Val Loss: 40.2836 | Val MAE: 40.284 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.284. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 59/500 | Train Loss: 40.5857 | Val Loss: 40.2764 | Val MAE: 40.276 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.276. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 60/500 | Train Loss: 40.1813 | Val Loss: 40.1885 | Val MAE: 40.188 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.188. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 61/500 | Train Loss: 39.7659 | Val Loss: 39.3021 | Val MAE: 39.302 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 39.302. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 62/500 | Train Loss: 39.3661 | Val Loss: 39.3402 | Val MAE: 39.340 | LR: 1.0e-04\n",
      "Epoch 63/500 | Train Loss: 38.9097 | Val Loss: 38.5998 | Val MAE: 38.600 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 38.600. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 64/500 | Train Loss: 38.5699 | Val Loss: 37.9943 | Val MAE: 37.994 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 37.994. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 65/500 | Train Loss: 38.0680 | Val Loss: 37.6402 | Val MAE: 37.640 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 37.640. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 66/500 | Train Loss: 37.6679 | Val Loss: 37.7562 | Val MAE: 37.756 | LR: 1.0e-04\n",
      "Epoch 67/500 | Train Loss: 37.2966 | Val Loss: 36.5144 | Val MAE: 36.514 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 36.514. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 68/500 | Train Loss: 36.8234 | Val Loss: 36.6546 | Val MAE: 36.655 | LR: 1.0e-04\n",
      "Epoch 69/500 | Train Loss: 36.4492 | Val Loss: 37.3897 | Val MAE: 37.390 | LR: 1.0e-04\n",
      "Epoch 70/500 | Train Loss: 35.9446 | Val Loss: 36.1088 | Val MAE: 36.109 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 36.109. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 71/500 | Train Loss: 35.5437 | Val Loss: 35.4362 | Val MAE: 35.436 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 35.436. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 72/500 | Train Loss: 35.1326 | Val Loss: 33.9077 | Val MAE: 33.908 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 33.908. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 73/500 | Train Loss: 34.5574 | Val Loss: 34.1130 | Val MAE: 34.113 | LR: 1.0e-04\n",
      "Epoch 74/500 | Train Loss: 34.2720 | Val Loss: 33.6445 | Val MAE: 33.645 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 33.645. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 75/500 | Train Loss: 33.8621 | Val Loss: 32.5598 | Val MAE: 32.560 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 32.560. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 76/500 | Train Loss: 33.2574 | Val Loss: 32.8896 | Val MAE: 32.890 | LR: 1.0e-04\n",
      "Epoch 77/500 | Train Loss: 32.9539 | Val Loss: 33.6659 | Val MAE: 33.666 | LR: 1.0e-04\n",
      "Epoch 78/500 | Train Loss: 32.4621 | Val Loss: 33.3141 | Val MAE: 33.314 | LR: 1.0e-04\n",
      "Epoch 79/500 | Train Loss: 32.1271 | Val Loss: 29.8266 | Val MAE: 29.827 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 29.827. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 80/500 | Train Loss: 31.4440 | Val Loss: 31.6316 | Val MAE: 31.632 | LR: 1.0e-04\n",
      "Epoch 81/500 | Train Loss: 31.0990 | Val Loss: 34.9026 | Val MAE: 34.903 | LR: 1.0e-04\n",
      "Epoch 82/500 | Train Loss: 30.5338 | Val Loss: 33.8869 | Val MAE: 33.887 | LR: 1.0e-04\n",
      "Epoch 83/500 | Train Loss: 30.1448 | Val Loss: 28.5148 | Val MAE: 28.515 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 28.515. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 84/500 | Train Loss: 29.7966 | Val Loss: 28.2902 | Val MAE: 28.290 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 28.290. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 85/500 | Train Loss: 29.2698 | Val Loss: 29.0000 | Val MAE: 29.000 | LR: 1.0e-04\n",
      "Epoch 86/500 | Train Loss: 28.8876 | Val Loss: 27.5637 | Val MAE: 27.564 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 27.564. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 87/500 | Train Loss: 28.3675 | Val Loss: 28.4812 | Val MAE: 28.481 | LR: 1.0e-04\n",
      "Epoch 88/500 | Train Loss: 27.7953 | Val Loss: 28.6234 | Val MAE: 28.623 | LR: 1.0e-04\n",
      "Epoch 89/500 | Train Loss: 27.5528 | Val Loss: 29.8417 | Val MAE: 29.842 | LR: 1.0e-04\n",
      "Epoch 90/500 | Train Loss: 26.9280 | Val Loss: 28.1432 | Val MAE: 28.143 | LR: 1.0e-04\n",
      "Epoch 91/500 | Train Loss: 26.5255 | Val Loss: 26.0314 | Val MAE: 26.031 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 26.031. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 92/500 | Train Loss: 26.0631 | Val Loss: 23.9432 | Val MAE: 23.943 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 23.943. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 93/500 | Train Loss: 25.5448 | Val Loss: 23.5280 | Val MAE: 23.528 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 23.528. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 94/500 | Train Loss: 25.0344 | Val Loss: 24.3481 | Val MAE: 24.348 | LR: 1.0e-04\n",
      "Epoch 95/500 | Train Loss: 24.6037 | Val Loss: 24.4753 | Val MAE: 24.475 | LR: 1.0e-04\n",
      "Epoch 96/500 | Train Loss: 24.1961 | Val Loss: 22.1712 | Val MAE: 22.171 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 22.171. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 97/500 | Train Loss: 23.7382 | Val Loss: 21.9615 | Val MAE: 21.961 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 21.961. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 98/500 | Train Loss: 23.2408 | Val Loss: 23.1542 | Val MAE: 23.154 | LR: 1.0e-04\n",
      "Epoch 99/500 | Train Loss: 22.8214 | Val Loss: 18.7953 | Val MAE: 18.795 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 18.795. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 100/500 | Train Loss: 22.4309 | Val Loss: 19.9393 | Val MAE: 19.939 | LR: 1.0e-04\n",
      "Epoch 101/500 | Train Loss: 21.6992 | Val Loss: 19.6589 | Val MAE: 19.659 | LR: 1.0e-04\n",
      "Epoch 102/500 | Train Loss: 21.3046 | Val Loss: 24.9958 | Val MAE: 24.996 | LR: 1.0e-04\n",
      "Epoch 103/500 | Train Loss: 21.0708 | Val Loss: 16.4496 | Val MAE: 16.450 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 16.450. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 104/500 | Train Loss: 20.4353 | Val Loss: 17.8360 | Val MAE: 17.836 | LR: 1.0e-04\n",
      "Epoch 105/500 | Train Loss: 20.2265 | Val Loss: 17.6034 | Val MAE: 17.603 | LR: 1.0e-04\n",
      "Epoch 106/500 | Train Loss: 19.7937 | Val Loss: 25.3033 | Val MAE: 25.303 | LR: 1.0e-04\n",
      "Epoch 107/500 | Train Loss: 19.0319 | Val Loss: 18.9349 | Val MAE: 18.935 | LR: 1.0e-04\n",
      "Epoch 108/500 | Train Loss: 18.8799 | Val Loss: 20.9616 | Val MAE: 20.962 | LR: 1.0e-04\n",
      "Epoch 109/500 | Train Loss: 18.3211 | Val Loss: 17.3798 | Val MAE: 17.380 | LR: 1.0e-04\n",
      "Epoch 110/500 | Train Loss: 17.8208 | Val Loss: 18.8300 | Val MAE: 18.830 | LR: 1.0e-04\n",
      "Epoch 111/500 | Train Loss: 17.4665 | Val Loss: 19.7609 | Val MAE: 19.761 | LR: 1.0e-04\n",
      "Epoch 112/500 | Train Loss: 17.3690 | Val Loss: 13.2908 | Val MAE: 13.291 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 13.291. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 113/500 | Train Loss: 16.8995 | Val Loss: 17.5346 | Val MAE: 17.535 | LR: 1.0e-04\n",
      "Epoch 114/500 | Train Loss: 16.3926 | Val Loss: 13.0762 | Val MAE: 13.076 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 13.076. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 115/500 | Train Loss: 15.8196 | Val Loss: 15.2492 | Val MAE: 15.249 | LR: 1.0e-04\n",
      "Epoch 116/500 | Train Loss: 15.6303 | Val Loss: 11.9168 | Val MAE: 11.917 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 11.917. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 117/500 | Train Loss: 15.0218 | Val Loss: 18.8476 | Val MAE: 18.848 | LR: 1.0e-04\n",
      "Epoch 118/500 | Train Loss: 14.8413 | Val Loss: 14.6487 | Val MAE: 14.649 | LR: 1.0e-04\n",
      "Epoch 119/500 | Train Loss: 14.6018 | Val Loss: 15.2900 | Val MAE: 15.290 | LR: 1.0e-04\n",
      "Epoch 120/500 | Train Loss: 14.0834 | Val Loss: 19.7039 | Val MAE: 19.704 | LR: 1.0e-04\n",
      "Epoch 121/500 | Train Loss: 13.8323 | Val Loss: 13.5523 | Val MAE: 13.552 | LR: 1.0e-04\n",
      "Epoch 122/500 | Train Loss: 13.5709 | Val Loss: 15.5498 | Val MAE: 15.550 | LR: 1.0e-04\n",
      "Epoch 123/500 | Train Loss: 13.2519 | Val Loss: 16.6018 | Val MAE: 16.602 | LR: 1.0e-04\n",
      "Epoch 124/500 | Train Loss: 13.1292 | Val Loss: 17.2681 | Val MAE: 17.268 | LR: 1.0e-04\n",
      "Epoch 125/500 | Train Loss: 12.7737 | Val Loss: 12.5826 | Val MAE: 12.583 | LR: 1.0e-04\n",
      "Epoch 126/500 | Train Loss: 12.4731 | Val Loss: 12.0680 | Val MAE: 12.068 | LR: 1.0e-04\n",
      "Epoch 127/500 | Train Loss: 12.3414 | Val Loss: 22.2644 | Val MAE: 22.264 | LR: 1.0e-04\n",
      "Epoch 128/500 | Train Loss: 12.1000 | Val Loss: 11.2122 | Val MAE: 11.212 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 11.212. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 129/500 | Train Loss: 12.0861 | Val Loss: 16.5235 | Val MAE: 16.524 | LR: 1.0e-04\n",
      "Epoch 130/500 | Train Loss: 11.5860 | Val Loss: 10.5861 | Val MAE: 10.586 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 10.586. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 131/500 | Train Loss: 11.4973 | Val Loss: 15.7090 | Val MAE: 15.709 | LR: 1.0e-04\n",
      "Epoch 132/500 | Train Loss: 11.1687 | Val Loss: 9.3241 | Val MAE: 9.324 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 9.324. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 133/500 | Train Loss: 11.1240 | Val Loss: 9.8244 | Val MAE: 9.824 | LR: 1.0e-04\n",
      "Epoch 134/500 | Train Loss: 11.5214 | Val Loss: 12.2534 | Val MAE: 12.253 | LR: 1.0e-04\n",
      "Epoch 135/500 | Train Loss: 10.8298 | Val Loss: 9.0333 | Val MAE: 9.033 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 9.033. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 136/500 | Train Loss: 11.0346 | Val Loss: 12.0647 | Val MAE: 12.065 | LR: 1.0e-04\n",
      "Epoch 137/500 | Train Loss: 10.5843 | Val Loss: 9.5886 | Val MAE: 9.589 | LR: 1.0e-04\n",
      "Epoch 138/500 | Train Loss: 10.6902 | Val Loss: 8.9110 | Val MAE: 8.911 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.911. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 139/500 | Train Loss: 10.4145 | Val Loss: 10.9122 | Val MAE: 10.912 | LR: 1.0e-04\n",
      "Epoch 140/500 | Train Loss: 10.7680 | Val Loss: 7.9548 | Val MAE: 7.955 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.955. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 141/500 | Train Loss: 10.4207 | Val Loss: 8.2008 | Val MAE: 8.201 | LR: 1.0e-04\n",
      "Epoch 142/500 | Train Loss: 10.1487 | Val Loss: 8.6055 | Val MAE: 8.605 | LR: 1.0e-04\n",
      "Epoch 143/500 | Train Loss: 10.0738 | Val Loss: 7.9344 | Val MAE: 7.934 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.934. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 144/500 | Train Loss: 10.3741 | Val Loss: 9.5501 | Val MAE: 9.550 | LR: 1.0e-04\n",
      "Epoch 145/500 | Train Loss: 10.2087 | Val Loss: 9.2508 | Val MAE: 9.251 | LR: 1.0e-04\n",
      "Epoch 146/500 | Train Loss: 10.1731 | Val Loss: 8.7518 | Val MAE: 8.752 | LR: 1.0e-04\n",
      "Epoch 147/500 | Train Loss: 10.4118 | Val Loss: 9.9314 | Val MAE: 9.931 | LR: 1.0e-04\n",
      "Epoch 148/500 | Train Loss: 10.1570 | Val Loss: 7.8576 | Val MAE: 7.858 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.858. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 149/500 | Train Loss: 9.9964 | Val Loss: 7.6726 | Val MAE: 7.673 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.673. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 150/500 | Train Loss: 9.9459 | Val Loss: 8.1748 | Val MAE: 8.175 | LR: 1.0e-04\n",
      "Epoch 151/500 | Train Loss: 10.1034 | Val Loss: 7.8607 | Val MAE: 7.861 | LR: 1.0e-04\n",
      "Epoch 152/500 | Train Loss: 9.5659 | Val Loss: 9.7402 | Val MAE: 9.740 | LR: 1.0e-04\n",
      "Epoch 153/500 | Train Loss: 9.6852 | Val Loss: 9.5542 | Val MAE: 9.554 | LR: 1.0e-04\n",
      "Epoch 154/500 | Train Loss: 9.8616 | Val Loss: 8.1074 | Val MAE: 8.107 | LR: 1.0e-04\n",
      "Epoch 155/500 | Train Loss: 9.9710 | Val Loss: 9.7018 | Val MAE: 9.702 | LR: 1.0e-04\n",
      "Epoch 156/500 | Train Loss: 9.8883 | Val Loss: 13.2254 | Val MAE: 13.225 | LR: 1.0e-04\n",
      "Epoch 157/500 | Train Loss: 10.0392 | Val Loss: 7.7305 | Val MAE: 7.731 | LR: 1.0e-04\n",
      "Epoch 158/500 | Train Loss: 9.9233 | Val Loss: 7.3732 | Val MAE: 7.373 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.373. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 159/500 | Train Loss: 9.6114 | Val Loss: 8.6384 | Val MAE: 8.638 | LR: 1.0e-04\n",
      "Epoch 160/500 | Train Loss: 9.6072 | Val Loss: 9.6405 | Val MAE: 9.641 | LR: 1.0e-04\n",
      "Epoch 161/500 | Train Loss: 9.6902 | Val Loss: 7.8105 | Val MAE: 7.810 | LR: 1.0e-04\n",
      "Epoch 162/500 | Train Loss: 9.8812 | Val Loss: 9.6195 | Val MAE: 9.619 | LR: 1.0e-04\n",
      "Epoch 163/500 | Train Loss: 9.8879 | Val Loss: 7.7611 | Val MAE: 7.761 | LR: 1.0e-04\n",
      "Epoch 164/500 | Train Loss: 9.6624 | Val Loss: 9.1552 | Val MAE: 9.155 | LR: 1.0e-04\n",
      "Epoch 165/500 | Train Loss: 9.9888 | Val Loss: 7.3645 | Val MAE: 7.365 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.365. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 166/500 | Train Loss: 9.8075 | Val Loss: 8.3647 | Val MAE: 8.365 | LR: 1.0e-04\n",
      "Epoch 167/500 | Train Loss: 9.6121 | Val Loss: 7.5600 | Val MAE: 7.560 | LR: 1.0e-04\n",
      "Epoch 168/500 | Train Loss: 9.6703 | Val Loss: 8.4273 | Val MAE: 8.427 | LR: 1.0e-04\n",
      "Epoch 169/500 | Train Loss: 9.9126 | Val Loss: 8.3428 | Val MAE: 8.343 | LR: 1.0e-04\n",
      "Epoch 170/500 | Train Loss: 9.6839 | Val Loss: 8.7781 | Val MAE: 8.778 | LR: 1.0e-04\n",
      "Epoch 171/500 | Train Loss: 9.7220 | Val Loss: 7.8098 | Val MAE: 7.810 | LR: 1.0e-04\n",
      "Epoch 172/500 | Train Loss: 9.9236 | Val Loss: 7.6950 | Val MAE: 7.695 | LR: 1.0e-04\n",
      "Epoch 173/500 | Train Loss: 9.4826 | Val Loss: 9.4009 | Val MAE: 9.401 | LR: 1.0e-04\n",
      "Epoch 174/500 | Train Loss: 9.7644 | Val Loss: 7.7654 | Val MAE: 7.765 | LR: 1.0e-04\n",
      "Epoch 175/500 | Train Loss: 9.5538 | Val Loss: 7.8327 | Val MAE: 7.833 | LR: 1.0e-04\n",
      "Epoch 176/500 | Train Loss: 9.7108 | Val Loss: 8.2190 | Val MAE: 8.219 | LR: 1.0e-04\n",
      "Epoch 177/500 | Train Loss: 9.5585 | Val Loss: 8.3894 | Val MAE: 8.389 | LR: 1.0e-04\n",
      "Epoch 178/500 | Train Loss: 9.6673 | Val Loss: 7.2224 | Val MAE: 7.222 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.222. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 179/500 | Train Loss: 9.8594 | Val Loss: 9.4808 | Val MAE: 9.481 | LR: 1.0e-04\n",
      "Epoch 180/500 | Train Loss: 9.5613 | Val Loss: 9.3531 | Val MAE: 9.353 | LR: 1.0e-04\n",
      "Epoch 181/500 | Train Loss: 9.6159 | Val Loss: 7.7203 | Val MAE: 7.720 | LR: 1.0e-04\n",
      "Epoch 182/500 | Train Loss: 9.3192 | Val Loss: 9.5618 | Val MAE: 9.562 | LR: 1.0e-04\n",
      "Epoch 183/500 | Train Loss: 9.6922 | Val Loss: 8.6817 | Val MAE: 8.682 | LR: 1.0e-04\n",
      "Epoch 184/500 | Train Loss: 9.4281 | Val Loss: 9.0104 | Val MAE: 9.010 | LR: 1.0e-04\n",
      "Epoch 185/500 | Train Loss: 9.5068 | Val Loss: 7.5272 | Val MAE: 7.527 | LR: 1.0e-04\n",
      "Epoch 186/500 | Train Loss: 9.7470 | Val Loss: 8.4521 | Val MAE: 8.452 | LR: 1.0e-04\n",
      "Epoch 187/500 | Train Loss: 9.6144 | Val Loss: 7.4796 | Val MAE: 7.480 | LR: 1.0e-04\n",
      "Epoch 188/500 | Train Loss: 9.3376 | Val Loss: 10.7764 | Val MAE: 10.776 | LR: 1.0e-04\n",
      "Epoch 189/500 | Train Loss: 9.3836 | Val Loss: 8.0403 | Val MAE: 8.040 | LR: 1.0e-04\n",
      "Epoch 190/500 | Train Loss: 9.4700 | Val Loss: 7.6326 | Val MAE: 7.633 | LR: 1.0e-04\n",
      "Epoch 191/500 | Train Loss: 9.6644 | Val Loss: 8.4828 | Val MAE: 8.483 | LR: 1.0e-04\n",
      "Epoch 192/500 | Train Loss: 9.6596 | Val Loss: 9.0620 | Val MAE: 9.062 | LR: 1.0e-04\n",
      "Epoch 193/500 | Train Loss: 9.5675 | Val Loss: 8.8311 | Val MAE: 8.831 | LR: 1.0e-04\n",
      "Epoch 194/500 | Train Loss: 9.4906 | Val Loss: 7.7822 | Val MAE: 7.782 | LR: 1.0e-04\n",
      "Epoch 195/500 | Train Loss: 9.5812 | Val Loss: 7.7995 | Val MAE: 7.799 | LR: 1.0e-04\n",
      "Epoch 196/500 | Train Loss: 9.5876 | Val Loss: 7.8722 | Val MAE: 7.872 | LR: 1.0e-04\n",
      "Epoch 197/500 | Train Loss: 9.3294 | Val Loss: 8.5508 | Val MAE: 8.551 | LR: 1.0e-04\n",
      "Epoch 198/500 | Train Loss: 9.5849 | Val Loss: 7.6182 | Val MAE: 7.618 | LR: 1.0e-04\n",
      "Epoch 199/500 | Train Loss: 9.5364 | Val Loss: 7.7514 | Val MAE: 7.751 | LR: 1.0e-04\n",
      "Epoch 200/500 | Train Loss: 9.3508 | Val Loss: 11.2563 | Val MAE: 11.256 | LR: 1.0e-04\n",
      "Epoch 201/500 | Train Loss: 9.3867 | Val Loss: 9.2196 | Val MAE: 9.220 | LR: 1.0e-04\n",
      "Epoch 202/500 | Train Loss: 9.5486 | Val Loss: 7.9685 | Val MAE: 7.969 | LR: 1.0e-04\n",
      "Epoch 203/500 | Train Loss: 9.1635 | Val Loss: 7.6730 | Val MAE: 7.673 | LR: 1.0e-04\n",
      "Epoch 204/500 | Train Loss: 9.4296 | Val Loss: 8.0292 | Val MAE: 8.029 | LR: 1.0e-04\n",
      "Epoch 205/500 | Train Loss: 9.3659 | Val Loss: 7.3571 | Val MAE: 7.357 | LR: 1.0e-04\n",
      "Epoch 206/500 | Train Loss: 9.6800 | Val Loss: 7.2539 | Val MAE: 7.254 | LR: 1.0e-04\n",
      "Epoch 207/500 | Train Loss: 9.4286 | Val Loss: 8.1229 | Val MAE: 8.123 | LR: 1.0e-04\n",
      "Epoch 208/500 | Train Loss: 9.6315 | Val Loss: 7.4092 | Val MAE: 7.409 | LR: 1.0e-04\n",
      "Epoch 209/500 | Train Loss: 9.6202 | Val Loss: 8.8799 | Val MAE: 8.880 | LR: 1.0e-04\n",
      "Epoch 210/500 | Train Loss: 9.1758 | Val Loss: 7.5652 | Val MAE: 7.565 | LR: 1.0e-04\n",
      "Epoch 211/500 | Train Loss: 9.3666 | Val Loss: 7.3191 | Val MAE: 7.319 | LR: 1.0e-04\n",
      "Epoch 212/500 | Train Loss: 9.0926 | Val Loss: 8.0412 | Val MAE: 8.041 | LR: 1.0e-04\n",
      "Epoch 213/500 | Train Loss: 9.1524 | Val Loss: 8.1944 | Val MAE: 8.194 | LR: 1.0e-04\n",
      "Epoch 214/500 | Train Loss: 9.3931 | Val Loss: 7.4035 | Val MAE: 7.404 | LR: 1.0e-04\n",
      "Epoch 215/500 | Train Loss: 9.4083 | Val Loss: 8.8985 | Val MAE: 8.898 | LR: 1.0e-04\n",
      "Epoch 216/500 | Train Loss: 9.4525 | Val Loss: 7.4770 | Val MAE: 7.477 | LR: 1.0e-04\n",
      "Epoch 217/500 | Train Loss: 9.4986 | Val Loss: 9.9388 | Val MAE: 9.939 | LR: 1.0e-04\n",
      "Epoch 218/500 | Train Loss: 9.3244 | Val Loss: 7.2285 | Val MAE: 7.228 | LR: 1.0e-04\n",
      "Epoch 219/500 | Train Loss: 9.3800 | Val Loss: 7.7672 | Val MAE: 7.767 | LR: 1.0e-04\n",
      "Epoch 220/500 | Train Loss: 9.4614 | Val Loss: 8.3085 | Val MAE: 8.308 | LR: 1.0e-04\n",
      "Epoch 221/500 | Train Loss: 9.5154 | Val Loss: 7.1440 | Val MAE: 7.144 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.144. Saved model to specific_slice_models_multi_dir/best_model_axial_slice_80.pth\n",
      "Epoch 222/500 | Train Loss: 9.3011 | Val Loss: 7.7057 | Val MAE: 7.706 | LR: 1.0e-04\n",
      "Epoch 223/500 | Train Loss: 9.2855 | Val Loss: 7.2489 | Val MAE: 7.249 | LR: 1.0e-04\n",
      "Epoch 224/500 | Train Loss: 9.2482 | Val Loss: 8.5031 | Val MAE: 8.503 | LR: 1.0e-04\n",
      "Epoch 225/500 | Train Loss: 9.2753 | Val Loss: 7.8015 | Val MAE: 7.802 | LR: 1.0e-04\n",
      "Epoch 226/500 | Train Loss: 9.4959 | Val Loss: 7.6197 | Val MAE: 7.620 | LR: 1.0e-04\n",
      "Epoch 227/500 | Train Loss: 9.2963 | Val Loss: 7.8259 | Val MAE: 7.826 | LR: 1.0e-04\n",
      "Epoch 228/500 | Train Loss: 9.0787 | Val Loss: 9.1595 | Val MAE: 9.159 | LR: 1.0e-04\n",
      "Epoch 229/500 | Train Loss: 9.5497 | Val Loss: 7.6390 | Val MAE: 7.639 | LR: 1.0e-04\n",
      "Epoch 230/500 | Train Loss: 9.3997 | Val Loss: 7.6031 | Val MAE: 7.603 | LR: 1.0e-04\n",
      "Epoch 231/500 | Train Loss: 9.3225 | Val Loss: 7.7340 | Val MAE: 7.734 | LR: 1.0e-04\n",
      "Epoch 232/500 | Train Loss: 9.3243 | Val Loss: 7.5291 | Val MAE: 7.529 | LR: 1.0e-04\n",
      "Epoch 233/500 | Train Loss: 9.1366 | Val Loss: 7.5597 | Val MAE: 7.560 | LR: 1.0e-04\n",
      "Epoch 234/500 | Train Loss: 8.8479 | Val Loss: 8.6429 | Val MAE: 8.643 | LR: 1.0e-04\n",
      "Epoch 235/500 | Train Loss: 9.0430 | Val Loss: 8.0906 | Val MAE: 8.091 | LR: 1.0e-04\n",
      "Epoch 236/500 | Train Loss: 9.3623 | Val Loss: 12.0522 | Val MAE: 12.052 | LR: 1.0e-04\n",
      "Epoch 237/500 | Train Loss: 9.0196 | Val Loss: 7.5122 | Val MAE: 7.512 | LR: 1.0e-04\n",
      "Epoch 238/500 | Train Loss: 9.1212 | Val Loss: 7.9888 | Val MAE: 7.989 | LR: 1.0e-04\n",
      "Epoch 239/500 | Train Loss: 9.4275 | Val Loss: 7.4687 | Val MAE: 7.469 | LR: 1.0e-04\n",
      "Epoch 240/500 | Train Loss: 9.3323 | Val Loss: 7.3857 | Val MAE: 7.386 | LR: 1.0e-04\n",
      "Epoch 241/500 | Train Loss: 9.2229 | Val Loss: 7.4971 | Val MAE: 7.497 | LR: 1.0e-04\n",
      "Epoch 242/500 | Train Loss: 9.1633 | Val Loss: 7.4504 | Val MAE: 7.450 | LR: 1.0e-04\n",
      "Epoch 243/500 | Train Loss: 9.1689 | Val Loss: 7.5246 | Val MAE: 7.525 | LR: 1.0e-04\n",
      "Epoch 244/500 | Train Loss: 9.1316 | Val Loss: 10.0410 | Val MAE: 10.041 | LR: 1.0e-04\n",
      "Epoch 245/500 | Train Loss: 9.1816 | Val Loss: 7.9607 | Val MAE: 7.961 | LR: 1.0e-04\n",
      "Epoch 246/500 | Train Loss: 9.1727 | Val Loss: 7.3861 | Val MAE: 7.386 | LR: 1.0e-04\n",
      "Epoch 247/500 | Train Loss: 9.1932 | Val Loss: 7.4422 | Val MAE: 7.442 | LR: 1.0e-04\n",
      "Epoch 248/500 | Train Loss: 9.3177 | Val Loss: 8.0003 | Val MAE: 8.000 | LR: 1.0e-04\n",
      "Epoch 249/500 | Train Loss: 9.0100 | Val Loss: 7.5032 | Val MAE: 7.503 | LR: 1.0e-04\n",
      "Epoch 250/500 | Train Loss: 9.1720 | Val Loss: 7.6950 | Val MAE: 7.695 | LR: 1.0e-04\n",
      "Epoch 251/500 | Train Loss: 9.3418 | Val Loss: 8.3279 | Val MAE: 8.328 | LR: 1.0e-04\n",
      "Epoch 252/500 | Train Loss: 9.1787 | Val Loss: 8.0498 | Val MAE: 8.050 | LR: 1.0e-04\n",
      "Epoch 253/500 | Train Loss: 9.1878 | Val Loss: 7.7947 | Val MAE: 7.795 | LR: 1.0e-04\n",
      "Epoch 254/500 | Train Loss: 9.2883 | Val Loss: 7.7979 | Val MAE: 7.798 | LR: 1.0e-04\n",
      "Epoch 255/500 | Train Loss: 9.0881 | Val Loss: 8.7869 | Val MAE: 8.787 | LR: 1.0e-04\n",
      "Epoch 256/500 | Train Loss: 8.9939 | Val Loss: 7.8372 | Val MAE: 7.837 | LR: 1.0e-04\n",
      "Epoch 257/500 | Train Loss: 9.6591 | Val Loss: 9.0036 | Val MAE: 9.004 | LR: 1.0e-04\n",
      "Epoch 258/500 | Train Loss: 9.0478 | Val Loss: 8.0760 | Val MAE: 8.076 | LR: 1.0e-04\n",
      "Epoch 259/500 | Train Loss: 9.1004 | Val Loss: 7.3683 | Val MAE: 7.368 | LR: 1.0e-04\n",
      "Epoch 260/500 | Train Loss: 9.1718 | Val Loss: 7.2436 | Val MAE: 7.244 | LR: 1.0e-04\n",
      "Epoch 261/500 | Train Loss: 9.1309 | Val Loss: 7.6731 | Val MAE: 7.673 | LR: 1.0e-04\n",
      "Epoch 262/500 | Train Loss: 9.3316 | Val Loss: 7.2077 | Val MAE: 7.208 | LR: 1.0e-04\n",
      "Epoch 263/500 | Train Loss: 9.1778 | Val Loss: 12.5325 | Val MAE: 12.532 | LR: 1.0e-04\n",
      "Epoch 264/500 | Train Loss: 8.9986 | Val Loss: 7.8387 | Val MAE: 7.839 | LR: 1.0e-04\n",
      "Epoch 265/500 | Train Loss: 9.0554 | Val Loss: 8.3394 | Val MAE: 8.339 | LR: 1.0e-04\n",
      "Epoch 266/500 | Train Loss: 9.4258 | Val Loss: 7.8250 | Val MAE: 7.825 | LR: 1.0e-04\n",
      "Epoch 267/500 | Train Loss: 9.1524 | Val Loss: 7.9903 | Val MAE: 7.990 | LR: 1.0e-04\n",
      "Epoch 268/500 | Train Loss: 9.1527 | Val Loss: 8.4886 | Val MAE: 8.489 | LR: 1.0e-04\n",
      "Epoch 269/500 | Train Loss: 9.1813 | Val Loss: 7.4766 | Val MAE: 7.477 | LR: 1.0e-04\n",
      "Epoch 270/500 | Train Loss: 9.4002 | Val Loss: 7.4985 | Val MAE: 7.499 | LR: 1.0e-04\n",
      "Epoch 271/500 | Train Loss: 9.1036 | Val Loss: 9.2651 | Val MAE: 9.265 | LR: 1.0e-04\n",
      "\n",
      "Early stopping triggered for axial slice 80 after 50 epochs without improvement.\n",
      "\n",
      "--- Training Finished for axial Slice 80 ---\n",
      "Best Validation MAE: 7.144 achieved at epoch 221\n",
      "\n",
      "\n",
      "{'='*20} Overall Training Summary {'='*20}\n",
      "Slice sagittal_80: Best Val MAE = 8.129 (Epoch 225)\n",
      "Slice sagittal_125: Best Val MAE = 7.779 (Epoch 158)\n",
      "Slice coronal_125: Best Val MAE = 7.100 (Epoch 242)\n",
      "Slice axial_80: Best Val MAE = 7.144 (Epoch 221)\n",
      "\n",
      "Specific slice model training finished.\n",
      "Best models saved in: specific_slice_models_multi_dir\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F # For Global Average Pooling\n",
    "import os\n",
    "import gc\n",
    "# Removed: from tqdm import tqdm # For progress bars\n",
    "\n",
    "# --- Configuration ---\n",
    "# Base directory where the multi-slice preprocessed features reside\n",
    "FEATURE_ROOT = Path(\"/data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice\") # Updated Path\n",
    "CSV_PATH = Path(\"/data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\")\n",
    "MODELS_OUTPUT_DIR = Path(\"./specific_slice_models_multi_dir\") # Directory to save best models\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\" # Use specific GPU if desired\n",
    "SPLITS = [\"train\", \"validation\", \"test\"] # Define dataset splits\n",
    "\n",
    "# Define the target slices\n",
    "# The script will look for files like 'slice_sagittal_80.npy', 'slice_coronal_125.npy', etc.\n",
    "# inside each subject's folder within the FEATURE_ROOT/split directories.\n",
    "TARGET_SLICES = [\n",
    "    {'orientation': 'sagittal', 'index': 80},\n",
    "    {'orientation': 'sagittal', 'index': 125},\n",
    "    {'orientation': 'coronal',  'index': 125},\n",
    "    {'orientation': 'axial',    'index': 80},\n",
    "]\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "EPOCHS = 500 # Adjust as needed\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32 # Adjust based on GPU memory\n",
    "WEIGHT_DECAY = 1e-5\n",
    "SCHEDULER_PATIENCE_PERCENT = 0.10 # e.g., 10% of EPOCHS for LR scheduler patience\n",
    "EARLY_STOPPING_PATIENCE = 50 # Number of epochs to wait for val_mae improvement\n",
    "NUM_WORKERS = 4 # Dataloader workers\n",
    "\n",
    "# --- Model Definition (Using AgeMLPWithAttentionBN) ---\n",
    "class AgeMLPWithAttentionBN(nn.Module):\n",
    "    def __init__(self, input_dim=256, embed_dim=256, num_heads=8,\n",
    "                 hidden_dim1=128, hidden_dim2=64, hidden_dim3=32, dropout_rate=0.3):\n",
    "        super(AgeMLPWithAttentionBN, self).__init__()\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\")\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=False)\n",
    "        self.norm_attn = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1), nn.BatchNorm1d(hidden_dim1), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2), nn.BatchNorm1d(hidden_dim2), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3), nn.BatchNorm1d(hidden_dim3), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, input_dim)\n",
    "        x_attn = x.unsqueeze(0) # (1, batch_size, input_dim)\n",
    "        attn_output, _ = self.attention(x_attn, x_attn, x_attn)\n",
    "        x = self.norm_attn(x_attn + attn_output) # Add residual\n",
    "        x = x.squeeze(0) # (batch_size, input_dim)\n",
    "        output = self.mlp(x)\n",
    "        return output.squeeze(-1) # (batch_size,)\n",
    "\n",
    "# --- Dataset Definition (Modified for Multi-Slice Directory Structure) ---\n",
    "class SliceAgePredictionDataset(Dataset):\n",
    "    def __init__(self, feature_root, csv_path, split, slice_index, orientation):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_root (Path): Base directory containing split folders\n",
    "                                 (e.g., .../BrainAGE_preprocessed_multi_slice).\n",
    "            csv_path (Path): Path to the CSV file with metadata.\n",
    "            split (str): The dataset split ('train', 'validation', or 'test').\n",
    "            slice_index (int): The specific slice index to load features for.\n",
    "            orientation (str): The orientation ('sagittal', 'coronal', 'axial').\n",
    "        \"\"\"\n",
    "        self.feature_root = Path(feature_root)\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.split = split\n",
    "        self.slice_index = slice_index\n",
    "        self.orientation = orientation\n",
    "        self.split_dir = self.feature_root / self.split\n",
    "\n",
    "        print(f\"\\n[Dataset Init] Split: {self.split}, Orientation: {self.orientation}, Slice: {self.slice_index}\")\n",
    "        print(f\"Scanning subjects in: {self.split_dir}\")\n",
    "        print(f\"Loading metadata from: {self.csv_path}\")\n",
    "\n",
    "        # Check if directories/files exist\n",
    "        if not self.feature_root.is_dir():\n",
    "             raise FileNotFoundError(f\"Feature root directory not found: {self.feature_root}\")\n",
    "        if not self.split_dir.is_dir():\n",
    "            raise FileNotFoundError(f\"Split directory not found: {self.split_dir}\")\n",
    "        if not self.csv_path.is_file():\n",
    "            raise FileNotFoundError(f\"CSV file not found: {self.csv_path}\")\n",
    "\n",
    "        # Load CSV and build filename -> age lookup\n",
    "        try:\n",
    "            df = pd.read_csv(self.csv_path)\n",
    "            self.meta_dict = df.set_index('filename')['age'].to_dict()\n",
    "\n",
    "            # Create mapping from directory name to original filename\n",
    "            self.subject_dir_to_filename = {}\n",
    "            nii_gz_keys = {k.replace(\".nii.gz\", \"\") for k in self.meta_dict.keys()}\n",
    "            nii_keys = {k.replace(\".nii\", \"\") for k in self.meta_dict.keys()}\n",
    "\n",
    "            potential_subject_dirs = [d.name for d in self.split_dir.iterdir() if d.is_dir()]\n",
    "            mapped_count = 0\n",
    "            for subj_dir_name in potential_subject_dirs:\n",
    "                matched_key = None\n",
    "                base_subj_name = subj_dir_name.split('_mri_brainmask')[0] # Example heuristic\n",
    "\n",
    "                for key_base in nii_gz_keys:\n",
    "                    if key_base.startswith(base_subj_name):\n",
    "                         matched_key = key_base + \".nii.gz\"\n",
    "                         break\n",
    "                if not matched_key:\n",
    "                     for key_base in nii_keys:\n",
    "                          if key_base.startswith(base_subj_name):\n",
    "                               matched_key = key_base + \".nii\"\n",
    "                               break\n",
    "\n",
    "                if matched_key and matched_key in self.meta_dict:\n",
    "                    self.subject_dir_to_filename[subj_dir_name] = matched_key\n",
    "                    mapped_count += 1\n",
    "\n",
    "            print(f\"Loaded metadata for {len(self.meta_dict)} subjects from CSV.\")\n",
    "            print(f\"Successfully mapped {mapped_count} subject directories in '{self.split}' to CSV entries.\")\n",
    "            if mapped_count < len(potential_subject_dirs):\n",
    "                 print(f\"Warning: Could not map {len(potential_subject_dirs) - mapped_count} directories to CSV.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading or processing CSV {self.csv_path} or mapping directories: {e}\")\n",
    "\n",
    "        # Find all subject directories in the split\n",
    "        all_subject_dirs = [d for d in self.split_dir.iterdir() if d.is_dir()]\n",
    "        self.valid_slice_files = []\n",
    "        missing_meta_count = 0\n",
    "        missing_slice_count = 0\n",
    "\n",
    "        print(f\"Scanning {len(all_subject_dirs)} potential subject directories for slice {self.orientation}_{self.slice_index}...\")\n",
    "\n",
    "        for subject_dir in all_subject_dirs:\n",
    "            subject_dir_name = subject_dir.name\n",
    "            original_filename = self.subject_dir_to_filename.get(subject_dir_name)\n",
    "            if original_filename:\n",
    "                slice_filename = f\"slice_{self.orientation}_{self.slice_index}.npy\"\n",
    "                expected_slice_path = subject_dir / slice_filename\n",
    "                if expected_slice_path.is_file():\n",
    "                    self.valid_slice_files.append(expected_slice_path)\n",
    "                else:\n",
    "                    missing_slice_count += 1\n",
    "            else:\n",
    "                missing_meta_count += 1\n",
    "\n",
    "        if missing_meta_count > 0:\n",
    "            print(f\"Info: {missing_meta_count} subject directories were skipped as they couldn't be mapped to metadata.\")\n",
    "        if missing_slice_count > 0:\n",
    "            print(f\"Warning: {missing_slice_count} subjects with metadata were missing slice {self.orientation}_{self.slice_index}.\")\n",
    "\n",
    "        if not self.valid_slice_files:\n",
    "             raise RuntimeError(f\"No valid slice files found for {self.orientation} slice {self.slice_index} in {self.split_dir} with matching metadata.\")\n",
    "\n",
    "        print(f\"Found {len(self.valid_slice_files)} valid files for {self.orientation} slice {self.slice_index} in split {self.split}.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_slice_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slice_path = self.valid_slice_files[idx]\n",
    "        subject_dir_name = slice_path.parent.name\n",
    "        original_filename = self.subject_dir_to_filename.get(subject_dir_name)\n",
    "        if not original_filename:\n",
    "             raise ValueError(f\"Could not find original filename mapping for subject directory {subject_dir_name}\")\n",
    "\n",
    "        try:\n",
    "            embedding = np.load(slice_path)\n",
    "            embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "\n",
    "            # Apply Global Average Pooling (GAP)\n",
    "            if len(embedding_tensor.shape) == 4 and embedding_tensor.shape[0] == 1:\n",
    "                 pooled_embedding = F.adaptive_avg_pool2d(embedding_tensor, (1, 1)).squeeze()\n",
    "            elif len(embedding_tensor.shape) == 3:\n",
    "                 pooled_embedding = F.adaptive_avg_pool2d(embedding_tensor.unsqueeze(0), (1, 1)).squeeze()\n",
    "            elif len(embedding_tensor.shape) == 1:\n",
    "                 pooled_embedding = embedding_tensor\n",
    "            else:\n",
    "                 raise ValueError(f\"Unexpected embedding shape {embedding_tensor.shape} for {slice_path}\")\n",
    "\n",
    "            if pooled_embedding.shape[0] != 256:\n",
    "                 raise ValueError(f\"Pooled embedding channel dimension is not 256 for {slice_path}: {pooled_embedding.shape}\")\n",
    "\n",
    "            age = self.meta_dict[original_filename]\n",
    "            age_tensor = torch.tensor(age, dtype=torch.float32)\n",
    "\n",
    "            return pooled_embedding, age_tensor\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing file {slice_path}: {e}\")\n",
    "            raise e\n",
    "\n",
    "# --- Training and Evaluation Functions (Progress Bars Removed) ---\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "    # Removed tqdm wrapper\n",
    "    for features, ages in loader:\n",
    "        features, ages = features.to(device), ages.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, ages)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss * features.size(0)\n",
    "        num_samples += features.size(0)\n",
    "        # Removed pbar.set_postfix\n",
    "    return total_loss / num_samples\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    num_samples = 0\n",
    "    # Removed tqdm wrapper\n",
    "    for features, ages in loader:\n",
    "        features, ages = features.to(device), ages.to(device)\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, ages)\n",
    "        total_loss += loss.item() * features.size(0)\n",
    "        mae = F.l1_loss(predictions, ages, reduction='sum')\n",
    "        total_mae += mae.item()\n",
    "        num_samples += features.size(0)\n",
    "        # Removed pbar.set_postfix\n",
    "\n",
    "    avg_loss = total_loss / num_samples\n",
    "    avg_mae = total_mae / num_samples\n",
    "    return avg_loss, avg_mae\n",
    "\n",
    "# --- Main Training Loop for Each Specific Slice ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting specific slice MLP training from multi-slice directory...\")\n",
    "    print(f\"Target Slices:\")\n",
    "    for slice_info in TARGET_SLICES:\n",
    "        print(f\"  - Orientation: {slice_info['orientation']}, Index: {slice_info['index']}\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"Feature Root: {FEATURE_ROOT}\")\n",
    "    print(f\"CSV Path: {CSV_PATH}\")\n",
    "    print(f\"Models will be saved to: {MODELS_OUTPUT_DIR}\")\n",
    "    print(f\"Hyperparameters: Epochs={EPOCHS}, LR={LEARNING_RATE}, Batch={BATCH_SIZE}, ES_Patience={EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "    MODELS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for slice_info in TARGET_SLICES:\n",
    "        orientation = slice_info['orientation']\n",
    "        slice_idx = slice_info['index']\n",
    "\n",
    "        print(f\"\\n{'='*20} Training for Slice: {orientation} {slice_idx} {'='*20}\")\n",
    "        print(f\"Using feature root: {FEATURE_ROOT}\")\n",
    "\n",
    "        model_save_path = MODELS_OUTPUT_DIR / f\"best_model_{orientation}_slice_{slice_idx}.pth\"\n",
    "\n",
    "        try:\n",
    "            print(\"Setting up datasets...\")\n",
    "            train_dataset = SliceAgePredictionDataset(FEATURE_ROOT, CSV_PATH, 'train', slice_idx, orientation)\n",
    "            val_dataset = SliceAgePredictionDataset(FEATURE_ROOT, CSV_PATH, 'validation', slice_idx, orientation)\n",
    "\n",
    "            print(\"Setting up dataloaders...\")\n",
    "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "        except (FileNotFoundError, ValueError, RuntimeError) as e:\n",
    "            print(f\"Error initializing datasets/loaders for {orientation} slice {slice_idx}: {e}\")\n",
    "            print(f\"Skipping training for this slice.\")\n",
    "            results[f\"{orientation}_{slice_idx}\"] = {'best_val_mae': float('inf'), 'best_epoch': -1, 'error': str(e)}\n",
    "            if 'train_dataset' in locals(): del train_dataset\n",
    "            if 'val_dataset' in locals(): del val_dataset\n",
    "            if 'train_loader' in locals(): del train_loader\n",
    "            if 'val_loader' in locals(): del val_loader\n",
    "            gc.collect()\n",
    "            if DEVICE.startswith('cuda'): torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "        print(\"Initializing model, optimizer, scheduler...\")\n",
    "        model = AgeMLPWithAttentionBN().to(DEVICE)\n",
    "        criterion = nn.L1Loss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler_patience_epochs = max(1, int(EPOCHS * SCHEDULER_PATIENCE_PERCENT))\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=scheduler_patience_epochs, factor=0.5, verbose=False)\n",
    "\n",
    "        best_val_mae = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        best_epoch = -1\n",
    "\n",
    "        print(f\"\\n--- Starting Training for {orientation} Slice {slice_idx} ---\")\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "            val_loss, val_mae = evaluate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            # Print epoch results without progress bar updates\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val MAE: {val_mae:.3f} | LR: {current_lr:.1e}\")\n",
    "\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            if val_mae < best_val_mae:\n",
    "                best_val_mae = val_mae\n",
    "                best_epoch = epoch + 1\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"  -> New best Val MAE: {best_val_mae:.3f}. Saved model to {model_save_path}\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered for {orientation} slice {slice_idx} after {EARLY_STOPPING_PATIENCE} epochs without improvement.\")\n",
    "                break\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                 gc.collect()\n",
    "                 if DEVICE.startswith('cuda'):\n",
    "                     torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"\\n--- Training Finished for {orientation} Slice {slice_idx} ---\")\n",
    "        slice_key = f\"{orientation}_{slice_idx}\"\n",
    "        if best_epoch != -1:\n",
    "             print(f\"Best Validation MAE: {best_val_mae:.3f} achieved at epoch {best_epoch}\")\n",
    "             results[slice_key] = {'best_val_mae': best_val_mae, 'best_epoch': best_epoch}\n",
    "        else:\n",
    "             print(\"No improvement found during training.\")\n",
    "             results[slice_key] = {'best_val_mae': float('inf'), 'best_epoch': -1, 'error': 'No improvement'}\n",
    "\n",
    "        # Optional Test Set Evaluation Code (remains the same logic)\n",
    "        # ...\n",
    "\n",
    "        del model, optimizer, scheduler, train_dataset, val_dataset, train_loader, val_loader\n",
    "        gc.collect()\n",
    "        if DEVICE.startswith('cuda'):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Final Summary ---\n",
    "    print(\"\\n\\n{'='*20} Overall Training Summary {'='*20}\")\n",
    "    for slice_key, res in results.items():\n",
    "        if 'error' in res and res['error'] is not None:\n",
    "            print(f\"Slice {slice_key}: Error - {res['error']}\")\n",
    "        elif res['best_epoch'] == -1:\n",
    "             print(f\"Slice {slice_key}: No improvement found.\")\n",
    "        else:\n",
    "            test_mae_str = \"\"\n",
    "            if 'test_mae' in res:\n",
    "                test_mae_val = res['test_mae']\n",
    "                test_mae_str = f\" | Test MAE: {test_mae_val:.3f}\" if isinstance(test_mae_val, (int, float)) else f\" | Test MAE: {test_mae_val}\"\n",
    "\n",
    "            print(f\"Slice {slice_key}: Best Val MAE = {res['best_val_mae']:.3f} (Epoch {res['best_epoch']}){test_mae_str}\")\n",
    "\n",
    "    print(\"\\nSpecific slice model training finished.\")\n",
    "    print(f\"Best models saved in: {MODELS_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee328796",
   "metadata": {},
   "source": [
    "Average predictions from 4 MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "096b1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F # For Global Average Pooling\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# --- Configuration ---\n",
    "# Base directory where the multi-slice preprocessed features reside\n",
    "FEATURE_ROOT = Path(\"/data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice\") # Updated Path\n",
    "CSV_PATH = Path(\"/data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\")\n",
    "MODELS_OUTPUT_DIR = Path(\"./specific_slice_models_multi_dir\") # Directory to save best models\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\" # Use specific GPU if desired\n",
    "SPLITS = [\"train\", \"validation\", \"test\"] # Define dataset splits\n",
    "\n",
    "# Define the target slices\n",
    "# The script will look for files like 'slice_sagittal_80.npy', 'slice_coronal_125.npy', etc.\n",
    "# inside each subject's folder within the FEATURE_ROOT/split directories.\n",
    "TARGET_SLICES = [\n",
    "    {'orientation': 'sagittal', 'index': 80},\n",
    "    {'orientation': 'sagittal', 'index': 125},\n",
    "    {'orientation': 'coronal',  'index': 125},\n",
    "    {'orientation': 'axial',    'index': 80},\n",
    "]\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "EPOCHS = 500 # Adjust as needed\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32 # Adjust based on GPU memory\n",
    "WEIGHT_DECAY = 1e-5\n",
    "SCHEDULER_PATIENCE_PERCENT = 0.10 # e.g., 10% of EPOCHS for LR scheduler patience\n",
    "EARLY_STOPPING_PATIENCE = 50 # Number of epochs to wait for val_mae improvement\n",
    "NUM_WORKERS = 4 # Dataloader workers\n",
    "\n",
    "# --- Model Definition (Using AgeMLPWithAttentionBN) ---\n",
    "class AgeMLPWithAttentionBN(nn.Module):\n",
    "    def __init__(self, input_dim=256, embed_dim=256, num_heads=8,\n",
    "                 hidden_dim1=128, hidden_dim2=64, hidden_dim3=32, dropout_rate=0.3):\n",
    "        super(AgeMLPWithAttentionBN, self).__init__()\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\")\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=False)\n",
    "        self.norm_attn = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1), nn.BatchNorm1d(hidden_dim1), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2), nn.BatchNorm1d(hidden_dim2), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3), nn.BatchNorm1d(hidden_dim3), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, input_dim)\n",
    "        x_attn = x.unsqueeze(0) # (1, batch_size, input_dim)\n",
    "        attn_output, _ = self.attention(x_attn, x_attn, x_attn)\n",
    "        x = self.norm_attn(x_attn + attn_output) # Add residual\n",
    "        x = x.squeeze(0) # (batch_size, input_dim)\n",
    "        output = self.mlp(x)\n",
    "        return output.squeeze(-1) # (batch_size,)\n",
    "\n",
    "# --- Dataset Definition (Modified for Multi-Slice Directory Structure) ---\n",
    "class SliceAgePredictionDataset(Dataset):\n",
    "    def __init__(self, feature_root, csv_path, split, slice_index, orientation):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_root (Path): Base directory containing split folders\n",
    "                                 (e.g., .../BrainAGE_preprocessed_multi_slice).\n",
    "            csv_path (Path): Path to the CSV file with metadata.\n",
    "            split (str): The dataset split ('train', 'validation', or 'test').\n",
    "            slice_index (int): The specific slice index to load features for.\n",
    "            orientation (str): The orientation ('sagittal', 'coronal', 'axial').\n",
    "        \"\"\"\n",
    "        self.feature_root = Path(feature_root)\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.split = split\n",
    "        self.slice_index = slice_index\n",
    "        self.orientation = orientation\n",
    "        self.split_dir = self.feature_root / self.split\n",
    "\n",
    "        print(f\"\\n[Dataset Init] Split: {self.split}, Orientation: {self.orientation}, Slice: {self.slice_index}\")\n",
    "        print(f\"Scanning subjects in: {self.split_dir}\")\n",
    "        print(f\"Loading metadata from: {self.csv_path}\")\n",
    "\n",
    "        # Check if directories/files exist\n",
    "        if not self.feature_root.is_dir():\n",
    "             raise FileNotFoundError(f\"Feature root directory not found: {self.feature_root}\")\n",
    "        if not self.split_dir.is_dir():\n",
    "            raise FileNotFoundError(f\"Split directory not found: {self.split_dir}\")\n",
    "        if not self.csv_path.is_file():\n",
    "            raise FileNotFoundError(f\"CSV file not found: {self.csv_path}\")\n",
    "\n",
    "        # Load CSV and build filename -> age lookup\n",
    "        try:\n",
    "            df = pd.read_csv(self.csv_path)\n",
    "            self.meta_dict = df.set_index('filename')['age'].to_dict()\n",
    "\n",
    "            # Create mapping from directory name to original filename\n",
    "            self.subject_dir_to_filename = {}\n",
    "            nii_gz_keys = {k.replace(\".nii.gz\", \"\") for k in self.meta_dict.keys()}\n",
    "            nii_keys = {k.replace(\".nii\", \"\") for k in self.meta_dict.keys()}\n",
    "\n",
    "            potential_subject_dirs = [d.name for d in self.split_dir.iterdir() if d.is_dir()]\n",
    "            mapped_count = 0\n",
    "            for subj_dir_name in potential_subject_dirs:\n",
    "                matched_key = None\n",
    "                base_subj_name = subj_dir_name.split('_mri_brainmask')[0] # Example heuristic\n",
    "\n",
    "                for key_base in nii_gz_keys:\n",
    "                    if key_base.startswith(base_subj_name):\n",
    "                         matched_key = key_base + \".nii.gz\"\n",
    "                         break\n",
    "                if not matched_key:\n",
    "                     for key_base in nii_keys:\n",
    "                          if key_base.startswith(base_subj_name):\n",
    "                               matched_key = key_base + \".nii\"\n",
    "                               break\n",
    "\n",
    "                if matched_key and matched_key in self.meta_dict:\n",
    "                    self.subject_dir_to_filename[subj_dir_name] = matched_key\n",
    "                    mapped_count += 1\n",
    "\n",
    "            print(f\"Loaded metadata for {len(self.meta_dict)} subjects from CSV.\")\n",
    "            print(f\"Successfully mapped {mapped_count} subject directories in '{self.split}' to CSV entries.\")\n",
    "            if mapped_count < len(potential_subject_dirs):\n",
    "                 print(f\"Warning: Could not map {len(potential_subject_dirs) - mapped_count} directories to CSV.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading or processing CSV {self.csv_path} or mapping directories: {e}\")\n",
    "\n",
    "        # Find all subject directories in the split\n",
    "        all_subject_dirs = [d for d in self.split_dir.iterdir() if d.is_dir()]\n",
    "        self.valid_slice_files = []\n",
    "        missing_meta_count = 0\n",
    "        missing_slice_count = 0\n",
    "\n",
    "        print(f\"Scanning {len(all_subject_dirs)} potential subject directories for slice {self.orientation}_{self.slice_index}...\")\n",
    "\n",
    "        for subject_dir in all_subject_dirs:\n",
    "            subject_dir_name = subject_dir.name\n",
    "            original_filename = self.subject_dir_to_filename.get(subject_dir_name)\n",
    "            if original_filename:\n",
    "                slice_filename = f\"slice_{self.orientation}_{self.slice_index}.npy\"\n",
    "                expected_slice_path = subject_dir / slice_filename\n",
    "                if expected_slice_path.is_file():\n",
    "                    self.valid_slice_files.append(expected_slice_path)\n",
    "                else:\n",
    "                    missing_slice_count += 1\n",
    "            else:\n",
    "                missing_meta_count += 1\n",
    "\n",
    "        if missing_meta_count > 0:\n",
    "            print(f\"Info: {missing_meta_count} subject directories were skipped as they couldn't be mapped to metadata.\")\n",
    "        if missing_slice_count > 0:\n",
    "            print(f\"Warning: {missing_slice_count} subjects with metadata were missing slice {self.orientation}_{self.slice_index}.\")\n",
    "\n",
    "        if not self.valid_slice_files:\n",
    "             raise RuntimeError(f\"No valid slice files found for {self.orientation} slice {self.slice_index} in {self.split_dir} with matching metadata.\")\n",
    "\n",
    "        print(f\"Found {len(self.valid_slice_files)} valid files for {self.orientation} slice {self.slice_index} in split {self.split}.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_slice_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slice_path = self.valid_slice_files[idx]\n",
    "        subject_dir_name = slice_path.parent.name\n",
    "        original_filename = self.subject_dir_to_filename.get(subject_dir_name)\n",
    "        if not original_filename:\n",
    "             raise ValueError(f\"Could not find original filename mapping for subject directory {subject_dir_name}\")\n",
    "\n",
    "        try:\n",
    "            embedding = np.load(slice_path)\n",
    "            embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "\n",
    "            # Apply Global Average Pooling (GAP)\n",
    "            if len(embedding_tensor.shape) == 4 and embedding_tensor.shape[0] == 1:\n",
    "                 pooled_embedding = F.adaptive_avg_pool2d(embedding_tensor, (1, 1)).squeeze()\n",
    "            elif len(embedding_tensor.shape) == 3:\n",
    "                 pooled_embedding = F.adaptive_avg_pool2d(embedding_tensor.unsqueeze(0), (1, 1)).squeeze()\n",
    "            elif len(embedding_tensor.shape) == 1:\n",
    "                 pooled_embedding = embedding_tensor\n",
    "            else:\n",
    "                 raise ValueError(f\"Unexpected embedding shape {embedding_tensor.shape} for {slice_path}\")\n",
    "\n",
    "            if pooled_embedding.shape[0] != 256:\n",
    "                 raise ValueError(f\"Pooled embedding channel dimension is not 256 for {slice_path}: {pooled_embedding.shape}\")\n",
    "\n",
    "            age = self.meta_dict[original_filename]\n",
    "            age_tensor = torch.tensor(age, dtype=torch.float32)\n",
    "\n",
    "            return pooled_embedding, age_tensor\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing file {slice_path}: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "011c9ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Test Set Evaluation ---\n",
      "Loading models and creating test datasets/loaders...\n",
      "Loading test data for sagittal_80...\n",
      "\n",
      "[Dataset Init] Split: test, Orientation: sagittal, Slice: 80\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/test\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 296 subject directories in 'test' to CSV entries.\n",
      "Scanning 296 potential subject directories for slice sagittal_80...\n",
      "Found 296 valid files for sagittal slice 80 in split test.\n",
      "Loading model for sagittal_80 from specific_slice_models_multi_dir/best_model_sagittal_slice_80.pth...\n",
      "Successfully loaded model and data for sagittal_80.\n",
      "Loading test data for sagittal_125...\n",
      "\n",
      "[Dataset Init] Split: test, Orientation: sagittal, Slice: 125\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/test\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 296 subject directories in 'test' to CSV entries.\n",
      "Scanning 296 potential subject directories for slice sagittal_125...\n",
      "Found 296 valid files for sagittal slice 125 in split test.\n",
      "Loading model for sagittal_125 from specific_slice_models_multi_dir/best_model_sagittal_slice_125.pth...\n",
      "Successfully loaded model and data for sagittal_125.\n",
      "Loading test data for coronal_125...\n",
      "\n",
      "[Dataset Init] Split: test, Orientation: coronal, Slice: 125\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/test\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 296 subject directories in 'test' to CSV entries.\n",
      "Scanning 296 potential subject directories for slice coronal_125...\n",
      "Found 296 valid files for coronal slice 125 in split test.\n",
      "Loading model for coronal_125 from specific_slice_models_multi_dir/best_model_coronal_slice_125.pth...\n",
      "Successfully loaded model and data for coronal_125.\n",
      "Loading test data for axial_80...\n",
      "\n",
      "[Dataset Init] Split: test, Orientation: axial, Slice: 80\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/test\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 296 subject directories in 'test' to CSV entries.\n",
      "Scanning 296 potential subject directories for slice axial_80...\n",
      "Found 296 valid files for axial slice 80 in split test.\n",
      "Loading model for axial_80 from specific_slice_models_multi_dir/best_model_axial_slice_80.pth...\n",
      "Successfully loaded model and data for axial_80.\n",
      "\n",
      "Evaluating individual models on the test set...\n",
      "Evaluating sagittal_80...\n",
      "  sagittal_80 - Test MAE: 8.238, Test Loss: 8.2379, Samples: 296\n",
      "Evaluating sagittal_125...\n",
      "  sagittal_125 - Test MAE: 9.229, Test Loss: 9.2287, Samples: 296\n",
      "Evaluating coronal_125...\n",
      "  coronal_125 - Test MAE: 7.704, Test Loss: 7.7038, Samples: 296\n",
      "Evaluating axial_80...\n",
      "  axial_80 - Test MAE: 7.059, Test Loss: 7.0586, Samples: 296\n",
      "\n",
      "Calculating MAE for averaged predictions...\n",
      "Number of test samples per model: 296\n",
      "Averaging predictions from 4 models: ['sagittal_80', 'sagittal_125', 'coronal_125', 'axial_80']\n",
      "\n",
      "---> Average Prediction Test MAE: 6.868 <---\n",
      "\n",
      "Cleaning up resources...\n",
      "\n",
      "Test evaluation finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Test Set Evaluation ---\n",
    "print(\"\\n--- Starting Test Set Evaluation ---\")\n",
    "\n",
    "models = {}\n",
    "test_loaders = {}\n",
    "test_results = {}\n",
    "all_predictions = defaultdict(list)\n",
    "all_ages_collected = defaultdict(list) # To store ages collected per model run for verification\n",
    "\n",
    "# 1. Load Models and Create Test Loaders\n",
    "print(\"Loading models and creating test datasets/loaders...\")\n",
    "# Ensure required variables (DEVICE, MODELS_OUTPUT_DIR, TARGET_SLICES, FEATURE_ROOT, CSV_PATH,\n",
    "# AgeMLPWithAttentionBN, SliceAgePredictionDataset, BATCH_SIZE, NUM_WORKERS)\n",
    "# are available from the previous cell.\n",
    "successful_loads = 0\n",
    "for slice_info in TARGET_SLICES:\n",
    "    orientation = slice_info['orientation']\n",
    "    slice_idx = slice_info['index']\n",
    "    slice_key = f\"{orientation}_{slice_idx}\"\n",
    "    model_save_path = MODELS_OUTPUT_DIR / f\"best_model_{orientation}_slice_{slice_idx}.pth\"\n",
    "\n",
    "    if model_save_path.is_file():\n",
    "        print(f\"Loading test data for {slice_key}...\")\n",
    "        try:\n",
    "            # Use 'test' split for evaluation\n",
    "            test_dataset = SliceAgePredictionDataset(FEATURE_ROOT, CSV_PATH, 'test', slice_idx, orientation)\n",
    "            # shuffle=False is crucial for potentially aligning predictions later\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "            test_loaders[slice_key] = test_loader\n",
    "\n",
    "            print(f\"Loading model for {slice_key} from {model_save_path}...\")\n",
    "            model = AgeMLPWithAttentionBN().to(DEVICE)\n",
    "            # Load state dict, ensuring map_location handles CPU/GPU transfers\n",
    "            model.load_state_dict(torch.load(model_save_path, map_location=DEVICE))\n",
    "            model.eval() # Set model to evaluation mode\n",
    "            models[slice_key] = model\n",
    "            print(f\"Successfully loaded model and data for {slice_key}.\")\n",
    "            successful_loads += 1\n",
    "\n",
    "        except (FileNotFoundError, ValueError, RuntimeError) as e:\n",
    "            print(f\"Error initializing test dataset/loader for {slice_key}: {e}\")\n",
    "            print(f\"Skipping evaluation for this slice.\")\n",
    "        except Exception as e:\n",
    "             print(f\"An unexpected error occurred loading data/model for {slice_key}: {e}\")\n",
    "             print(f\"Skipping evaluation for this slice.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Model file not found for {slice_key} at {model_save_path}. Skipping.\")\n",
    "\n",
    "if not models:\n",
    "    print(\"No models were loaded successfully. Exiting evaluation.\")\n",
    "elif successful_loads < len(TARGET_SLICES):\n",
    "    print(f\"Warning: Only loaded {successful_loads}/{len(TARGET_SLICES)} models. Averaging will be based on loaded models.\")\n",
    "\n",
    "if models: # Proceed only if at least one model was loaded\n",
    "    # Assume criterion = nn.L1Loss() was used for training/validation MAE reporting\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    # 2. Evaluate Individual Models and Collect Predictions\n",
    "    print(\"\\nEvaluating individual models on the test set...\")\n",
    "    with torch.no_grad():\n",
    "        for slice_key, model in models.items():\n",
    "            print(f\"Evaluating {slice_key}...\")\n",
    "            loader = test_loaders[slice_key]\n",
    "            total_loss = 0.0\n",
    "            total_mae = 0.0\n",
    "            num_samples = 0\n",
    "\n",
    "            for i, (features, ages) in enumerate(loader):\n",
    "                features, ages = features.to(DEVICE), ages.to(DEVICE)\n",
    "                predictions = model(features)\n",
    "\n",
    "                # Store predictions and ages for averaging later\n",
    "                # Ensure storing happens in the same order by iterating through loader\n",
    "                all_predictions[slice_key].extend(predictions.cpu().numpy())\n",
    "                # Store ages per model run to verify consistency later\n",
    "                all_ages_collected[slice_key].extend(ages.cpu().numpy())\n",
    "\n",
    "                # Calculate metrics for individual model report\n",
    "                loss = criterion(predictions, ages)\n",
    "                total_loss += loss.item() * features.size(0)\n",
    "                mae = F.l1_loss(predictions, ages, reduction='sum') # Sum MAE over batch\n",
    "                total_mae += mae.item()\n",
    "                num_samples += features.size(0)\n",
    "\n",
    "            if num_samples > 0:\n",
    "                avg_loss = total_loss / num_samples\n",
    "                avg_mae = total_mae / num_samples\n",
    "                test_results[slice_key] = {'test_mae': avg_mae, 'test_loss': avg_loss, 'num_samples': num_samples}\n",
    "                print(f\"  {slice_key} - Test MAE: {avg_mae:.3f}, Test Loss: {avg_loss:.4f}, Samples: {num_samples}\")\n",
    "            else:\n",
    "                 print(f\"  {slice_key} - No samples evaluated.\")\n",
    "                 test_results[slice_key] = {'test_mae': float('inf'), 'test_loss': float('inf'), 'num_samples': 0}\n",
    "\n",
    "\n",
    "    # 3. Verify Data Consistency and Calculate Averaged MAE\n",
    "    print(\"\\nCalculating MAE for averaged predictions...\")\n",
    "\n",
    "    # Check if we have predictions from all loaded models\n",
    "    if len(all_predictions) != len(models):\n",
    "        print(\"Error: Prediction collection failed for some models. Cannot compute average.\")\n",
    "    else:\n",
    "        # Check for consistent number of samples across models\n",
    "        num_samples_list = [len(preds) for preds in all_predictions.values()]\n",
    "        if len(set(num_samples_list)) > 1:\n",
    "            print(f\"Error: Inconsistent number of predictions across models: {num_samples_list}. Cannot compute average reliably.\")\n",
    "            print(\"This might happen if test datasets for different slices have different numbers of valid files.\")\n",
    "        elif not num_samples_list or num_samples_list[0] == 0:\n",
    "             print(\"No test samples found to calculate average MAE.\")\n",
    "        else:\n",
    "            num_test_samples = num_samples_list[0]\n",
    "            print(f\"Number of test samples per model: {num_test_samples}\")\n",
    "\n",
    "            # Verify that the true ages collected are consistent across runs\n",
    "            # This assumes shuffle=False in DataLoader worked as expected\n",
    "            first_key = list(all_ages_collected.keys())[0]\n",
    "            reference_ages = np.array(all_ages_collected[first_key])\n",
    "            consistent_ages = True\n",
    "            for key in all_ages_collected:\n",
    "                if len(all_ages_collected[key]) != num_test_samples or not np.array_equal(reference_ages, np.array(all_ages_collected[key])):\n",
    "                    consistent_ages = False\n",
    "                    print(f\"Error: True ages collected for model {key} (count: {len(all_ages_collected[key])}) do not match the reference ages (count: {len(reference_ages)}).\")\n",
    "                    break\n",
    "\n",
    "            if not consistent_ages:\n",
    "                 print(\"Cannot compute average MAE due to inconsistent ground truth ages across loaders.\")\n",
    "            else:\n",
    "                # Calculate average predictions\n",
    "                avg_preds = np.zeros(num_test_samples)\n",
    "                print(f\"Averaging predictions from {len(all_predictions)} models: {list(all_predictions.keys())}\")\n",
    "                for slice_key in all_predictions:\n",
    "                    avg_preds += np.array(all_predictions[slice_key])\n",
    "                avg_preds /= len(all_predictions) # Divide by the number of models we actually have predictions for\n",
    "\n",
    "                # Calculate MAE for the averaged predictions using the verified reference_ages\n",
    "                average_mae = np.mean(np.abs(avg_preds - reference_ages))\n",
    "                print(f\"\\n---> Average Prediction Test MAE: {average_mae:.3f} <---\")\n",
    "\n",
    "\n",
    "# 4. Clean up\n",
    "print(\"\\nCleaning up resources...\")\n",
    "del models, test_loaders, all_predictions, all_ages_collected, test_results\n",
    "if 'model' in locals(): del model # Ensure loop variables are cleared\n",
    "if 'test_loader' in locals(): del test_loader\n",
    "if 'test_dataset' in locals(): del test_dataset\n",
    "gc.collect()\n",
    "if DEVICE.startswith('cuda'):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nTest evaluation finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34919ff0",
   "metadata": {},
   "source": [
    "Early fusion using concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab1364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combined MLP training using concatenated slices...\n",
      "Target Slices for Concatenation: [{'orientation': 'sagittal', 'index': 80}, {'orientation': 'sagittal', 'index': 125}, {'orientation': 'coronal', 'index': 125}, {'orientation': 'axial', 'index': 80}]\n",
      "Concatenated Input Dimension: 1024\n",
      "Using device: cuda:1\n",
      "Feature Root: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice\n",
      "CSV Path: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Model will be saved to: combined_slice_model\n",
      "Hyperparameters: Epochs=500, LR=0.0001, Batch=32, ES_Patience=50\n",
      "\n",
      "Setting up combined datasets...\n",
      "\n",
      "[Dataset Init - Combined] Split: train\n",
      "Target Slices: [{'orientation': 'sagittal', 'index': 80}, {'orientation': 'sagittal', 'index': 125}, {'orientation': 'coronal', 'index': 125}, {'orientation': 'axial', 'index': 80}]\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/train\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 2274 subject directories in 'train' to CSV entries.\n",
      "Warning: Could not map 1 directories to CSV.\n",
      "Scanning 2275 potential subject directories for all target slices...\n",
      "Info: 1 subject directories were skipped (no metadata mapping).\n",
      "Found 2274 valid subjects with all required slices in split train.\n",
      "\n",
      "[Dataset Init - Combined] Split: validation\n",
      "Target Slices: [{'orientation': 'sagittal', 'index': 80}, {'orientation': 'sagittal', 'index': 125}, {'orientation': 'coronal', 'index': 125}, {'orientation': 'axial', 'index': 80}]\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/validation\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 280 subject directories in 'validation' to CSV entries.\n",
      "Scanning 280 potential subject directories for all target slices...\n",
      "Found 280 valid subjects with all required slices in split validation.\n",
      "Setting up dataloaders...\n",
      "\n",
      "Initializing model, optimizer, scheduler...\n",
      "\n",
      "--- Starting Combined Model Training ---\n",
      "Epoch 1/500 | Train Loss: 53.8675 | Val Loss: 53.3167 | Val MAE: 53.317 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.317. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 2/500 | Train Loss: 53.7566 | Val Loss: 53.1651 | Val MAE: 53.165 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.165. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 3/500 | Train Loss: 53.6476 | Val Loss: 53.1306 | Val MAE: 53.131 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.131. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 4/500 | Train Loss: 53.5192 | Val Loss: 52.9986 | Val MAE: 52.999 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.999. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 5/500 | Train Loss: 53.4053 | Val Loss: 52.9135 | Val MAE: 52.913 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.913. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 6/500 | Train Loss: 53.2675 | Val Loss: 52.7379 | Val MAE: 52.738 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.738. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 7/500 | Train Loss: 53.1358 | Val Loss: 52.6773 | Val MAE: 52.677 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.677. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 8/500 | Train Loss: 53.0265 | Val Loss: 52.5191 | Val MAE: 52.519 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.519. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 9/500 | Train Loss: 52.8994 | Val Loss: 52.3519 | Val MAE: 52.352 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.352. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 10/500 | Train Loss: 52.7370 | Val Loss: 52.3487 | Val MAE: 52.349 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.349. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 11/500 | Train Loss: 52.6155 | Val Loss: 52.1258 | Val MAE: 52.126 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.126. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 12/500 | Train Loss: 52.4319 | Val Loss: 52.0346 | Val MAE: 52.035 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.035. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 13/500 | Train Loss: 52.2843 | Val Loss: 51.8559 | Val MAE: 51.856 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.856. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 14/500 | Train Loss: 52.1798 | Val Loss: 51.5153 | Val MAE: 51.515 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.515. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 15/500 | Train Loss: 51.9902 | Val Loss: 51.6311 | Val MAE: 51.631 | LR: 1.0e-04\n",
      "Epoch 16/500 | Train Loss: 51.8127 | Val Loss: 51.3589 | Val MAE: 51.359 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.359. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 17/500 | Train Loss: 51.6439 | Val Loss: 51.1851 | Val MAE: 51.185 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.185. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 18/500 | Train Loss: 51.4522 | Val Loss: 50.9596 | Val MAE: 50.960 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.960. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 19/500 | Train Loss: 51.2969 | Val Loss: 50.5844 | Val MAE: 50.584 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.584. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 20/500 | Train Loss: 51.0656 | Val Loss: 50.2853 | Val MAE: 50.285 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.285. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 21/500 | Train Loss: 50.8878 | Val Loss: 50.5753 | Val MAE: 50.575 | LR: 1.0e-04\n",
      "Epoch 22/500 | Train Loss: 50.6732 | Val Loss: 50.1481 | Val MAE: 50.148 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.148. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 23/500 | Train Loss: 50.4217 | Val Loss: 49.5430 | Val MAE: 49.543 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.543. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 24/500 | Train Loss: 50.2523 | Val Loss: 49.7243 | Val MAE: 49.724 | LR: 1.0e-04\n",
      "Epoch 25/500 | Train Loss: 50.0409 | Val Loss: 49.0900 | Val MAE: 49.090 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.090. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 26/500 | Train Loss: 49.8012 | Val Loss: 49.3813 | Val MAE: 49.381 | LR: 1.0e-04\n",
      "Epoch 27/500 | Train Loss: 49.4858 | Val Loss: 48.9863 | Val MAE: 48.986 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.986. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 28/500 | Train Loss: 49.2170 | Val Loss: 48.9670 | Val MAE: 48.967 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.967. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 29/500 | Train Loss: 48.9818 | Val Loss: 48.5670 | Val MAE: 48.567 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.567. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 30/500 | Train Loss: 48.7546 | Val Loss: 47.9865 | Val MAE: 47.986 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.986. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 31/500 | Train Loss: 48.4191 | Val Loss: 47.1651 | Val MAE: 47.165 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.165. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 32/500 | Train Loss: 48.1327 | Val Loss: 47.9338 | Val MAE: 47.934 | LR: 1.0e-04\n",
      "Epoch 33/500 | Train Loss: 47.8448 | Val Loss: 47.4528 | Val MAE: 47.453 | LR: 1.0e-04\n",
      "Epoch 34/500 | Train Loss: 47.5758 | Val Loss: 47.5786 | Val MAE: 47.579 | LR: 1.0e-04\n",
      "Epoch 35/500 | Train Loss: 47.2097 | Val Loss: 45.6853 | Val MAE: 45.685 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.685. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 36/500 | Train Loss: 46.9050 | Val Loss: 46.4646 | Val MAE: 46.465 | LR: 1.0e-04\n",
      "Epoch 37/500 | Train Loss: 46.5551 | Val Loss: 46.9863 | Val MAE: 46.986 | LR: 1.0e-04\n",
      "Epoch 38/500 | Train Loss: 46.2786 | Val Loss: 45.9632 | Val MAE: 45.963 | LR: 1.0e-04\n",
      "Epoch 39/500 | Train Loss: 46.0388 | Val Loss: 45.6761 | Val MAE: 45.676 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.676. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 40/500 | Train Loss: 45.6644 | Val Loss: 45.2069 | Val MAE: 45.207 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.207. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 41/500 | Train Loss: 45.2820 | Val Loss: 44.5857 | Val MAE: 44.586 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 44.586. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 42/500 | Train Loss: 45.0476 | Val Loss: 45.5997 | Val MAE: 45.600 | LR: 1.0e-04\n",
      "Epoch 43/500 | Train Loss: 44.6556 | Val Loss: 43.7793 | Val MAE: 43.779 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 43.779. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 44/500 | Train Loss: 44.3266 | Val Loss: 44.0218 | Val MAE: 44.022 | LR: 1.0e-04\n",
      "Epoch 45/500 | Train Loss: 43.9840 | Val Loss: 43.9388 | Val MAE: 43.939 | LR: 1.0e-04\n",
      "Epoch 46/500 | Train Loss: 43.6658 | Val Loss: 43.5514 | Val MAE: 43.551 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 43.551. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 47/500 | Train Loss: 43.3099 | Val Loss: 42.8353 | Val MAE: 42.835 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 42.835. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 48/500 | Train Loss: 42.9161 | Val Loss: 42.6855 | Val MAE: 42.685 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 42.685. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 49/500 | Train Loss: 42.5788 | Val Loss: 42.4102 | Val MAE: 42.410 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 42.410. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 50/500 | Train Loss: 42.1853 | Val Loss: 42.0909 | Val MAE: 42.091 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 42.091. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 51/500 | Train Loss: 41.8885 | Val Loss: 41.9241 | Val MAE: 41.924 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 41.924. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 52/500 | Train Loss: 41.5499 | Val Loss: 41.0129 | Val MAE: 41.013 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 41.013. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 53/500 | Train Loss: 41.1405 | Val Loss: 40.7502 | Val MAE: 40.750 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.750. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 54/500 | Train Loss: 40.6825 | Val Loss: 40.3465 | Val MAE: 40.347 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.347. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 55/500 | Train Loss: 40.3937 | Val Loss: 38.4444 | Val MAE: 38.444 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 38.444. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 56/500 | Train Loss: 40.0257 | Val Loss: 40.1634 | Val MAE: 40.163 | LR: 1.0e-04\n",
      "Epoch 57/500 | Train Loss: 39.6529 | Val Loss: 37.8888 | Val MAE: 37.889 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 37.889. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 58/500 | Train Loss: 39.2143 | Val Loss: 39.1702 | Val MAE: 39.170 | LR: 1.0e-04\n",
      "Epoch 59/500 | Train Loss: 38.7300 | Val Loss: 39.9486 | Val MAE: 39.949 | LR: 1.0e-04\n",
      "Epoch 60/500 | Train Loss: 38.3294 | Val Loss: 38.0711 | Val MAE: 38.071 | LR: 1.0e-04\n",
      "Epoch 61/500 | Train Loss: 37.9568 | Val Loss: 36.0756 | Val MAE: 36.076 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 36.076. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 62/500 | Train Loss: 37.6087 | Val Loss: 39.5625 | Val MAE: 39.562 | LR: 1.0e-04\n",
      "Epoch 63/500 | Train Loss: 37.1288 | Val Loss: 34.8946 | Val MAE: 34.895 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 34.895. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 64/500 | Train Loss: 36.7687 | Val Loss: 32.9969 | Val MAE: 32.997 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 32.997. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 65/500 | Train Loss: 36.1366 | Val Loss: 37.3140 | Val MAE: 37.314 | LR: 1.0e-04\n",
      "Epoch 66/500 | Train Loss: 35.8118 | Val Loss: 40.9153 | Val MAE: 40.915 | LR: 1.0e-04\n",
      "Epoch 67/500 | Train Loss: 35.1781 | Val Loss: 38.0388 | Val MAE: 38.039 | LR: 1.0e-04\n",
      "Epoch 68/500 | Train Loss: 34.8652 | Val Loss: 34.8892 | Val MAE: 34.889 | LR: 1.0e-04\n",
      "Epoch 69/500 | Train Loss: 34.2699 | Val Loss: 27.7068 | Val MAE: 27.707 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 27.707. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 70/500 | Train Loss: 33.9926 | Val Loss: 34.3160 | Val MAE: 34.316 | LR: 1.0e-04\n",
      "Epoch 71/500 | Train Loss: 33.5061 | Val Loss: 31.0958 | Val MAE: 31.096 | LR: 1.0e-04\n",
      "Epoch 72/500 | Train Loss: 32.9172 | Val Loss: 31.5810 | Val MAE: 31.581 | LR: 1.0e-04\n",
      "Epoch 73/500 | Train Loss: 32.5290 | Val Loss: 34.2185 | Val MAE: 34.218 | LR: 1.0e-04\n",
      "Epoch 74/500 | Train Loss: 31.8871 | Val Loss: 30.5318 | Val MAE: 30.532 | LR: 1.0e-04\n",
      "Epoch 75/500 | Train Loss: 31.6959 | Val Loss: 31.4453 | Val MAE: 31.445 | LR: 1.0e-04\n",
      "Epoch 76/500 | Train Loss: 31.0685 | Val Loss: 31.9328 | Val MAE: 31.933 | LR: 1.0e-04\n",
      "Epoch 77/500 | Train Loss: 30.7358 | Val Loss: 33.6157 | Val MAE: 33.616 | LR: 1.0e-04\n",
      "Epoch 78/500 | Train Loss: 30.1630 | Val Loss: 26.7555 | Val MAE: 26.756 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 26.756. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 79/500 | Train Loss: 29.6725 | Val Loss: 25.6344 | Val MAE: 25.634 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 25.634. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 80/500 | Train Loss: 29.3080 | Val Loss: 27.6887 | Val MAE: 27.689 | LR: 1.0e-04\n",
      "Epoch 81/500 | Train Loss: 28.7511 | Val Loss: 27.2258 | Val MAE: 27.226 | LR: 1.0e-04\n",
      "Epoch 82/500 | Train Loss: 28.3623 | Val Loss: 24.7782 | Val MAE: 24.778 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 24.778. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 83/500 | Train Loss: 27.9074 | Val Loss: 37.1218 | Val MAE: 37.122 | LR: 1.0e-04\n",
      "Epoch 84/500 | Train Loss: 27.5536 | Val Loss: 30.4078 | Val MAE: 30.408 | LR: 1.0e-04\n",
      "Epoch 85/500 | Train Loss: 26.9068 | Val Loss: 22.4342 | Val MAE: 22.434 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 22.434. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 86/500 | Train Loss: 26.2354 | Val Loss: 28.7881 | Val MAE: 28.788 | LR: 1.0e-04\n",
      "Epoch 87/500 | Train Loss: 25.8632 | Val Loss: 24.2352 | Val MAE: 24.235 | LR: 1.0e-04\n",
      "Epoch 88/500 | Train Loss: 25.3776 | Val Loss: 29.4466 | Val MAE: 29.447 | LR: 1.0e-04\n",
      "Epoch 89/500 | Train Loss: 25.0134 | Val Loss: 24.1332 | Val MAE: 24.133 | LR: 1.0e-04\n",
      "Epoch 90/500 | Train Loss: 24.5084 | Val Loss: 24.4518 | Val MAE: 24.452 | LR: 1.0e-04\n",
      "Epoch 91/500 | Train Loss: 24.0139 | Val Loss: 18.5863 | Val MAE: 18.586 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 18.586. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 92/500 | Train Loss: 23.5867 | Val Loss: 34.9708 | Val MAE: 34.971 | LR: 1.0e-04\n",
      "Epoch 93/500 | Train Loss: 23.0572 | Val Loss: 20.9141 | Val MAE: 20.914 | LR: 1.0e-04\n",
      "Epoch 94/500 | Train Loss: 22.4333 | Val Loss: 31.6152 | Val MAE: 31.615 | LR: 1.0e-04\n",
      "Epoch 95/500 | Train Loss: 22.2939 | Val Loss: 24.9208 | Val MAE: 24.921 | LR: 1.0e-04\n",
      "Epoch 96/500 | Train Loss: 21.4702 | Val Loss: 22.1073 | Val MAE: 22.107 | LR: 1.0e-04\n",
      "Epoch 97/500 | Train Loss: 21.1452 | Val Loss: 19.1405 | Val MAE: 19.141 | LR: 1.0e-04\n",
      "Epoch 98/500 | Train Loss: 20.5030 | Val Loss: 29.4303 | Val MAE: 29.430 | LR: 1.0e-04\n",
      "Epoch 99/500 | Train Loss: 20.2449 | Val Loss: 25.5254 | Val MAE: 25.525 | LR: 1.0e-04\n",
      "Epoch 100/500 | Train Loss: 19.9121 | Val Loss: 21.6174 | Val MAE: 21.617 | LR: 1.0e-04\n",
      "Epoch 101/500 | Train Loss: 19.0361 | Val Loss: 14.6056 | Val MAE: 14.606 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 14.606. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 102/500 | Train Loss: 18.7459 | Val Loss: 21.0431 | Val MAE: 21.043 | LR: 1.0e-04\n",
      "Epoch 103/500 | Train Loss: 18.3599 | Val Loss: 14.1486 | Val MAE: 14.149 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 14.149. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 104/500 | Train Loss: 17.9617 | Val Loss: 24.1613 | Val MAE: 24.161 | LR: 1.0e-04\n",
      "Epoch 105/500 | Train Loss: 17.4661 | Val Loss: 20.6837 | Val MAE: 20.684 | LR: 1.0e-04\n",
      "Epoch 106/500 | Train Loss: 17.0889 | Val Loss: 19.4045 | Val MAE: 19.405 | LR: 1.0e-04\n",
      "Epoch 107/500 | Train Loss: 16.4162 | Val Loss: 15.2970 | Val MAE: 15.297 | LR: 1.0e-04\n",
      "Epoch 108/500 | Train Loss: 16.2190 | Val Loss: 20.6713 | Val MAE: 20.671 | LR: 1.0e-04\n",
      "Epoch 109/500 | Train Loss: 15.7559 | Val Loss: 19.1830 | Val MAE: 19.183 | LR: 1.0e-04\n",
      "Epoch 110/500 | Train Loss: 15.5640 | Val Loss: 17.0961 | Val MAE: 17.096 | LR: 1.0e-04\n",
      "Epoch 111/500 | Train Loss: 14.8520 | Val Loss: 13.1973 | Val MAE: 13.197 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 13.197. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 112/500 | Train Loss: 14.4058 | Val Loss: 17.4629 | Val MAE: 17.463 | LR: 1.0e-04\n",
      "Epoch 113/500 | Train Loss: 14.1972 | Val Loss: 12.3808 | Val MAE: 12.381 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 12.381. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 114/500 | Train Loss: 13.9853 | Val Loss: 15.3085 | Val MAE: 15.309 | LR: 1.0e-04\n",
      "Epoch 115/500 | Train Loss: 13.7470 | Val Loss: 18.8221 | Val MAE: 18.822 | LR: 1.0e-04\n",
      "Epoch 116/500 | Train Loss: 13.5216 | Val Loss: 14.8237 | Val MAE: 14.824 | LR: 1.0e-04\n",
      "Epoch 117/500 | Train Loss: 13.1240 | Val Loss: 14.8029 | Val MAE: 14.803 | LR: 1.0e-04\n",
      "Epoch 118/500 | Train Loss: 12.6790 | Val Loss: 17.2112 | Val MAE: 17.211 | LR: 1.0e-04\n",
      "Epoch 119/500 | Train Loss: 12.3436 | Val Loss: 16.4404 | Val MAE: 16.440 | LR: 1.0e-04\n",
      "Epoch 120/500 | Train Loss: 12.3716 | Val Loss: 11.3258 | Val MAE: 11.326 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 11.326. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 121/500 | Train Loss: 11.7499 | Val Loss: 16.1321 | Val MAE: 16.132 | LR: 1.0e-04\n",
      "Epoch 122/500 | Train Loss: 11.6826 | Val Loss: 26.9295 | Val MAE: 26.930 | LR: 1.0e-04\n",
      "Epoch 123/500 | Train Loss: 11.5956 | Val Loss: 11.5231 | Val MAE: 11.523 | LR: 1.0e-04\n",
      "Epoch 124/500 | Train Loss: 10.9338 | Val Loss: 10.7254 | Val MAE: 10.725 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 10.725. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 125/500 | Train Loss: 11.1153 | Val Loss: 9.4468 | Val MAE: 9.447 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 9.447. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 126/500 | Train Loss: 10.8079 | Val Loss: 22.4257 | Val MAE: 22.426 | LR: 1.0e-04\n",
      "Epoch 127/500 | Train Loss: 10.8276 | Val Loss: 13.1530 | Val MAE: 13.153 | LR: 1.0e-04\n",
      "Epoch 128/500 | Train Loss: 10.7579 | Val Loss: 11.6771 | Val MAE: 11.677 | LR: 1.0e-04\n",
      "Epoch 129/500 | Train Loss: 10.4908 | Val Loss: 11.4003 | Val MAE: 11.400 | LR: 1.0e-04\n",
      "Epoch 130/500 | Train Loss: 10.4346 | Val Loss: 13.4841 | Val MAE: 13.484 | LR: 1.0e-04\n",
      "Epoch 131/500 | Train Loss: 10.4375 | Val Loss: 8.6488 | Val MAE: 8.649 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.649. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 132/500 | Train Loss: 9.9639 | Val Loss: 8.9167 | Val MAE: 8.917 | LR: 1.0e-04\n",
      "Epoch 133/500 | Train Loss: 9.9504 | Val Loss: 13.6740 | Val MAE: 13.674 | LR: 1.0e-04\n",
      "Epoch 134/500 | Train Loss: 10.1072 | Val Loss: 9.7279 | Val MAE: 9.728 | LR: 1.0e-04\n",
      "Epoch 135/500 | Train Loss: 10.1794 | Val Loss: 11.1479 | Val MAE: 11.148 | LR: 1.0e-04\n",
      "Epoch 136/500 | Train Loss: 9.6773 | Val Loss: 8.9670 | Val MAE: 8.967 | LR: 1.0e-04\n",
      "Epoch 137/500 | Train Loss: 9.4847 | Val Loss: 7.0729 | Val MAE: 7.073 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.073. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 138/500 | Train Loss: 9.5201 | Val Loss: 12.1589 | Val MAE: 12.159 | LR: 1.0e-04\n",
      "Epoch 139/500 | Train Loss: 9.3695 | Val Loss: 9.5969 | Val MAE: 9.597 | LR: 1.0e-04\n",
      "Epoch 140/500 | Train Loss: 9.3928 | Val Loss: 10.3039 | Val MAE: 10.304 | LR: 1.0e-04\n",
      "Epoch 141/500 | Train Loss: 9.3138 | Val Loss: 9.2607 | Val MAE: 9.261 | LR: 1.0e-04\n",
      "Epoch 142/500 | Train Loss: 9.3091 | Val Loss: 12.9379 | Val MAE: 12.938 | LR: 1.0e-04\n",
      "Epoch 143/500 | Train Loss: 9.3181 | Val Loss: 14.1637 | Val MAE: 14.164 | LR: 1.0e-04\n",
      "Epoch 144/500 | Train Loss: 9.1911 | Val Loss: 9.8349 | Val MAE: 9.835 | LR: 1.0e-04\n",
      "Epoch 145/500 | Train Loss: 9.0787 | Val Loss: 8.1162 | Val MAE: 8.116 | LR: 1.0e-04\n",
      "Epoch 146/500 | Train Loss: 9.0144 | Val Loss: 8.4519 | Val MAE: 8.452 | LR: 1.0e-04\n",
      "Epoch 147/500 | Train Loss: 9.4080 | Val Loss: 6.8234 | Val MAE: 6.823 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 6.823. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 148/500 | Train Loss: 9.0688 | Val Loss: 11.3733 | Val MAE: 11.373 | LR: 1.0e-04\n",
      "Epoch 149/500 | Train Loss: 9.1851 | Val Loss: 12.2001 | Val MAE: 12.200 | LR: 1.0e-04\n",
      "Epoch 150/500 | Train Loss: 9.2790 | Val Loss: 7.8845 | Val MAE: 7.884 | LR: 1.0e-04\n",
      "Epoch 151/500 | Train Loss: 9.0980 | Val Loss: 14.0906 | Val MAE: 14.091 | LR: 1.0e-04\n",
      "Epoch 152/500 | Train Loss: 8.8722 | Val Loss: 9.3992 | Val MAE: 9.399 | LR: 1.0e-04\n",
      "Epoch 153/500 | Train Loss: 9.1895 | Val Loss: 9.1510 | Val MAE: 9.151 | LR: 1.0e-04\n",
      "Epoch 154/500 | Train Loss: 8.9994 | Val Loss: 6.9302 | Val MAE: 6.930 | LR: 1.0e-04\n",
      "Epoch 155/500 | Train Loss: 8.8146 | Val Loss: 7.1850 | Val MAE: 7.185 | LR: 1.0e-04\n",
      "Epoch 156/500 | Train Loss: 8.9896 | Val Loss: 7.7563 | Val MAE: 7.756 | LR: 1.0e-04\n",
      "Epoch 157/500 | Train Loss: 8.8012 | Val Loss: 6.4941 | Val MAE: 6.494 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 6.494. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 158/500 | Train Loss: 8.7686 | Val Loss: 7.6302 | Val MAE: 7.630 | LR: 1.0e-04\n",
      "Epoch 159/500 | Train Loss: 8.8118 | Val Loss: 6.6676 | Val MAE: 6.668 | LR: 1.0e-04\n",
      "Epoch 160/500 | Train Loss: 8.9336 | Val Loss: 7.1964 | Val MAE: 7.196 | LR: 1.0e-04\n",
      "Epoch 161/500 | Train Loss: 9.0537 | Val Loss: 12.6121 | Val MAE: 12.612 | LR: 1.0e-04\n",
      "Epoch 162/500 | Train Loss: 8.8619 | Val Loss: 6.8912 | Val MAE: 6.891 | LR: 1.0e-04\n",
      "Epoch 163/500 | Train Loss: 8.8952 | Val Loss: 6.8243 | Val MAE: 6.824 | LR: 1.0e-04\n",
      "Epoch 164/500 | Train Loss: 8.8981 | Val Loss: 7.6516 | Val MAE: 7.652 | LR: 1.0e-04\n",
      "Epoch 165/500 | Train Loss: 8.8594 | Val Loss: 8.1321 | Val MAE: 8.132 | LR: 1.0e-04\n",
      "Epoch 166/500 | Train Loss: 8.7589 | Val Loss: 7.4608 | Val MAE: 7.461 | LR: 1.0e-04\n",
      "Epoch 167/500 | Train Loss: 8.9315 | Val Loss: 10.0994 | Val MAE: 10.099 | LR: 1.0e-04\n",
      "Epoch 168/500 | Train Loss: 8.7495 | Val Loss: 8.4852 | Val MAE: 8.485 | LR: 1.0e-04\n",
      "Epoch 169/500 | Train Loss: 8.8781 | Val Loss: 9.2338 | Val MAE: 9.234 | LR: 1.0e-04\n",
      "Epoch 170/500 | Train Loss: 8.7344 | Val Loss: 7.4056 | Val MAE: 7.406 | LR: 1.0e-04\n",
      "Epoch 171/500 | Train Loss: 9.0425 | Val Loss: 7.4219 | Val MAE: 7.422 | LR: 1.0e-04\n",
      "Epoch 172/500 | Train Loss: 8.7456 | Val Loss: 7.8627 | Val MAE: 7.863 | LR: 1.0e-04\n",
      "Epoch 173/500 | Train Loss: 8.7056 | Val Loss: 7.1815 | Val MAE: 7.182 | LR: 1.0e-04\n",
      "Epoch 174/500 | Train Loss: 8.4986 | Val Loss: 6.9742 | Val MAE: 6.974 | LR: 1.0e-04\n",
      "Epoch 175/500 | Train Loss: 8.7404 | Val Loss: 8.1810 | Val MAE: 8.181 | LR: 1.0e-04\n",
      "Epoch 176/500 | Train Loss: 9.0912 | Val Loss: 7.2594 | Val MAE: 7.259 | LR: 1.0e-04\n",
      "Epoch 177/500 | Train Loss: 8.6301 | Val Loss: 8.2576 | Val MAE: 8.258 | LR: 1.0e-04\n",
      "Epoch 178/500 | Train Loss: 8.6506 | Val Loss: 6.9586 | Val MAE: 6.959 | LR: 1.0e-04\n",
      "Epoch 179/500 | Train Loss: 8.7594 | Val Loss: 6.4986 | Val MAE: 6.499 | LR: 1.0e-04\n",
      "Epoch 180/500 | Train Loss: 8.7581 | Val Loss: 8.5850 | Val MAE: 8.585 | LR: 1.0e-04\n",
      "Epoch 181/500 | Train Loss: 8.6388 | Val Loss: 6.2841 | Val MAE: 6.284 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 6.284. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 182/500 | Train Loss: 8.7701 | Val Loss: 8.8907 | Val MAE: 8.891 | LR: 1.0e-04\n",
      "Epoch 183/500 | Train Loss: 8.6179 | Val Loss: 9.9269 | Val MAE: 9.927 | LR: 1.0e-04\n",
      "Epoch 184/500 | Train Loss: 8.8111 | Val Loss: 7.7915 | Val MAE: 7.792 | LR: 1.0e-04\n",
      "Epoch 185/500 | Train Loss: 8.3742 | Val Loss: 7.4537 | Val MAE: 7.454 | LR: 1.0e-04\n",
      "Epoch 186/500 | Train Loss: 8.6961 | Val Loss: 6.1916 | Val MAE: 6.192 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 6.192. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 187/500 | Train Loss: 8.7130 | Val Loss: 7.1499 | Val MAE: 7.150 | LR: 1.0e-04\n",
      "Epoch 188/500 | Train Loss: 8.7563 | Val Loss: 6.5088 | Val MAE: 6.509 | LR: 1.0e-04\n",
      "Epoch 189/500 | Train Loss: 8.8614 | Val Loss: 9.4379 | Val MAE: 9.438 | LR: 1.0e-04\n",
      "Epoch 190/500 | Train Loss: 8.6410 | Val Loss: 6.6952 | Val MAE: 6.695 | LR: 1.0e-04\n",
      "Epoch 191/500 | Train Loss: 8.6124 | Val Loss: 8.7589 | Val MAE: 8.759 | LR: 1.0e-04\n",
      "Epoch 192/500 | Train Loss: 8.6895 | Val Loss: 7.8095 | Val MAE: 7.809 | LR: 1.0e-04\n",
      "Epoch 193/500 | Train Loss: 8.4199 | Val Loss: 7.1813 | Val MAE: 7.181 | LR: 1.0e-04\n",
      "Epoch 194/500 | Train Loss: 8.6932 | Val Loss: 9.3571 | Val MAE: 9.357 | LR: 1.0e-04\n",
      "Epoch 195/500 | Train Loss: 8.5302 | Val Loss: 6.8775 | Val MAE: 6.878 | LR: 1.0e-04\n",
      "Epoch 196/500 | Train Loss: 8.7288 | Val Loss: 9.8866 | Val MAE: 9.887 | LR: 1.0e-04\n",
      "Epoch 197/500 | Train Loss: 8.6983 | Val Loss: 6.6915 | Val MAE: 6.692 | LR: 1.0e-04\n",
      "Epoch 198/500 | Train Loss: 8.6458 | Val Loss: 7.5422 | Val MAE: 7.542 | LR: 1.0e-04\n",
      "Epoch 199/500 | Train Loss: 8.6885 | Val Loss: 9.0556 | Val MAE: 9.056 | LR: 1.0e-04\n",
      "Epoch 200/500 | Train Loss: 8.4819 | Val Loss: 7.6298 | Val MAE: 7.630 | LR: 1.0e-04\n",
      "Epoch 201/500 | Train Loss: 8.4381 | Val Loss: 8.2093 | Val MAE: 8.209 | LR: 1.0e-04\n",
      "Epoch 202/500 | Train Loss: 8.3162 | Val Loss: 7.7213 | Val MAE: 7.721 | LR: 1.0e-04\n",
      "Epoch 203/500 | Train Loss: 8.4197 | Val Loss: 8.6921 | Val MAE: 8.692 | LR: 1.0e-04\n",
      "Epoch 204/500 | Train Loss: 8.3796 | Val Loss: 7.7117 | Val MAE: 7.712 | LR: 1.0e-04\n",
      "Epoch 205/500 | Train Loss: 8.7178 | Val Loss: 6.6322 | Val MAE: 6.632 | LR: 1.0e-04\n",
      "Epoch 206/500 | Train Loss: 8.4739 | Val Loss: 7.0122 | Val MAE: 7.012 | LR: 1.0e-04\n",
      "Epoch 207/500 | Train Loss: 8.4281 | Val Loss: 6.9048 | Val MAE: 6.905 | LR: 1.0e-04\n",
      "Epoch 208/500 | Train Loss: 8.4597 | Val Loss: 7.4277 | Val MAE: 7.428 | LR: 1.0e-04\n",
      "Epoch 209/500 | Train Loss: 8.4796 | Val Loss: 6.9549 | Val MAE: 6.955 | LR: 1.0e-04\n",
      "Epoch 210/500 | Train Loss: 8.4804 | Val Loss: 6.6943 | Val MAE: 6.694 | LR: 1.0e-04\n",
      "Epoch 211/500 | Train Loss: 8.4777 | Val Loss: 10.1419 | Val MAE: 10.142 | LR: 1.0e-04\n",
      "Epoch 212/500 | Train Loss: 8.3680 | Val Loss: 6.7004 | Val MAE: 6.700 | LR: 1.0e-04\n",
      "Epoch 213/500 | Train Loss: 8.5168 | Val Loss: 15.0551 | Val MAE: 15.055 | LR: 1.0e-04\n",
      "Epoch 214/500 | Train Loss: 8.3346 | Val Loss: 7.1217 | Val MAE: 7.122 | LR: 1.0e-04\n",
      "Epoch 215/500 | Train Loss: 8.4630 | Val Loss: 7.2487 | Val MAE: 7.249 | LR: 1.0e-04\n",
      "Epoch 216/500 | Train Loss: 8.2399 | Val Loss: 6.4723 | Val MAE: 6.472 | LR: 1.0e-04\n",
      "Epoch 217/500 | Train Loss: 8.4274 | Val Loss: 6.9466 | Val MAE: 6.947 | LR: 1.0e-04\n",
      "Epoch 218/500 | Train Loss: 8.3118 | Val Loss: 6.8302 | Val MAE: 6.830 | LR: 1.0e-04\n",
      "Epoch 219/500 | Train Loss: 8.6230 | Val Loss: 6.2942 | Val MAE: 6.294 | LR: 1.0e-04\n",
      "Epoch 220/500 | Train Loss: 8.6591 | Val Loss: 6.6432 | Val MAE: 6.643 | LR: 1.0e-04\n",
      "Epoch 221/500 | Train Loss: 8.6954 | Val Loss: 7.9200 | Val MAE: 7.920 | LR: 1.0e-04\n",
      "Epoch 222/500 | Train Loss: 8.5320 | Val Loss: 6.2394 | Val MAE: 6.239 | LR: 1.0e-04\n",
      "Epoch 223/500 | Train Loss: 8.4517 | Val Loss: 6.4544 | Val MAE: 6.454 | LR: 1.0e-04\n",
      "Epoch 224/500 | Train Loss: 8.2822 | Val Loss: 6.4270 | Val MAE: 6.427 | LR: 1.0e-04\n",
      "Epoch 225/500 | Train Loss: 8.1265 | Val Loss: 7.0305 | Val MAE: 7.030 | LR: 1.0e-04\n",
      "Epoch 226/500 | Train Loss: 8.5077 | Val Loss: 6.7153 | Val MAE: 6.715 | LR: 1.0e-04\n",
      "Epoch 227/500 | Train Loss: 8.6330 | Val Loss: 6.7604 | Val MAE: 6.760 | LR: 1.0e-04\n",
      "Epoch 228/500 | Train Loss: 8.3662 | Val Loss: 9.6014 | Val MAE: 9.601 | LR: 1.0e-04\n",
      "Epoch 229/500 | Train Loss: 8.5260 | Val Loss: 7.5205 | Val MAE: 7.520 | LR: 1.0e-04\n",
      "Epoch 230/500 | Train Loss: 8.3108 | Val Loss: 6.6932 | Val MAE: 6.693 | LR: 1.0e-04\n",
      "Epoch 231/500 | Train Loss: 8.5072 | Val Loss: 8.2195 | Val MAE: 8.219 | LR: 1.0e-04\n",
      "Epoch 232/500 | Train Loss: 8.7273 | Val Loss: 6.2156 | Val MAE: 6.216 | LR: 1.0e-04\n",
      "Epoch 233/500 | Train Loss: 8.5198 | Val Loss: 6.8835 | Val MAE: 6.883 | LR: 1.0e-04\n",
      "Epoch 234/500 | Train Loss: 8.4508 | Val Loss: 5.9524 | Val MAE: 5.952 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 5.952. Saved model to combined_slice_model/best_combined_model.pth\n",
      "Epoch 235/500 | Train Loss: 8.3488 | Val Loss: 6.0992 | Val MAE: 6.099 | LR: 1.0e-04\n",
      "Epoch 236/500 | Train Loss: 8.5392 | Val Loss: 7.4808 | Val MAE: 7.481 | LR: 1.0e-04\n",
      "Epoch 237/500 | Train Loss: 8.6017 | Val Loss: 8.2977 | Val MAE: 8.298 | LR: 1.0e-04\n",
      "Epoch 238/500 | Train Loss: 8.5082 | Val Loss: 7.7118 | Val MAE: 7.712 | LR: 1.0e-04\n",
      "Epoch 239/500 | Train Loss: 8.2398 | Val Loss: 6.3709 | Val MAE: 6.371 | LR: 1.0e-04\n",
      "Epoch 240/500 | Train Loss: 8.1853 | Val Loss: 6.2744 | Val MAE: 6.274 | LR: 1.0e-04\n",
      "Epoch 241/500 | Train Loss: 8.3175 | Val Loss: 6.5870 | Val MAE: 6.587 | LR: 1.0e-04\n",
      "Epoch 242/500 | Train Loss: 8.1757 | Val Loss: 6.4585 | Val MAE: 6.458 | LR: 1.0e-04\n",
      "Epoch 243/500 | Train Loss: 8.4263 | Val Loss: 7.2651 | Val MAE: 7.265 | LR: 1.0e-04\n",
      "Epoch 244/500 | Train Loss: 8.3888 | Val Loss: 6.5944 | Val MAE: 6.594 | LR: 1.0e-04\n",
      "Epoch 245/500 | Train Loss: 8.4228 | Val Loss: 7.1277 | Val MAE: 7.128 | LR: 1.0e-04\n",
      "Epoch 246/500 | Train Loss: 8.3532 | Val Loss: 6.5750 | Val MAE: 6.575 | LR: 1.0e-04\n",
      "Epoch 247/500 | Train Loss: 8.5176 | Val Loss: 14.1350 | Val MAE: 14.135 | LR: 1.0e-04\n",
      "Epoch 248/500 | Train Loss: 8.0798 | Val Loss: 6.8333 | Val MAE: 6.833 | LR: 1.0e-04\n",
      "Epoch 249/500 | Train Loss: 8.4221 | Val Loss: 7.3961 | Val MAE: 7.396 | LR: 1.0e-04\n",
      "Epoch 250/500 | Train Loss: 8.2406 | Val Loss: 6.8602 | Val MAE: 6.860 | LR: 1.0e-04\n",
      "Epoch 251/500 | Train Loss: 8.2737 | Val Loss: 9.4295 | Val MAE: 9.429 | LR: 1.0e-04\n",
      "Epoch 252/500 | Train Loss: 8.1543 | Val Loss: 6.2432 | Val MAE: 6.243 | LR: 1.0e-04\n",
      "Epoch 253/500 | Train Loss: 7.9717 | Val Loss: 7.1147 | Val MAE: 7.115 | LR: 1.0e-04\n",
      "Epoch 254/500 | Train Loss: 8.2363 | Val Loss: 6.4858 | Val MAE: 6.486 | LR: 1.0e-04\n",
      "Epoch 255/500 | Train Loss: 8.0805 | Val Loss: 6.1738 | Val MAE: 6.174 | LR: 1.0e-04\n",
      "Epoch 256/500 | Train Loss: 8.2793 | Val Loss: 7.8737 | Val MAE: 7.874 | LR: 1.0e-04\n",
      "Epoch 257/500 | Train Loss: 8.0984 | Val Loss: 7.4115 | Val MAE: 7.412 | LR: 1.0e-04\n",
      "Epoch 258/500 | Train Loss: 8.2225 | Val Loss: 10.8917 | Val MAE: 10.892 | LR: 1.0e-04\n",
      "Epoch 259/500 | Train Loss: 8.0994 | Val Loss: 12.1051 | Val MAE: 12.105 | LR: 1.0e-04\n",
      "Epoch 260/500 | Train Loss: 8.5498 | Val Loss: 7.0518 | Val MAE: 7.052 | LR: 1.0e-04\n",
      "Epoch 261/500 | Train Loss: 8.0874 | Val Loss: 7.6071 | Val MAE: 7.607 | LR: 1.0e-04\n",
      "Epoch 262/500 | Train Loss: 8.1105 | Val Loss: 6.2258 | Val MAE: 6.226 | LR: 1.0e-04\n",
      "Epoch 263/500 | Train Loss: 8.2623 | Val Loss: 7.1167 | Val MAE: 7.117 | LR: 1.0e-04\n",
      "Epoch 264/500 | Train Loss: 8.2229 | Val Loss: 6.2121 | Val MAE: 6.212 | LR: 1.0e-04\n",
      "Epoch 265/500 | Train Loss: 8.2100 | Val Loss: 9.9146 | Val MAE: 9.915 | LR: 1.0e-04\n",
      "Epoch 266/500 | Train Loss: 8.6025 | Val Loss: 7.9522 | Val MAE: 7.952 | LR: 1.0e-04\n",
      "Epoch 267/500 | Train Loss: 8.2686 | Val Loss: 6.5041 | Val MAE: 6.504 | LR: 1.0e-04\n",
      "Epoch 268/500 | Train Loss: 8.2920 | Val Loss: 6.7456 | Val MAE: 6.746 | LR: 1.0e-04\n",
      "Epoch 269/500 | Train Loss: 8.4501 | Val Loss: 6.7197 | Val MAE: 6.720 | LR: 1.0e-04\n",
      "Epoch 270/500 | Train Loss: 8.3498 | Val Loss: 6.7691 | Val MAE: 6.769 | LR: 1.0e-04\n",
      "Epoch 271/500 | Train Loss: 8.5447 | Val Loss: 6.5023 | Val MAE: 6.502 | LR: 1.0e-04\n",
      "Epoch 272/500 | Train Loss: 8.1186 | Val Loss: 6.4340 | Val MAE: 6.434 | LR: 1.0e-04\n",
      "Epoch 273/500 | Train Loss: 8.4917 | Val Loss: 6.1567 | Val MAE: 6.157 | LR: 1.0e-04\n",
      "Epoch 274/500 | Train Loss: 8.1726 | Val Loss: 6.2358 | Val MAE: 6.236 | LR: 1.0e-04\n",
      "Epoch 275/500 | Train Loss: 8.0737 | Val Loss: 6.8117 | Val MAE: 6.812 | LR: 1.0e-04\n",
      "Epoch 276/500 | Train Loss: 8.2933 | Val Loss: 7.4266 | Val MAE: 7.427 | LR: 1.0e-04\n",
      "Epoch 277/500 | Train Loss: 8.5711 | Val Loss: 8.2801 | Val MAE: 8.280 | LR: 1.0e-04\n",
      "Epoch 278/500 | Train Loss: 8.3308 | Val Loss: 6.9334 | Val MAE: 6.933 | LR: 1.0e-04\n",
      "Epoch 279/500 | Train Loss: 8.3473 | Val Loss: 6.4616 | Val MAE: 6.462 | LR: 1.0e-04\n",
      "Epoch 280/500 | Train Loss: 8.1688 | Val Loss: 8.7927 | Val MAE: 8.793 | LR: 1.0e-04\n",
      "Epoch 281/500 | Train Loss: 8.2330 | Val Loss: 7.7766 | Val MAE: 7.777 | LR: 1.0e-04\n",
      "Epoch 282/500 | Train Loss: 8.1121 | Val Loss: 7.4482 | Val MAE: 7.448 | LR: 1.0e-04\n",
      "Epoch 283/500 | Train Loss: 7.7618 | Val Loss: 6.1899 | Val MAE: 6.190 | LR: 1.0e-04\n",
      "Epoch 284/500 | Train Loss: 8.1099 | Val Loss: 6.8175 | Val MAE: 6.818 | LR: 1.0e-04\n",
      "\n",
      "Early stopping triggered after 50 epochs without improvement.\n",
      "\n",
      "--- Combined Model Training Finished ---\n",
      "Best Validation MAE: 5.952 achieved at epoch 234\n",
      "Best model saved to: combined_slice_model/best_combined_model.pth\n",
      "\n",
      "Cleaning up resources...\n",
      "\n",
      "Combined slice model training finished.\n"
     ]
    }
   ],
   "source": [
    "# In the third code cell\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F # For Global Average Pooling\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict # Keep for potential future use if needed\n",
    "\n",
    "# --- Configuration (Mostly unchanged, ensure these are defined) ---\n",
    "FEATURE_ROOT = Path(\"/data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice\")\n",
    "CSV_PATH = Path(\"/data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\")\n",
    "MODELS_OUTPUT_DIR = Path(\"./combined_slice_model\") # Updated output directory\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "SPLITS = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "TARGET_SLICES = [\n",
    "    {'orientation': 'sagittal', 'index': 80},\n",
    "    {'orientation': 'sagittal', 'index': 125},\n",
    "    {'orientation': 'coronal',  'index': 125},\n",
    "    {'orientation': 'axial',    'index': 80},\n",
    "]\n",
    "CONCAT_DIM = 256 * len(TARGET_SLICES) # 4 * 256 = 1024\n",
    "\n",
    "# --- Hyperparameters (Mostly unchanged) ---\n",
    "EPOCHS = 500\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32 # Adjust based on GPU memory for the larger input\n",
    "WEIGHT_DECAY = 1e-5\n",
    "SCHEDULER_PATIENCE_PERCENT = 0.10\n",
    "EARLY_STOPPING_PATIENCE = 50\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# --- Model Definition (Adjusted Input Dimension) ---\n",
    "class AgeMLPWithAttentionBN(nn.Module):\n",
    "    # Adjusted input_dim and embed_dim defaults\n",
    "    def __init__(self, input_dim=1024, embed_dim=1024, num_heads=16, # Adjusted num_heads for divisibility\n",
    "                 hidden_dim1=128, hidden_dim2=64, hidden_dim3=32, dropout_rate=0.3):\n",
    "        super(AgeMLPWithAttentionBN, self).__init__()\n",
    "        if embed_dim % num_heads != 0:\n",
    "             # Adjust num_heads if necessary, e.g., find a divisor of input_dim\n",
    "             # For input_dim=1024, potential heads: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024\n",
    "             # Let's default to 16, but raise error if user provides incompatible combination\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\")\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=False)\n",
    "        self.norm_attn = nn.LayerNorm(embed_dim)\n",
    "        # MLP input layer now takes the concatenated dimension\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1), nn.BatchNorm1d(hidden_dim1), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2), nn.BatchNorm1d(hidden_dim2), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3), nn.BatchNorm1d(hidden_dim3), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, input_dim) which is now 1024\n",
    "        x_attn = x.unsqueeze(0) # (1, batch_size, input_dim)\n",
    "        attn_output, _ = self.attention(x_attn, x_attn, x_attn)\n",
    "        x = self.norm_attn(x_attn + attn_output) # Add residual\n",
    "        x = x.squeeze(0) # (batch_size, input_dim)\n",
    "        output = self.mlp(x)\n",
    "        return output.squeeze(-1) # (batch_size,)\n",
    "\n",
    "# --- Dataset Definition (Modified for Concatenating Slices) ---\n",
    "class CombinedSliceDataset(Dataset):\n",
    "    def __init__(self, feature_root, csv_path, split, target_slices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_root (Path): Base directory containing split folders.\n",
    "            csv_path (Path): Path to the CSV file with metadata.\n",
    "            split (str): The dataset split ('train', 'validation', or 'test').\n",
    "            target_slices (list): List of dicts, each specifying {'orientation': str, 'index': int}.\n",
    "        \"\"\"\n",
    "        self.feature_root = Path(feature_root)\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.split = split\n",
    "        self.target_slices = target_slices\n",
    "        self.split_dir = self.feature_root / self.split\n",
    "\n",
    "        print(f\"\\n[Dataset Init - Combined] Split: {self.split}\")\n",
    "        print(f\"Target Slices: {self.target_slices}\")\n",
    "        print(f\"Scanning subjects in: {self.split_dir}\")\n",
    "        print(f\"Loading metadata from: {self.csv_path}\")\n",
    "\n",
    "        # Basic checks\n",
    "        if not self.feature_root.is_dir(): raise FileNotFoundError(f\"Feature root not found: {self.feature_root}\")\n",
    "        if not self.split_dir.is_dir(): raise FileNotFoundError(f\"Split directory not found: {self.split_dir}\")\n",
    "        if not self.csv_path.is_file(): raise FileNotFoundError(f\"CSV file not found: {self.csv_path}\")\n",
    "\n",
    "        # Load CSV and build mapping (same logic as before)\n",
    "        try:\n",
    "            df = pd.read_csv(self.csv_path)\n",
    "            self.meta_dict = df.set_index('filename')['age'].to_dict()\n",
    "            self.subject_dir_to_filename = {}\n",
    "            nii_gz_keys = {k.replace(\".nii.gz\", \"\") for k in self.meta_dict.keys()}\n",
    "            nii_keys = {k.replace(\".nii\", \"\") for k in self.meta_dict.keys()}\n",
    "            potential_subject_dirs = [d.name for d in self.split_dir.iterdir() if d.is_dir()]\n",
    "            mapped_count = 0\n",
    "            for subj_dir_name in potential_subject_dirs:\n",
    "                matched_key = None\n",
    "                # Using the same heuristic, adjust if needed based on actual directory names\n",
    "                base_subj_name = subj_dir_name.split('_mri_brainmask')[0]\n",
    "                for key_base in nii_gz_keys:\n",
    "                    if key_base.startswith(base_subj_name):\n",
    "                         matched_key = key_base + \".nii.gz\"\n",
    "                         break\n",
    "                if not matched_key:\n",
    "                     for key_base in nii_keys:\n",
    "                          if key_base.startswith(base_subj_name):\n",
    "                               matched_key = key_base + \".nii\"\n",
    "                               break\n",
    "                if matched_key and matched_key in self.meta_dict:\n",
    "                    self.subject_dir_to_filename[subj_dir_name] = matched_key\n",
    "                    mapped_count += 1\n",
    "            print(f\"Loaded metadata for {len(self.meta_dict)} subjects from CSV.\")\n",
    "            print(f\"Successfully mapped {mapped_count} subject directories in '{self.split}' to CSV entries.\")\n",
    "            if mapped_count < len(potential_subject_dirs):\n",
    "                 print(f\"Warning: Could not map {len(potential_subject_dirs) - mapped_count} directories to CSV.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading/processing CSV or mapping directories: {e}\")\n",
    "\n",
    "        # Find subject directories that have ALL required slice files\n",
    "        self.valid_subject_dirs = []\n",
    "        subjects_missing_slices = 0\n",
    "        subjects_without_meta = 0\n",
    "\n",
    "        print(f\"Scanning {len(potential_subject_dirs)} potential subject directories for all target slices...\")\n",
    "        all_subject_dirs_in_split = [d for d in self.split_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "        for subject_dir in all_subject_dirs_in_split:\n",
    "            subject_dir_name = subject_dir.name\n",
    "            # Check if metadata exists for this subject directory\n",
    "            if subject_dir_name in self.subject_dir_to_filename:\n",
    "                all_slices_found = True\n",
    "                for slice_info in self.target_slices:\n",
    "                    slice_filename = f\"slice_{slice_info['orientation']}_{slice_info['index']}.npy\"\n",
    "                    expected_slice_path = subject_dir / slice_filename\n",
    "                    if not expected_slice_path.is_file():\n",
    "                        all_slices_found = False\n",
    "                        break # No need to check further slices for this subject\n",
    "                if all_slices_found:\n",
    "                    self.valid_subject_dirs.append(subject_dir)\n",
    "                else:\n",
    "                    subjects_missing_slices += 1\n",
    "            else:\n",
    "                subjects_without_meta += 1 # Count dirs we couldn't map to CSV\n",
    "\n",
    "        if subjects_without_meta > 0:\n",
    "             print(f\"Info: {subjects_without_meta} subject directories were skipped (no metadata mapping).\")\n",
    "        if subjects_missing_slices > 0:\n",
    "            print(f\"Warning: {subjects_missing_slices} subjects with metadata were missing at least one target slice.\")\n",
    "\n",
    "        if not self.valid_subject_dirs:\n",
    "             raise RuntimeError(f\"No subjects found with all required slices in {self.split_dir} with matching metadata.\")\n",
    "\n",
    "        print(f\"Found {len(self.valid_subject_dirs)} valid subjects with all required slices in split {self.split}.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_subject_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_dir = self.valid_subject_dirs[idx]\n",
    "        subject_dir_name = subject_dir.name\n",
    "        original_filename = self.subject_dir_to_filename.get(subject_dir_name)\n",
    "        if not original_filename:\n",
    "             raise ValueError(f\"Internal Error: Could not find original filename for valid subject dir {subject_dir_name}\")\n",
    "\n",
    "        pooled_embeddings = []\n",
    "        try:\n",
    "            for slice_info in self.target_slices:\n",
    "                slice_filename = f\"slice_{slice_info['orientation']}_{slice_info['index']}.npy\"\n",
    "                slice_path = subject_dir / slice_filename\n",
    "                embedding = np.load(slice_path)\n",
    "                embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "\n",
    "                # Apply Global Average Pooling (GAP) - same logic as before\n",
    "                if len(embedding_tensor.shape) == 4 and embedding_tensor.shape[0] == 1:\n",
    "                     pooled = F.adaptive_avg_pool2d(embedding_tensor, (1, 1)).squeeze() # -> (C,) e.g., (256,)\n",
    "                elif len(embedding_tensor.shape) == 3: # Assume (C, H, W)\n",
    "                     pooled = F.adaptive_avg_pool2d(embedding_tensor.unsqueeze(0), (1, 1)).squeeze() # Add batch, pool, squeeze -> (C,)\n",
    "                elif len(embedding_tensor.shape) == 1: # Assume already pooled (C,)\n",
    "                     pooled = embedding_tensor\n",
    "                else:\n",
    "                     raise ValueError(f\"Unexpected embedding shape {embedding_tensor.shape} for {slice_path}\")\n",
    "\n",
    "                if pooled.shape[0] != 256: # Ensure channel dim is correct after pooling\n",
    "                     raise ValueError(f\"Pooled embedding channel dim is not 256 for {slice_path}: {pooled.shape}\")\n",
    "\n",
    "                pooled_embeddings.append(pooled)\n",
    "\n",
    "            # Concatenate the pooled embeddings\n",
    "            concatenated_embedding = torch.cat(pooled_embeddings, dim=0) # Shape: (1024,)\n",
    "\n",
    "            # Get age\n",
    "            age = self.meta_dict[original_filename]\n",
    "            age_tensor = torch.tensor(age, dtype=torch.float32)\n",
    "\n",
    "            return concatenated_embedding, age_tensor\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading/processing slices for subject {subject_dir_name}: {e}\")\n",
    "            raise e # Re-raise\n",
    "\n",
    "# --- Training and Evaluation Functions (Unchanged from previous notebook cell) ---\n",
    "# Assume train_one_epoch and evaluate functions are defined here as they were before\n",
    "# (without the tqdm wrappers if running in a notebook context where they were removed)\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "    for features, ages in loader:\n",
    "        features, ages = features.to(device), ages.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, ages)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss * features.size(0)\n",
    "        num_samples += features.size(0)\n",
    "    return total_loss / num_samples\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    num_samples = 0\n",
    "    for features, ages in loader:\n",
    "        features, ages = features.to(device), ages.to(device)\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, ages)\n",
    "        total_loss += loss.item() * features.size(0)\n",
    "        mae = F.l1_loss(predictions, ages, reduction='sum')\n",
    "        total_mae += mae.item()\n",
    "        num_samples += features.size(0)\n",
    "    avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "    avg_mae = total_mae / num_samples if num_samples > 0 else float('inf')\n",
    "    return avg_loss, avg_mae\n",
    "\n",
    "\n",
    "# --- Main Training Loop (Single Model) ---\n",
    "if __name__ == \"__main__\": # Or run directly in notebook cell\n",
    "    print(f\"Starting combined MLP training using concatenated slices...\")\n",
    "    print(f\"Target Slices for Concatenation: {TARGET_SLICES}\")\n",
    "    print(f\"Concatenated Input Dimension: {CONCAT_DIM}\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"Feature Root: {FEATURE_ROOT}\")\n",
    "    print(f\"CSV Path: {CSV_PATH}\")\n",
    "    print(f\"Model will be saved to: {MODELS_OUTPUT_DIR}\")\n",
    "    print(f\"Hyperparameters: Epochs={EPOCHS}, LR={LEARNING_RATE}, Batch={BATCH_SIZE}, ES_Patience={EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "    MODELS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    model_save_path = MODELS_OUTPUT_DIR / \"best_combined_model.pth\"\n",
    "\n",
    "    try:\n",
    "        print(\"\\nSetting up combined datasets...\")\n",
    "        train_dataset = CombinedSliceDataset(FEATURE_ROOT, CSV_PATH, 'train', TARGET_SLICES)\n",
    "        val_dataset = CombinedSliceDataset(FEATURE_ROOT, CSV_PATH, 'validation', TARGET_SLICES)\n",
    "        # Optional: test_dataset = CombinedSliceDataset(FEATURE_ROOT, CSV_PATH, 'test', TARGET_SLICES)\n",
    "\n",
    "        print(\"Setting up dataloaders...\")\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        # Optional: test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    except (FileNotFoundError, ValueError, RuntimeError) as e:\n",
    "        print(f\"\\nError initializing combined datasets/loaders: {e}\")\n",
    "        print(\"Stopping training.\")\n",
    "        # Exit or handle error appropriately\n",
    "        exit() # Or raise e\n",
    "\n",
    "    print(\"\\nInitializing model, optimizer, scheduler...\")\n",
    "    # Instantiate model with the correct concatenated dimensions\n",
    "    model = AgeMLPWithAttentionBN(input_dim=CONCAT_DIM, embed_dim=CONCAT_DIM, num_heads=16).to(DEVICE) # Ensure num_heads is compatible\n",
    "    criterion = nn.L1Loss() # MAE Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler_patience_epochs = max(1, int(EPOCHS * SCHEDULER_PATIENCE_PERCENT))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=scheduler_patience_epochs, factor=0.5, verbose=False)\n",
    "\n",
    "    best_val_mae = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_epoch = -1\n",
    "\n",
    "    print(f\"\\n--- Starting Combined Model Training ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        val_loss, val_mae = evaluate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val MAE: {val_mae:.3f} | LR: {current_lr:.1e}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_mae < best_val_mae:\n",
    "            best_val_mae = val_mae\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"  -> New best Val MAE: {best_val_mae:.3f}. Saved model to {model_save_path}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered after {EARLY_STOPPING_PATIENCE} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "             gc.collect()\n",
    "             if DEVICE.startswith('cuda'):\n",
    "                 torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\n--- Combined Model Training Finished ---\")\n",
    "    if best_epoch != -1:\n",
    "         print(f\"Best Validation MAE: {best_val_mae:.3f} achieved at epoch {best_epoch}\")\n",
    "         print(f\"Best model saved to: {model_save_path}\")\n",
    "         # Store results if needed for comparison later\n",
    "         final_results = {'best_val_mae': best_val_mae, 'best_epoch': best_epoch}\n",
    "    else:\n",
    "         print(\"No improvement found during training.\")\n",
    "         final_results = {'best_val_mae': float('inf'), 'best_epoch': -1, 'error': 'No improvement'}\n",
    "\n",
    "\n",
    "    # --- Optional: Evaluate on Test Set ---\n",
    "    # if 'test_loader' in locals() and model_save_path.exists():\n",
    "    #     print(f\"\\n--- Evaluating Combined Model on Test Set ---\")\n",
    "    #     try:\n",
    "    #         # Load the best model state\n",
    "    #         model.load_state_dict(torch.load(model_save_path))\n",
    "    #         test_loss, test_mae = evaluate(model, test_loader, criterion, DEVICE)\n",
    "    #         print(f\"Combined Model Test Loss: {test_loss:.4f} | Test MAE: {test_mae:.3f}\")\n",
    "    #         final_results['test_mae'] = test_mae\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error during test set evaluation: {e}\")\n",
    "    # else:\n",
    "    #     print(\"\\nSkipping test set evaluation.\")\n",
    "\n",
    "    # Clean up\n",
    "    print(\"\\nCleaning up resources...\")\n",
    "    del model, optimizer, scheduler, train_dataset, val_dataset, train_loader, val_loader\n",
    "    # if 'test_dataset' in locals(): del test_dataset\n",
    "    # if 'test_loader' in locals(): del test_loader\n",
    "    gc.collect()\n",
    "    if DEVICE.startswith('cuda'):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\nCombined slice model training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f071f",
   "metadata": {},
   "source": [
    "Evaluating Early Fusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d70a4431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Combined Model on Test Set ---\n",
      "Loading model from: combined_slice_model/best_combined_model.pth\n",
      "Using test data from: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice (split: test)\n",
      "CSV Path: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Device: cuda:1\n",
      "Setting up test dataset...\n",
      "\n",
      "[Dataset Init - Combined] Split: test\n",
      "Target Slices: [{'orientation': 'sagittal', 'index': 80}, {'orientation': 'sagittal', 'index': 125}, {'orientation': 'coronal', 'index': 125}, {'orientation': 'axial', 'index': 80}]\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/test\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 296 subject directories in 'test' to CSV entries.\n",
      "Scanning 296 potential subject directories for all target slices...\n",
      "Found 296 valid subjects with all required slices in split test.\n",
      "Setting up test dataloader...\n",
      "Initializing model...\n",
      "Loading model state dict from combined_slice_model/best_combined_model.pth...\n",
      "Model loaded successfully.\n",
      "Starting evaluation on the test set...\n",
      "\n",
      "--- Combined Model Test Set Results ---\n",
      "Test Loss (MAE): 6.1839\n",
      "Test MAE: 6.184\n",
      "Test results added to 'final_results' dictionary.\n",
      "\n",
      "Cleaning up test evaluation resources...\n",
      "Test evaluation finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Ensure necessary classes and functions from the previous cell are available\n",
    "# (AgeMLPWithAttentionBN, CombinedSliceDataset, evaluate)\n",
    "\n",
    "# --- Configuration (Ensure these are consistent with the training cell) ---\n",
    "# MODEL_SAVE_PATH is derived from MODELS_OUTPUT_DIR used in the previous cell\n",
    "MODEL_SAVE_PATH = MODELS_OUTPUT_DIR / \"best_combined_model.pth\"\n",
    "TEST_SPLIT = 'test' # Explicitly define the split for clarity\n",
    "\n",
    "# --- Test Set Evaluation ---\n",
    "print(f\"\\n--- Evaluating Combined Model on Test Set ---\")\n",
    "print(f\"Loading model from: {MODEL_SAVE_PATH}\")\n",
    "print(f\"Using test data from: {FEATURE_ROOT} (split: {TEST_SPLIT})\")\n",
    "print(f\"CSV Path: {CSV_PATH}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "test_dataset = None\n",
    "test_loader = None\n",
    "model = None\n",
    "\n",
    "try:\n",
    "    # 1. Create Test Dataset and DataLoader\n",
    "    print(\"Setting up test dataset...\")\n",
    "    test_dataset = CombinedSliceDataset(FEATURE_ROOT, CSV_PATH, TEST_SPLIT, TARGET_SLICES)\n",
    "    print(\"Setting up test dataloader...\")\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    # 2. Initialize Model\n",
    "    print(\"Initializing model...\")\n",
    "    # Ensure input_dim and embed_dim match the trained model's configuration\n",
    "    # Assuming num_heads=16 was used during training as per the previous cell's default\n",
    "    model = AgeMLPWithAttentionBN(input_dim=CONCAT_DIM, embed_dim=CONCAT_DIM, num_heads=16).to(DEVICE)\n",
    "\n",
    "    # 3. Load Trained Weights\n",
    "    if MODEL_SAVE_PATH.is_file():\n",
    "        print(f\"Loading model state dict from {MODEL_SAVE_PATH}...\")\n",
    "        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        print(\"Model loaded successfully.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model file not found at {MODEL_SAVE_PATH}. Cannot evaluate.\")\n",
    "\n",
    "    # 4. Define Criterion\n",
    "    criterion = nn.L1Loss() # Use the same loss function (MAE) for evaluation\n",
    "\n",
    "    # 5. Evaluate\n",
    "    print(\"Starting evaluation on the test set...\")\n",
    "    test_loss, test_mae = evaluate(model, test_loader, criterion, DEVICE) # Use the existing evaluate function\n",
    "\n",
    "    # 6. Print Results\n",
    "    print(f\"\\n--- Combined Model Test Set Results ---\")\n",
    "    print(f\"Test Loss (MAE): {test_loss:.4f}\")\n",
    "    print(f\"Test MAE: {test_mae:.3f}\")\n",
    "\n",
    "    # Optionally add results to the final_results dict if it exists and you want to store them\n",
    "    if 'final_results' in locals() and isinstance(final_results, dict):\n",
    "        final_results['test_mae'] = test_mae\n",
    "        final_results['test_loss'] = test_loss\n",
    "        print(\"Test results added to 'final_results' dictionary.\")\n",
    "\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"Skipping test set evaluation.\")\n",
    "except (ValueError, RuntimeError) as e:\n",
    "    print(f\"\\nError during dataset/dataloader creation or evaluation: {e}\")\n",
    "    print(\"Skipping test set evaluation.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during test set evaluation: {e}\")\n",
    "    print(\"Skipping test set evaluation.\")\n",
    "finally:\n",
    "    # 7. Clean up\n",
    "    print(\"\\nCleaning up test evaluation resources...\")\n",
    "    del model, test_loader, test_dataset, criterion\n",
    "    if 'test_loss' in locals(): del test_loss\n",
    "    if 'test_mae' in locals(): del test_mae\n",
    "    gc.collect()\n",
    "    if DEVICE.startswith('cuda'):\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Test evaluation finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf21f3",
   "metadata": {},
   "source": [
    "Feature fusion using Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baa929fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define this new model class in the same cell as the dataset and training loop,\n",
    "# or import it if defined elsewhere.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionFusionMLP(nn.Module):\n",
    "    def __init__(self, num_slices=4, embed_dim_per_slice=256, num_heads=4,\n",
    "                 mlp_hidden_dim1=128, mlp_hidden_dim2=64, mlp_hidden_dim3=32, dropout_rate=0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_slices (int): Number of input slices (e.g., 4).\n",
    "            embed_dim_per_slice (int): Dimension of the embedding from each slice (e.g., 256).\n",
    "            num_heads (int): Number of attention heads for fusing slice features. Must divide embed_dim_per_slice.\n",
    "            mlp_hidden_dim1, mlp_hidden_dim2, mlp_hidden_dim3 (int): Hidden dimensions for the final MLP.\n",
    "            dropout_rate (float): Dropout rate for the MLP.\n",
    "        \"\"\"\n",
    "        super(AttentionFusionMLP, self).__init__()\n",
    "        self.num_slices = num_slices\n",
    "        self.embed_dim = embed_dim_per_slice\n",
    "\n",
    "        if self.embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim_per_slice ({self.embed_dim}) must be divisible by num_heads ({num_heads})\")\n",
    "\n",
    "        # Attention layer to fuse features across slices\n",
    "        # Input: (batch_size, num_slices, embed_dim)\n",
    "        # We use embed_dim as query, key, and value dimension.\n",
    "        # batch_first=True expects input shape (batch_size, seq_len, feature_dim)\n",
    "        self.fusion_attention = nn.MultiheadAttention(embed_dim=self.embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.norm_attn = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "        # Learnable query vector (or use mean/max of input features as query)\n",
    "        # Using a learnable query allows the model to focus on relevant aspects for age prediction\n",
    "        self.query_vector = nn.Parameter(torch.randn(1, 1, self.embed_dim)) # (1, 1, embed_dim) for broadcasting\n",
    "\n",
    "        # MLP for final age prediction (takes the fused feature vector)\n",
    "        self.mlp = nn.Sequential(\n",
    "            # Input dimension is embed_dim_per_slice (the output of attention fusion)\n",
    "            nn.Linear(self.embed_dim, mlp_hidden_dim1), nn.BatchNorm1d(mlp_hidden_dim1), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mlp_hidden_dim1, mlp_hidden_dim2), nn.BatchNorm1d(mlp_hidden_dim2), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mlp_hidden_dim2, mlp_hidden_dim3), nn.BatchNorm1d(mlp_hidden_dim3), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mlp_hidden_dim3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, concat_dim) e.g., (32, 1024)\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Reshape concatenated features into (batch_size, num_slices, embed_dim)\n",
    "        # Example: (32, 1024) -> (32, 4, 256)\n",
    "        x_reshaped = x.view(batch_size, self.num_slices, self.embed_dim)\n",
    "\n",
    "        # Expand the learnable query vector to match the batch size\n",
    "        query = self.query_vector.expand(batch_size, -1, -1) # -> (batch_size, 1, embed_dim)\n",
    "\n",
    "        # Apply attention: Query attends to the slice features (Keys and Values)\n",
    "        # query: (batch_size, 1, embed_dim)\n",
    "        # key:   (batch_size, num_slices, embed_dim)\n",
    "        # value: (batch_size, num_slices, embed_dim)\n",
    "        # attn_output shape: (batch_size, 1, embed_dim)\n",
    "        attn_output, attn_weights = self.fusion_attention(query=query, key=x_reshaped, value=x_reshaped)\n",
    "\n",
    "        # Apply layer normalization (optional but often helpful)\n",
    "        # We apply it to the output corresponding to the query\n",
    "        fused_features = self.norm_attn(attn_output) # Still (batch_size, 1, embed_dim)\n",
    "\n",
    "        # Squeeze the sequence dimension (which was 1 for the query)\n",
    "        fused_features = fused_features.squeeze(1) # -> (batch_size, embed_dim)\n",
    "\n",
    "        # Pass the fused features through the MLP\n",
    "        output = self.mlp(fused_features) # -> (batch_size, 1)\n",
    "\n",
    "        return output.squeeze(-1) # -> (batch_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff063565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n",
      "Starting Attention Fusion MLP training...\n",
      "Target Slices for Fusion: [{'orientation': 'sagittal', 'index': 80}, {'orientation': 'sagittal', 'index': 125}, {'orientation': 'coronal', 'index': 125}, {'orientation': 'axial', 'index': 80}]\n",
      "Using device: cuda:1\n",
      "Feature Root: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice\n",
      "CSV Path: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Model will be saved to: attention_fusion_model\n",
      "Hyperparameters: Epochs=500, LR=0.0001, Batch=32, ES_Patience=50\n",
      "\n",
      "Setting up combined datasets (still concatenates for loading)...\n",
      "\n",
      "[Dataset Init - Combined] Split: train\n",
      "Target Slices: [{'orientation': 'sagittal', 'index': 80}, {'orientation': 'sagittal', 'index': 125}, {'orientation': 'coronal', 'index': 125}, {'orientation': 'axial', 'index': 80}]\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/train\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 2274 subject directories in 'train' to CSV entries.\n",
      "Warning: Could not map 1 directories to CSV.\n",
      "Scanning 2275 potential subject directories for all target slices...\n",
      "Info: 1 subject directories were skipped (no metadata mapping).\n",
      "Found 2274 valid subjects with all required slices in split train.\n",
      "\n",
      "[Dataset Init - Combined] Split: validation\n",
      "Target Slices: [{'orientation': 'sagittal', 'index': 80}, {'orientation': 'sagittal', 'index': 125}, {'orientation': 'coronal', 'index': 125}, {'orientation': 'axial', 'index': 80}]\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/validation\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 280 subject directories in 'validation' to CSV entries.\n",
      "Scanning 280 potential subject directories for all target slices...\n",
      "Found 280 valid subjects with all required slices in split validation.\n",
      "Setting up dataloaders...\n",
      "\n",
      "Initializing Attention Fusion model, optimizer, scheduler...\n",
      "\n",
      "--- Starting Attention Fusion Model Training ---\n",
      "Epoch 1/500 | Train Loss: 54.4538 | Val Loss: 53.7196 | Val MAE: 53.720 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.720. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 2/500 | Train Loss: 54.3330 | Val Loss: 53.5379 | Val MAE: 53.538 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.538. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 3/500 | Train Loss: 54.2198 | Val Loss: 53.4864 | Val MAE: 53.486 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.486. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 4/500 | Train Loss: 54.1130 | Val Loss: 53.4629 | Val MAE: 53.463 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.463. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 5/500 | Train Loss: 54.0012 | Val Loss: 53.3546 | Val MAE: 53.355 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.355. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 6/500 | Train Loss: 53.8797 | Val Loss: 53.2176 | Val MAE: 53.218 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.218. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 7/500 | Train Loss: 53.7642 | Val Loss: 53.1404 | Val MAE: 53.140 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.140. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 8/500 | Train Loss: 53.6525 | Val Loss: 53.0480 | Val MAE: 53.048 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 53.048. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 9/500 | Train Loss: 53.5177 | Val Loss: 52.8976 | Val MAE: 52.898 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.898. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 10/500 | Train Loss: 53.3871 | Val Loss: 52.6924 | Val MAE: 52.692 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.692. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 11/500 | Train Loss: 53.2533 | Val Loss: 52.7232 | Val MAE: 52.723 | LR: 1.0e-04\n",
      "Epoch 12/500 | Train Loss: 53.1116 | Val Loss: 52.5250 | Val MAE: 52.525 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.525. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 13/500 | Train Loss: 52.9720 | Val Loss: 52.4995 | Val MAE: 52.499 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.499. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 14/500 | Train Loss: 52.8567 | Val Loss: 52.2843 | Val MAE: 52.284 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.284. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 15/500 | Train Loss: 52.7102 | Val Loss: 52.0027 | Val MAE: 52.003 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 52.003. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 16/500 | Train Loss: 52.5155 | Val Loss: 51.6547 | Val MAE: 51.655 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.655. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 17/500 | Train Loss: 52.3342 | Val Loss: 51.6908 | Val MAE: 51.691 | LR: 1.0e-04\n",
      "Epoch 18/500 | Train Loss: 52.1656 | Val Loss: 51.3094 | Val MAE: 51.309 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.309. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 19/500 | Train Loss: 52.0158 | Val Loss: 51.2753 | Val MAE: 51.275 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.275. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 20/500 | Train Loss: 51.8073 | Val Loss: 51.2673 | Val MAE: 51.267 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 51.267. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 21/500 | Train Loss: 51.6307 | Val Loss: 50.9354 | Val MAE: 50.935 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.935. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 22/500 | Train Loss: 51.3892 | Val Loss: 50.8001 | Val MAE: 50.800 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.800. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 23/500 | Train Loss: 51.2612 | Val Loss: 50.5424 | Val MAE: 50.542 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.542. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 24/500 | Train Loss: 51.0173 | Val Loss: 50.2795 | Val MAE: 50.279 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.279. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 25/500 | Train Loss: 50.8169 | Val Loss: 50.1669 | Val MAE: 50.167 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.167. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 26/500 | Train Loss: 50.6047 | Val Loss: 50.0257 | Val MAE: 50.026 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 50.026. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 27/500 | Train Loss: 50.3972 | Val Loss: 49.7522 | Val MAE: 49.752 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.752. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 28/500 | Train Loss: 50.1018 | Val Loss: 49.3663 | Val MAE: 49.366 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.366. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 29/500 | Train Loss: 49.8772 | Val Loss: 49.1701 | Val MAE: 49.170 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 49.170. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 30/500 | Train Loss: 49.6190 | Val Loss: 48.9871 | Val MAE: 48.987 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.987. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 31/500 | Train Loss: 49.4122 | Val Loss: 48.7639 | Val MAE: 48.764 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.764. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 32/500 | Train Loss: 49.1057 | Val Loss: 48.7371 | Val MAE: 48.737 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.737. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 33/500 | Train Loss: 48.8718 | Val Loss: 48.1903 | Val MAE: 48.190 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.190. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 34/500 | Train Loss: 48.6484 | Val Loss: 48.2503 | Val MAE: 48.250 | LR: 1.0e-04\n",
      "Epoch 35/500 | Train Loss: 48.3721 | Val Loss: 48.0185 | Val MAE: 48.019 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 48.019. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 36/500 | Train Loss: 48.0943 | Val Loss: 47.5223 | Val MAE: 47.522 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.522. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 37/500 | Train Loss: 47.8006 | Val Loss: 47.4758 | Val MAE: 47.476 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.476. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 38/500 | Train Loss: 47.5164 | Val Loss: 47.1013 | Val MAE: 47.101 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 47.101. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 39/500 | Train Loss: 47.1809 | Val Loss: 46.6656 | Val MAE: 46.666 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 46.666. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 40/500 | Train Loss: 46.8588 | Val Loss: 46.4377 | Val MAE: 46.438 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 46.438. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 41/500 | Train Loss: 46.6080 | Val Loss: 46.0069 | Val MAE: 46.007 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 46.007. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 42/500 | Train Loss: 46.2567 | Val Loss: 45.9926 | Val MAE: 45.993 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.993. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 43/500 | Train Loss: 45.9257 | Val Loss: 45.4971 | Val MAE: 45.497 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.497. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 44/500 | Train Loss: 45.6344 | Val Loss: 45.4617 | Val MAE: 45.462 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 45.462. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 45/500 | Train Loss: 45.3199 | Val Loss: 44.5452 | Val MAE: 44.545 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 44.545. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 46/500 | Train Loss: 45.0024 | Val Loss: 44.6180 | Val MAE: 44.618 | LR: 1.0e-04\n",
      "Epoch 47/500 | Train Loss: 44.6323 | Val Loss: 44.1838 | Val MAE: 44.184 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 44.184. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 48/500 | Train Loss: 44.2854 | Val Loss: 43.9011 | Val MAE: 43.901 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 43.901. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 49/500 | Train Loss: 43.9379 | Val Loss: 43.6136 | Val MAE: 43.614 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 43.614. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 50/500 | Train Loss: 43.5674 | Val Loss: 43.3748 | Val MAE: 43.375 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 43.375. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 51/500 | Train Loss: 43.2769 | Val Loss: 42.8654 | Val MAE: 42.865 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 42.865. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 52/500 | Train Loss: 42.9304 | Val Loss: 42.3988 | Val MAE: 42.399 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 42.399. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 53/500 | Train Loss: 42.6371 | Val Loss: 42.2559 | Val MAE: 42.256 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 42.256. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 54/500 | Train Loss: 42.3186 | Val Loss: 41.8629 | Val MAE: 41.863 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 41.863. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 55/500 | Train Loss: 41.8222 | Val Loss: 41.5956 | Val MAE: 41.596 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 41.596. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 56/500 | Train Loss: 41.4992 | Val Loss: 41.1340 | Val MAE: 41.134 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 41.134. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 57/500 | Train Loss: 41.0563 | Val Loss: 40.5517 | Val MAE: 40.552 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.552. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 58/500 | Train Loss: 40.7492 | Val Loss: 40.3623 | Val MAE: 40.362 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.362. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 59/500 | Train Loss: 40.3114 | Val Loss: 40.0654 | Val MAE: 40.065 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 40.065. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 60/500 | Train Loss: 39.9439 | Val Loss: 39.6003 | Val MAE: 39.600 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 39.600. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 61/500 | Train Loss: 39.4350 | Val Loss: 39.1232 | Val MAE: 39.123 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 39.123. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 62/500 | Train Loss: 39.2302 | Val Loss: 39.2288 | Val MAE: 39.229 | LR: 1.0e-04\n",
      "Epoch 63/500 | Train Loss: 38.7957 | Val Loss: 37.4853 | Val MAE: 37.485 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 37.485. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 64/500 | Train Loss: 38.4101 | Val Loss: 37.2965 | Val MAE: 37.296 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 37.296. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 65/500 | Train Loss: 37.8946 | Val Loss: 37.7188 | Val MAE: 37.719 | LR: 1.0e-04\n",
      "Epoch 66/500 | Train Loss: 37.3863 | Val Loss: 38.0245 | Val MAE: 38.024 | LR: 1.0e-04\n",
      "Epoch 67/500 | Train Loss: 37.0955 | Val Loss: 40.2992 | Val MAE: 40.299 | LR: 1.0e-04\n",
      "Epoch 68/500 | Train Loss: 36.6108 | Val Loss: 38.6350 | Val MAE: 38.635 | LR: 1.0e-04\n",
      "Epoch 69/500 | Train Loss: 36.2915 | Val Loss: 36.0643 | Val MAE: 36.064 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 36.064. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 70/500 | Train Loss: 35.6528 | Val Loss: 32.3481 | Val MAE: 32.348 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 32.348. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 71/500 | Train Loss: 35.3650 | Val Loss: 34.0390 | Val MAE: 34.039 | LR: 1.0e-04\n",
      "Epoch 72/500 | Train Loss: 34.8794 | Val Loss: 34.2093 | Val MAE: 34.209 | LR: 1.0e-04\n",
      "Epoch 73/500 | Train Loss: 34.5128 | Val Loss: 32.0595 | Val MAE: 32.060 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 32.060. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 74/500 | Train Loss: 33.9213 | Val Loss: 36.1110 | Val MAE: 36.111 | LR: 1.0e-04\n",
      "Epoch 75/500 | Train Loss: 33.4959 | Val Loss: 36.2737 | Val MAE: 36.274 | LR: 1.0e-04\n",
      "Epoch 76/500 | Train Loss: 32.9721 | Val Loss: 35.8308 | Val MAE: 35.831 | LR: 1.0e-04\n",
      "Epoch 77/500 | Train Loss: 32.4808 | Val Loss: 36.7418 | Val MAE: 36.742 | LR: 1.0e-04\n",
      "Epoch 78/500 | Train Loss: 32.2218 | Val Loss: 29.1286 | Val MAE: 29.129 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 29.129. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 79/500 | Train Loss: 31.6441 | Val Loss: 28.5968 | Val MAE: 28.597 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 28.597. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 80/500 | Train Loss: 31.3289 | Val Loss: 31.2499 | Val MAE: 31.250 | LR: 1.0e-04\n",
      "Epoch 81/500 | Train Loss: 30.8794 | Val Loss: 27.8544 | Val MAE: 27.854 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 27.854. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 82/500 | Train Loss: 30.3354 | Val Loss: 38.1970 | Val MAE: 38.197 | LR: 1.0e-04\n",
      "Epoch 83/500 | Train Loss: 29.7174 | Val Loss: 32.2552 | Val MAE: 32.255 | LR: 1.0e-04\n",
      "Epoch 84/500 | Train Loss: 29.3201 | Val Loss: 36.0154 | Val MAE: 36.015 | LR: 1.0e-04\n",
      "Epoch 85/500 | Train Loss: 28.8107 | Val Loss: 43.2617 | Val MAE: 43.262 | LR: 1.0e-04\n",
      "Epoch 86/500 | Train Loss: 28.4507 | Val Loss: 30.6074 | Val MAE: 30.607 | LR: 1.0e-04\n",
      "Epoch 87/500 | Train Loss: 27.8074 | Val Loss: 22.4039 | Val MAE: 22.404 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 22.404. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 88/500 | Train Loss: 27.4453 | Val Loss: 29.6150 | Val MAE: 29.615 | LR: 1.0e-04\n",
      "Epoch 89/500 | Train Loss: 26.7475 | Val Loss: 26.9643 | Val MAE: 26.964 | LR: 1.0e-04\n",
      "Epoch 90/500 | Train Loss: 26.3914 | Val Loss: 29.9661 | Val MAE: 29.966 | LR: 1.0e-04\n",
      "Epoch 91/500 | Train Loss: 25.9916 | Val Loss: 37.6169 | Val MAE: 37.617 | LR: 1.0e-04\n",
      "Epoch 92/500 | Train Loss: 25.3197 | Val Loss: 26.5333 | Val MAE: 26.533 | LR: 1.0e-04\n",
      "Epoch 93/500 | Train Loss: 24.9151 | Val Loss: 26.8275 | Val MAE: 26.827 | LR: 1.0e-04\n",
      "Epoch 94/500 | Train Loss: 24.7162 | Val Loss: 25.0594 | Val MAE: 25.059 | LR: 1.0e-04\n",
      "Epoch 95/500 | Train Loss: 23.9211 | Val Loss: 28.9513 | Val MAE: 28.951 | LR: 1.0e-04\n",
      "Epoch 96/500 | Train Loss: 23.5538 | Val Loss: 22.3731 | Val MAE: 22.373 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 22.373. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 97/500 | Train Loss: 22.9559 | Val Loss: 27.5664 | Val MAE: 27.566 | LR: 1.0e-04\n",
      "Epoch 98/500 | Train Loss: 22.6136 | Val Loss: 36.1772 | Val MAE: 36.177 | LR: 1.0e-04\n",
      "Epoch 99/500 | Train Loss: 22.1595 | Val Loss: 28.0909 | Val MAE: 28.091 | LR: 1.0e-04\n",
      "Epoch 100/500 | Train Loss: 21.5570 | Val Loss: 23.9943 | Val MAE: 23.994 | LR: 1.0e-04\n",
      "Epoch 101/500 | Train Loss: 21.3193 | Val Loss: 22.9050 | Val MAE: 22.905 | LR: 1.0e-04\n",
      "Epoch 102/500 | Train Loss: 20.6623 | Val Loss: 19.1067 | Val MAE: 19.107 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 19.107. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 103/500 | Train Loss: 20.1328 | Val Loss: 15.2810 | Val MAE: 15.281 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 15.281. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 104/500 | Train Loss: 19.6866 | Val Loss: 30.0914 | Val MAE: 30.091 | LR: 1.0e-04\n",
      "Epoch 105/500 | Train Loss: 19.2310 | Val Loss: 23.1341 | Val MAE: 23.134 | LR: 1.0e-04\n",
      "Epoch 106/500 | Train Loss: 18.8469 | Val Loss: 20.1051 | Val MAE: 20.105 | LR: 1.0e-04\n",
      "Epoch 107/500 | Train Loss: 18.3850 | Val Loss: 32.4034 | Val MAE: 32.403 | LR: 1.0e-04\n",
      "Epoch 108/500 | Train Loss: 17.8762 | Val Loss: 15.5784 | Val MAE: 15.578 | LR: 1.0e-04\n",
      "Epoch 109/500 | Train Loss: 17.4504 | Val Loss: 34.3571 | Val MAE: 34.357 | LR: 1.0e-04\n",
      "Epoch 110/500 | Train Loss: 16.9990 | Val Loss: 13.5907 | Val MAE: 13.591 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 13.591. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 111/500 | Train Loss: 16.7914 | Val Loss: 24.1905 | Val MAE: 24.191 | LR: 1.0e-04\n",
      "Epoch 112/500 | Train Loss: 16.2867 | Val Loss: 17.2547 | Val MAE: 17.255 | LR: 1.0e-04\n",
      "Epoch 113/500 | Train Loss: 15.8975 | Val Loss: 12.8105 | Val MAE: 12.811 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 12.811. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 114/500 | Train Loss: 15.4195 | Val Loss: 13.0087 | Val MAE: 13.009 | LR: 1.0e-04\n",
      "Epoch 115/500 | Train Loss: 15.2106 | Val Loss: 16.4442 | Val MAE: 16.444 | LR: 1.0e-04\n",
      "Epoch 116/500 | Train Loss: 14.6561 | Val Loss: 11.5243 | Val MAE: 11.524 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 11.524. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 117/500 | Train Loss: 14.4880 | Val Loss: 9.5327 | Val MAE: 9.533 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 9.533. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 118/500 | Train Loss: 14.3901 | Val Loss: 23.3365 | Val MAE: 23.337 | LR: 1.0e-04\n",
      "Epoch 119/500 | Train Loss: 13.8082 | Val Loss: 12.6222 | Val MAE: 12.622 | LR: 1.0e-04\n",
      "Epoch 120/500 | Train Loss: 13.5869 | Val Loss: 12.6613 | Val MAE: 12.661 | LR: 1.0e-04\n",
      "Epoch 121/500 | Train Loss: 13.3888 | Val Loss: 11.2820 | Val MAE: 11.282 | LR: 1.0e-04\n",
      "Epoch 122/500 | Train Loss: 12.6910 | Val Loss: 13.0605 | Val MAE: 13.061 | LR: 1.0e-04\n",
      "Epoch 123/500 | Train Loss: 12.7408 | Val Loss: 9.1215 | Val MAE: 9.121 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 9.121. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 124/500 | Train Loss: 12.4917 | Val Loss: 11.0423 | Val MAE: 11.042 | LR: 1.0e-04\n",
      "Epoch 125/500 | Train Loss: 11.9635 | Val Loss: 13.5719 | Val MAE: 13.572 | LR: 1.0e-04\n",
      "Epoch 126/500 | Train Loss: 12.1112 | Val Loss: 8.7614 | Val MAE: 8.761 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.761. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 127/500 | Train Loss: 11.8221 | Val Loss: 8.3483 | Val MAE: 8.348 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.348. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 128/500 | Train Loss: 11.6425 | Val Loss: 10.3284 | Val MAE: 10.328 | LR: 1.0e-04\n",
      "Epoch 129/500 | Train Loss: 11.3730 | Val Loss: 11.3952 | Val MAE: 11.395 | LR: 1.0e-04\n",
      "Epoch 130/500 | Train Loss: 11.0854 | Val Loss: 8.2464 | Val MAE: 8.246 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 8.246. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 131/500 | Train Loss: 11.1141 | Val Loss: 7.9232 | Val MAE: 7.923 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.923. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 132/500 | Train Loss: 10.8322 | Val Loss: 11.1271 | Val MAE: 11.127 | LR: 1.0e-04\n",
      "Epoch 133/500 | Train Loss: 10.7666 | Val Loss: 13.2607 | Val MAE: 13.261 | LR: 1.0e-04\n",
      "Epoch 134/500 | Train Loss: 10.3493 | Val Loss: 15.6562 | Val MAE: 15.656 | LR: 1.0e-04\n",
      "Epoch 135/500 | Train Loss: 10.3388 | Val Loss: 7.9462 | Val MAE: 7.946 | LR: 1.0e-04\n",
      "Epoch 136/500 | Train Loss: 10.2341 | Val Loss: 11.4061 | Val MAE: 11.406 | LR: 1.0e-04\n",
      "Epoch 137/500 | Train Loss: 10.3203 | Val Loss: 11.6009 | Val MAE: 11.601 | LR: 1.0e-04\n",
      "Epoch 138/500 | Train Loss: 10.1537 | Val Loss: 7.9421 | Val MAE: 7.942 | LR: 1.0e-04\n",
      "Epoch 139/500 | Train Loss: 9.9552 | Val Loss: 9.3015 | Val MAE: 9.302 | LR: 1.0e-04\n",
      "Epoch 140/500 | Train Loss: 9.7580 | Val Loss: 8.1496 | Val MAE: 8.150 | LR: 1.0e-04\n",
      "Epoch 141/500 | Train Loss: 9.9197 | Val Loss: 10.3747 | Val MAE: 10.375 | LR: 1.0e-04\n",
      "Epoch 142/500 | Train Loss: 9.9372 | Val Loss: 7.2709 | Val MAE: 7.271 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.271. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 143/500 | Train Loss: 9.8698 | Val Loss: 7.6723 | Val MAE: 7.672 | LR: 1.0e-04\n",
      "Epoch 144/500 | Train Loss: 9.6772 | Val Loss: 8.5580 | Val MAE: 8.558 | LR: 1.0e-04\n",
      "Epoch 145/500 | Train Loss: 9.5361 | Val Loss: 14.2685 | Val MAE: 14.269 | LR: 1.0e-04\n",
      "Epoch 146/500 | Train Loss: 9.7733 | Val Loss: 7.9413 | Val MAE: 7.941 | LR: 1.0e-04\n",
      "Epoch 147/500 | Train Loss: 9.3973 | Val Loss: 8.4363 | Val MAE: 8.436 | LR: 1.0e-04\n",
      "Epoch 148/500 | Train Loss: 9.3412 | Val Loss: 8.5780 | Val MAE: 8.578 | LR: 1.0e-04\n",
      "Epoch 149/500 | Train Loss: 9.6555 | Val Loss: 8.5162 | Val MAE: 8.516 | LR: 1.0e-04\n",
      "Epoch 150/500 | Train Loss: 9.7303 | Val Loss: 8.5479 | Val MAE: 8.548 | LR: 1.0e-04\n",
      "Epoch 151/500 | Train Loss: 9.4634 | Val Loss: 10.1396 | Val MAE: 10.140 | LR: 1.0e-04\n",
      "Epoch 152/500 | Train Loss: 9.6234 | Val Loss: 8.9356 | Val MAE: 8.936 | LR: 1.0e-04\n",
      "Epoch 153/500 | Train Loss: 9.3678 | Val Loss: 7.8432 | Val MAE: 7.843 | LR: 1.0e-04\n",
      "Epoch 154/500 | Train Loss: 9.4180 | Val Loss: 8.6639 | Val MAE: 8.664 | LR: 1.0e-04\n",
      "Epoch 155/500 | Train Loss: 9.2148 | Val Loss: 7.3709 | Val MAE: 7.371 | LR: 1.0e-04\n",
      "Epoch 156/500 | Train Loss: 9.3084 | Val Loss: 7.1173 | Val MAE: 7.117 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 7.117. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 157/500 | Train Loss: 9.1943 | Val Loss: 6.7712 | Val MAE: 6.771 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 6.771. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 158/500 | Train Loss: 9.4541 | Val Loss: 7.1699 | Val MAE: 7.170 | LR: 1.0e-04\n",
      "Epoch 159/500 | Train Loss: 9.2105 | Val Loss: 9.5253 | Val MAE: 9.525 | LR: 1.0e-04\n",
      "Epoch 160/500 | Train Loss: 9.3795 | Val Loss: 9.8813 | Val MAE: 9.881 | LR: 1.0e-04\n",
      "Epoch 161/500 | Train Loss: 9.3428 | Val Loss: 12.5984 | Val MAE: 12.598 | LR: 1.0e-04\n",
      "Epoch 162/500 | Train Loss: 9.0551 | Val Loss: 9.2413 | Val MAE: 9.241 | LR: 1.0e-04\n",
      "Epoch 163/500 | Train Loss: 8.9233 | Val Loss: 13.5770 | Val MAE: 13.577 | LR: 1.0e-04\n",
      "Epoch 164/500 | Train Loss: 9.1413 | Val Loss: 7.0780 | Val MAE: 7.078 | LR: 1.0e-04\n",
      "Epoch 165/500 | Train Loss: 9.2217 | Val Loss: 9.7956 | Val MAE: 9.796 | LR: 1.0e-04\n",
      "Epoch 166/500 | Train Loss: 9.1994 | Val Loss: 9.2869 | Val MAE: 9.287 | LR: 1.0e-04\n",
      "Epoch 167/500 | Train Loss: 9.1532 | Val Loss: 6.9056 | Val MAE: 6.906 | LR: 1.0e-04\n",
      "Epoch 168/500 | Train Loss: 9.1216 | Val Loss: 6.9884 | Val MAE: 6.988 | LR: 1.0e-04\n",
      "Epoch 169/500 | Train Loss: 9.1036 | Val Loss: 7.1023 | Val MAE: 7.102 | LR: 1.0e-04\n",
      "Epoch 170/500 | Train Loss: 9.1133 | Val Loss: 7.6794 | Val MAE: 7.679 | LR: 1.0e-04\n",
      "Epoch 171/500 | Train Loss: 8.8861 | Val Loss: 10.1522 | Val MAE: 10.152 | LR: 1.0e-04\n",
      "Epoch 172/500 | Train Loss: 9.1618 | Val Loss: 7.0889 | Val MAE: 7.089 | LR: 1.0e-04\n",
      "Epoch 173/500 | Train Loss: 8.9762 | Val Loss: 6.9890 | Val MAE: 6.989 | LR: 1.0e-04\n",
      "Epoch 174/500 | Train Loss: 8.8920 | Val Loss: 7.4189 | Val MAE: 7.419 | LR: 1.0e-04\n",
      "Epoch 175/500 | Train Loss: 8.9872 | Val Loss: 7.7965 | Val MAE: 7.797 | LR: 1.0e-04\n",
      "Epoch 176/500 | Train Loss: 9.1400 | Val Loss: 8.0440 | Val MAE: 8.044 | LR: 1.0e-04\n",
      "Epoch 177/500 | Train Loss: 8.9594 | Val Loss: 7.0035 | Val MAE: 7.003 | LR: 1.0e-04\n",
      "Epoch 178/500 | Train Loss: 9.2376 | Val Loss: 7.2248 | Val MAE: 7.225 | LR: 1.0e-04\n",
      "Epoch 179/500 | Train Loss: 8.5852 | Val Loss: 8.2204 | Val MAE: 8.220 | LR: 1.0e-04\n",
      "Epoch 180/500 | Train Loss: 8.8811 | Val Loss: 7.4191 | Val MAE: 7.419 | LR: 1.0e-04\n",
      "Epoch 181/500 | Train Loss: 8.9400 | Val Loss: 6.9009 | Val MAE: 6.901 | LR: 1.0e-04\n",
      "Epoch 182/500 | Train Loss: 9.1266 | Val Loss: 7.0924 | Val MAE: 7.092 | LR: 1.0e-04\n",
      "Epoch 183/500 | Train Loss: 8.8048 | Val Loss: 11.7046 | Val MAE: 11.705 | LR: 1.0e-04\n",
      "Epoch 184/500 | Train Loss: 9.1186 | Val Loss: 7.8248 | Val MAE: 7.825 | LR: 1.0e-04\n",
      "Epoch 185/500 | Train Loss: 8.7702 | Val Loss: 6.9268 | Val MAE: 6.927 | LR: 1.0e-04\n",
      "Epoch 186/500 | Train Loss: 8.9339 | Val Loss: 9.5633 | Val MAE: 9.563 | LR: 1.0e-04\n",
      "Epoch 187/500 | Train Loss: 8.9084 | Val Loss: 12.1643 | Val MAE: 12.164 | LR: 1.0e-04\n",
      "Epoch 188/500 | Train Loss: 8.9360 | Val Loss: 13.7855 | Val MAE: 13.786 | LR: 1.0e-04\n",
      "Epoch 189/500 | Train Loss: 8.6086 | Val Loss: 6.7313 | Val MAE: 6.731 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 6.731. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 190/500 | Train Loss: 8.6554 | Val Loss: 7.2491 | Val MAE: 7.249 | LR: 1.0e-04\n",
      "Epoch 191/500 | Train Loss: 8.9402 | Val Loss: 6.9964 | Val MAE: 6.996 | LR: 1.0e-04\n",
      "Epoch 192/500 | Train Loss: 8.6263 | Val Loss: 10.3992 | Val MAE: 10.399 | LR: 1.0e-04\n",
      "Epoch 193/500 | Train Loss: 8.7961 | Val Loss: 7.2979 | Val MAE: 7.298 | LR: 1.0e-04\n",
      "Epoch 194/500 | Train Loss: 8.9653 | Val Loss: 8.7394 | Val MAE: 8.739 | LR: 1.0e-04\n",
      "Epoch 195/500 | Train Loss: 8.9500 | Val Loss: 6.8334 | Val MAE: 6.833 | LR: 1.0e-04\n",
      "Epoch 196/500 | Train Loss: 9.1261 | Val Loss: 10.8353 | Val MAE: 10.835 | LR: 1.0e-04\n",
      "Epoch 197/500 | Train Loss: 8.8621 | Val Loss: 7.6062 | Val MAE: 7.606 | LR: 1.0e-04\n",
      "Epoch 198/500 | Train Loss: 8.5737 | Val Loss: 8.8944 | Val MAE: 8.894 | LR: 1.0e-04\n",
      "Epoch 199/500 | Train Loss: 8.6078 | Val Loss: 7.5896 | Val MAE: 7.590 | LR: 1.0e-04\n",
      "Epoch 200/500 | Train Loss: 8.7480 | Val Loss: 7.4564 | Val MAE: 7.456 | LR: 1.0e-04\n",
      "Epoch 201/500 | Train Loss: 8.5361 | Val Loss: 8.9831 | Val MAE: 8.983 | LR: 1.0e-04\n",
      "Epoch 202/500 | Train Loss: 9.2318 | Val Loss: 7.3431 | Val MAE: 7.343 | LR: 1.0e-04\n",
      "Epoch 203/500 | Train Loss: 8.8603 | Val Loss: 9.0549 | Val MAE: 9.055 | LR: 1.0e-04\n",
      "Epoch 204/500 | Train Loss: 8.8086 | Val Loss: 6.9048 | Val MAE: 6.905 | LR: 1.0e-04\n",
      "Epoch 205/500 | Train Loss: 8.8873 | Val Loss: 13.4398 | Val MAE: 13.440 | LR: 1.0e-04\n",
      "Epoch 206/500 | Train Loss: 8.7645 | Val Loss: 7.7081 | Val MAE: 7.708 | LR: 1.0e-04\n",
      "Epoch 207/500 | Train Loss: 8.4745 | Val Loss: 7.3832 | Val MAE: 7.383 | LR: 1.0e-04\n",
      "Epoch 208/500 | Train Loss: 8.5456 | Val Loss: 8.1137 | Val MAE: 8.114 | LR: 1.0e-04\n",
      "Epoch 209/500 | Train Loss: 8.5183 | Val Loss: 10.3679 | Val MAE: 10.368 | LR: 1.0e-04\n",
      "Epoch 210/500 | Train Loss: 8.7824 | Val Loss: 7.2206 | Val MAE: 7.221 | LR: 1.0e-04\n",
      "Epoch 211/500 | Train Loss: 8.7653 | Val Loss: 8.1084 | Val MAE: 8.108 | LR: 1.0e-04\n",
      "Epoch 212/500 | Train Loss: 8.5423 | Val Loss: 7.0536 | Val MAE: 7.054 | LR: 1.0e-04\n",
      "Epoch 213/500 | Train Loss: 8.7802 | Val Loss: 7.5314 | Val MAE: 7.531 | LR: 1.0e-04\n",
      "Epoch 214/500 | Train Loss: 8.6902 | Val Loss: 6.9919 | Val MAE: 6.992 | LR: 1.0e-04\n",
      "Epoch 215/500 | Train Loss: 8.7126 | Val Loss: 7.6417 | Val MAE: 7.642 | LR: 1.0e-04\n",
      "Epoch 216/500 | Train Loss: 8.6111 | Val Loss: 7.3610 | Val MAE: 7.361 | LR: 1.0e-04\n",
      "Epoch 217/500 | Train Loss: 8.6568 | Val Loss: 9.1400 | Val MAE: 9.140 | LR: 1.0e-04\n",
      "Epoch 218/500 | Train Loss: 8.5708 | Val Loss: 7.3336 | Val MAE: 7.334 | LR: 1.0e-04\n",
      "Epoch 219/500 | Train Loss: 8.7289 | Val Loss: 6.7993 | Val MAE: 6.799 | LR: 1.0e-04\n",
      "Epoch 220/500 | Train Loss: 8.3737 | Val Loss: 6.8359 | Val MAE: 6.836 | LR: 1.0e-04\n",
      "Epoch 221/500 | Train Loss: 8.4218 | Val Loss: 6.5487 | Val MAE: 6.549 | LR: 1.0e-04\n",
      "  -> New best Val MAE: 6.549. Saved model to attention_fusion_model/best_fusion_model.pth\n",
      "Epoch 222/500 | Train Loss: 8.2972 | Val Loss: 9.5756 | Val MAE: 9.576 | LR: 1.0e-04\n",
      "Epoch 223/500 | Train Loss: 8.7724 | Val Loss: 10.4998 | Val MAE: 10.500 | LR: 1.0e-04\n",
      "Epoch 224/500 | Train Loss: 8.4581 | Val Loss: 7.7378 | Val MAE: 7.738 | LR: 1.0e-04\n",
      "Epoch 225/500 | Train Loss: 8.4932 | Val Loss: 8.3398 | Val MAE: 8.340 | LR: 1.0e-04\n",
      "Epoch 226/500 | Train Loss: 8.3979 | Val Loss: 7.1474 | Val MAE: 7.147 | LR: 1.0e-04\n",
      "Epoch 227/500 | Train Loss: 8.2944 | Val Loss: 6.7437 | Val MAE: 6.744 | LR: 1.0e-04\n",
      "Epoch 228/500 | Train Loss: 8.4859 | Val Loss: 6.8615 | Val MAE: 6.861 | LR: 1.0e-04\n",
      "Epoch 229/500 | Train Loss: 8.4585 | Val Loss: 7.2680 | Val MAE: 7.268 | LR: 1.0e-04\n",
      "Epoch 230/500 | Train Loss: 8.3904 | Val Loss: 8.1801 | Val MAE: 8.180 | LR: 1.0e-04\n",
      "Epoch 231/500 | Train Loss: 8.4073 | Val Loss: 8.9488 | Val MAE: 8.949 | LR: 1.0e-04\n",
      "Epoch 232/500 | Train Loss: 8.4225 | Val Loss: 7.3673 | Val MAE: 7.367 | LR: 1.0e-04\n",
      "Epoch 233/500 | Train Loss: 8.4700 | Val Loss: 7.4798 | Val MAE: 7.480 | LR: 1.0e-04\n",
      "Epoch 234/500 | Train Loss: 8.2493 | Val Loss: 8.1415 | Val MAE: 8.141 | LR: 1.0e-04\n",
      "Epoch 235/500 | Train Loss: 8.3267 | Val Loss: 6.8797 | Val MAE: 6.880 | LR: 1.0e-04\n",
      "Epoch 236/500 | Train Loss: 8.6592 | Val Loss: 10.7592 | Val MAE: 10.759 | LR: 1.0e-04\n",
      "Epoch 237/500 | Train Loss: 8.4631 | Val Loss: 7.2478 | Val MAE: 7.248 | LR: 1.0e-04\n",
      "Epoch 238/500 | Train Loss: 8.4645 | Val Loss: 7.7602 | Val MAE: 7.760 | LR: 1.0e-04\n",
      "Epoch 239/500 | Train Loss: 8.3254 | Val Loss: 7.4381 | Val MAE: 7.438 | LR: 1.0e-04\n",
      "Epoch 240/500 | Train Loss: 8.5723 | Val Loss: 7.8388 | Val MAE: 7.839 | LR: 1.0e-04\n",
      "Epoch 241/500 | Train Loss: 8.6032 | Val Loss: 7.7532 | Val MAE: 7.753 | LR: 1.0e-04\n",
      "Epoch 242/500 | Train Loss: 8.5645 | Val Loss: 6.7661 | Val MAE: 6.766 | LR: 1.0e-04\n",
      "Epoch 243/500 | Train Loss: 8.1322 | Val Loss: 10.5880 | Val MAE: 10.588 | LR: 1.0e-04\n",
      "Epoch 244/500 | Train Loss: 8.0728 | Val Loss: 9.2433 | Val MAE: 9.243 | LR: 1.0e-04\n",
      "Epoch 245/500 | Train Loss: 8.2848 | Val Loss: 9.6239 | Val MAE: 9.624 | LR: 1.0e-04\n",
      "Epoch 246/500 | Train Loss: 8.7398 | Val Loss: 7.1543 | Val MAE: 7.154 | LR: 1.0e-04\n",
      "Epoch 247/500 | Train Loss: 8.3862 | Val Loss: 6.9372 | Val MAE: 6.937 | LR: 1.0e-04\n",
      "Epoch 248/500 | Train Loss: 8.4517 | Val Loss: 11.9349 | Val MAE: 11.935 | LR: 1.0e-04\n",
      "Epoch 249/500 | Train Loss: 8.4119 | Val Loss: 8.8595 | Val MAE: 8.860 | LR: 1.0e-04\n",
      "Epoch 250/500 | Train Loss: 8.3154 | Val Loss: 7.3821 | Val MAE: 7.382 | LR: 1.0e-04\n",
      "Epoch 251/500 | Train Loss: 8.4767 | Val Loss: 10.0543 | Val MAE: 10.054 | LR: 1.0e-04\n",
      "Epoch 252/500 | Train Loss: 8.2463 | Val Loss: 9.3379 | Val MAE: 9.338 | LR: 1.0e-04\n",
      "Epoch 253/500 | Train Loss: 8.2282 | Val Loss: 7.2327 | Val MAE: 7.233 | LR: 1.0e-04\n",
      "Epoch 254/500 | Train Loss: 8.3016 | Val Loss: 8.5243 | Val MAE: 8.524 | LR: 1.0e-04\n",
      "Epoch 255/500 | Train Loss: 8.3947 | Val Loss: 8.5648 | Val MAE: 8.565 | LR: 1.0e-04\n",
      "Epoch 256/500 | Train Loss: 8.3481 | Val Loss: 7.3340 | Val MAE: 7.334 | LR: 1.0e-04\n",
      "Epoch 257/500 | Train Loss: 8.2739 | Val Loss: 12.0144 | Val MAE: 12.014 | LR: 1.0e-04\n",
      "Epoch 258/500 | Train Loss: 8.3934 | Val Loss: 7.5044 | Val MAE: 7.504 | LR: 1.0e-04\n",
      "Epoch 259/500 | Train Loss: 8.4495 | Val Loss: 7.1118 | Val MAE: 7.112 | LR: 1.0e-04\n",
      "Epoch 260/500 | Train Loss: 8.4524 | Val Loss: 6.7736 | Val MAE: 6.774 | LR: 1.0e-04\n",
      "Epoch 261/500 | Train Loss: 8.2570 | Val Loss: 7.3835 | Val MAE: 7.383 | LR: 1.0e-04\n",
      "Epoch 262/500 | Train Loss: 8.4998 | Val Loss: 15.6637 | Val MAE: 15.664 | LR: 1.0e-04\n",
      "Epoch 263/500 | Train Loss: 8.2260 | Val Loss: 8.0691 | Val MAE: 8.069 | LR: 1.0e-04\n",
      "Epoch 264/500 | Train Loss: 8.2767 | Val Loss: 7.0410 | Val MAE: 7.041 | LR: 1.0e-04\n",
      "Epoch 265/500 | Train Loss: 8.3854 | Val Loss: 7.4012 | Val MAE: 7.401 | LR: 1.0e-04\n",
      "Epoch 266/500 | Train Loss: 8.4074 | Val Loss: 13.0737 | Val MAE: 13.074 | LR: 1.0e-04\n",
      "Epoch 267/500 | Train Loss: 8.1179 | Val Loss: 9.3256 | Val MAE: 9.326 | LR: 1.0e-04\n",
      "Epoch 268/500 | Train Loss: 8.0841 | Val Loss: 6.9959 | Val MAE: 6.996 | LR: 1.0e-04\n",
      "Epoch 269/500 | Train Loss: 8.2454 | Val Loss: 6.8873 | Val MAE: 6.887 | LR: 1.0e-04\n",
      "Epoch 270/500 | Train Loss: 8.2512 | Val Loss: 7.1506 | Val MAE: 7.151 | LR: 1.0e-04\n",
      "Epoch 271/500 | Train Loss: 8.3952 | Val Loss: 8.2195 | Val MAE: 8.220 | LR: 1.0e-04\n",
      "\n",
      "Early stopping triggered after 50 epochs without improvement.\n",
      "\n",
      "--- Attention Fusion Model Training Finished ---\n",
      "Best Validation MAE: 6.549 achieved at epoch 221\n",
      "Best model saved to: attention_fusion_model/best_fusion_model.pth\n",
      "\n",
      "Cleaning up resources...\n",
      "\n",
      "Attention Fusion model training finished.\n"
     ]
    }
   ],
   "source": [
    "# In the third code cell (or whichever cell contains the main training loop)\n",
    "\n",
    "# ... (Keep imports, configurations, Dataset definition, train/eval functions) ...\n",
    "# ... (Define or import AttentionFusionMLP class here) ...\n",
    "\n",
    "# --- Main Training Loop (Single Model - Now Fusion) ---\n",
    "if __name__ == \"__main__\": # Or run directly in notebook cell\n",
    "    # --- Set Random Seed (Keep this if you added it) ---\n",
    "    SEED = 42\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {SEED}\")\n",
    "    # --- End Set Random Seed ---\n",
    "\n",
    "    # --- Update Output Directory for Fusion Model ---\n",
    "    MODELS_OUTPUT_DIR = Path(\"./attention_fusion_model\") # New directory for this model\n",
    "    # --- End Update ---\n",
    "\n",
    "    print(f\"Starting Attention Fusion MLP training...\") # Updated message\n",
    "    print(f\"Target Slices for Fusion: {TARGET_SLICES}\")\n",
    "    # CONCAT_DIM is still relevant for the dataset loading, but not the MLP input directly\n",
    "    # print(f\"Concatenated Input Dimension (for Dataset): {CONCAT_DIM}\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"Feature Root: {FEATURE_ROOT}\")\n",
    "    print(f\"CSV Path: {CSV_PATH}\")\n",
    "    print(f\"Model will be saved to: {MODELS_OUTPUT_DIR}\") # Updated path\n",
    "    print(f\"Hyperparameters: Epochs={EPOCHS}, LR={LEARNING_RATE}, Batch={BATCH_SIZE}, ES_Patience={EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "    MODELS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    model_save_path = MODELS_OUTPUT_DIR / \"best_fusion_model.pth\" # Updated filename\n",
    "\n",
    "    try:\n",
    "        print(\"\\nSetting up combined datasets (still concatenates for loading)...\")\n",
    "        # Dataset remains the same - it loads and concatenates features\n",
    "        train_dataset = CombinedSliceDataset(FEATURE_ROOT, CSV_PATH, 'train', TARGET_SLICES)\n",
    "        val_dataset = CombinedSliceDataset(FEATURE_ROOT, CSV_PATH, 'validation', TARGET_SLICES)\n",
    "        # Optional: test_dataset = CombinedSliceDataset(FEATURE_ROOT, CSV_PATH, 'test', TARGET_SLICES)\n",
    "\n",
    "        print(\"Setting up dataloaders...\")\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        # Optional: test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    except (FileNotFoundError, ValueError, RuntimeError) as e:\n",
    "        print(f\"\\nError initializing combined datasets/loaders: {e}\")\n",
    "        print(\"Stopping training.\")\n",
    "        exit() # Or raise e\n",
    "\n",
    "    print(\"\\nInitializing Attention Fusion model, optimizer, scheduler...\")\n",
    "    # --- Instantiate the NEW Fusion Model ---\n",
    "    model = AttentionFusionMLP(\n",
    "        num_slices=len(TARGET_SLICES),\n",
    "        embed_dim_per_slice=256, # Assuming 256 from GAP\n",
    "        num_heads=8, # Example: ensure 256 % 8 == 0\n",
    "        mlp_hidden_dim1=128,\n",
    "        mlp_hidden_dim2=64,\n",
    "        mlp_hidden_dim3=32,\n",
    "        dropout_rate=0.3\n",
    "    ).to(DEVICE)\n",
    "    # --- End Model Instantiation ---\n",
    "\n",
    "    criterion = nn.L1Loss() # MAE Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler_patience_epochs = max(1, int(EPOCHS * SCHEDULER_PATIENCE_PERCENT))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=scheduler_patience_epochs, factor=0.5, verbose=False)\n",
    "\n",
    "    best_val_mae = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_epoch = -1\n",
    "\n",
    "    print(f\"\\n--- Starting Attention Fusion Model Training ---\") # Updated message\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        val_loss, val_mae = evaluate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val MAE: {val_mae:.3f} | LR: {current_lr:.1e}\")\n",
    "\n",
    "        scheduler.step(val_loss) # Step based on validation loss\n",
    "\n",
    "        if val_mae < best_val_mae:\n",
    "            best_val_mae = val_mae\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"  -> New best Val MAE: {best_val_mae:.3f}. Saved model to {model_save_path}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered after {EARLY_STOPPING_PATIENCE} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "             gc.collect()\n",
    "             if DEVICE.startswith('cuda'):\n",
    "                 torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\n--- Attention Fusion Model Training Finished ---\") # Updated message\n",
    "    if best_epoch != -1:\n",
    "         print(f\"Best Validation MAE: {best_val_mae:.3f} achieved at epoch {best_epoch}\")\n",
    "         print(f\"Best model saved to: {model_save_path}\")\n",
    "         final_results = {'best_val_mae': best_val_mae, 'best_epoch': best_epoch}\n",
    "    else:\n",
    "         print(\"No improvement found during training.\")\n",
    "         final_results = {'best_val_mae': float('inf'), 'best_epoch': -1, 'error': 'No improvement'}\n",
    "\n",
    "\n",
    "    # --- Optional: Evaluate on Test Set (Ensure model instantiation matches) ---\n",
    "    # if 'test_loader' in locals() and model_save_path.exists():\n",
    "    #     print(f\"\\n--- Evaluating Fusion Model on Test Set ---\")\n",
    "    #     try:\n",
    "    #         # Re-initialize the model structure before loading state dict\n",
    "    #         model_test = AttentionFusionMLP(\n",
    "    #             num_slices=len(TARGET_SLICES), embed_dim_per_slice=256, num_heads=8 # Use same params\n",
    "    #         ).to(DEVICE)\n",
    "    #         model_test.load_state_dict(torch.load(model_save_path, map_location=DEVICE))\n",
    "    #         model_test.eval()\n",
    "    #         test_loss, test_mae = evaluate(model_test, test_loader, criterion, DEVICE) # Use model_test\n",
    "    #         print(f\"Fusion Model Test Loss: {test_loss:.4f} | Test MAE: {test_mae:.3f}\")\n",
    "    #         final_results['test_mae'] = test_mae\n",
    "    #         del model_test # Clean up test model\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error during test set evaluation: {e}\")\n",
    "    # else:\n",
    "    #     print(\"\\nSkipping test set evaluation.\")\n",
    "\n",
    "    # Clean up\n",
    "    print(\"\\nCleaning up resources...\")\n",
    "    # Ensure model is deleted if it exists\n",
    "    if 'model' in locals(): del model\n",
    "    del optimizer, scheduler, train_dataset, val_dataset, train_loader, val_loader\n",
    "    # if 'test_dataset' in locals(): del test_dataset\n",
    "    # if 'test_loader' in locals(): del test_loader\n",
    "    gc.collect()\n",
    "    if DEVICE.startswith('cuda'):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\nAttention Fusion model training finished.\") # Updated message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e2611c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Attention Fusion MLP on Test Set ---\n",
      "Loading model from: attention_fusion_model/best_fusion_model.pth\n",
      "Using test data from: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice (split: test)\n",
      "CSV Path: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Device: cuda:1\n",
      "\n",
      "Setting up test dataset...\n",
      "\n",
      "[Dataset Init - Combined] Split: test\n",
      "Target Slices: [{'orientation': 'sagittal', 'index': 80}, {'orientation': 'sagittal', 'index': 125}, {'orientation': 'coronal', 'index': 125}, {'orientation': 'axial', 'index': 80}]\n",
      "Scanning subjects in: /data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice/test\n",
      "Loading metadata from: /data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\n",
      "Loaded metadata for 2850 subjects from CSV.\n",
      "Successfully mapped 296 subject directories in 'test' to CSV entries.\n",
      "Scanning 296 potential subject directories for all target slices...\n",
      "Found 296 valid subjects with all required slices in split test.\n",
      "Setting up test dataloader...\n",
      "\n",
      "Initializing AttentionFusionMLP model structure...\n",
      "Loading model state dict from attention_fusion_model/best_fusion_model.pth...\n",
      "Model loaded successfully.\n",
      "\n",
      "Starting evaluation on the test set...\n",
      "\n",
      "--- Attention Fusion Model Test Set Results ---\n",
      "Test Loss (calculated using L1Loss): 6.4812\n",
      "Test MAE: 6.481\n",
      "\n",
      "Cleaning up test evaluation resources...\n",
      "Test evaluation finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict # Keep for potential future use if needed\n",
    "\n",
    "# --- Configuration ---\n",
    "FEATURE_ROOT = Path(\"/data/kuang/Projects/MedSAM/data/BrainAGE_preprocessed_multi_slice\")\n",
    "CSV_PATH = Path(\"/data/kuang/Projects/MedSAM/data/Subject_demographics_info_brain_age.csv\")\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\" # Use specific GPU if desired\n",
    "SPLITS = [\"train\", \"validation\", \"test\"] # Define dataset splits\n",
    "\n",
    "# Define the target slices used for the fusion model\n",
    "TARGET_SLICES = [\n",
    "    {'orientation': 'sagittal', 'index': 80},\n",
    "    {'orientation': 'sagittal', 'index': 125},\n",
    "    {'orientation': 'coronal',  'index': 125},\n",
    "    {'orientation': 'axial',    'index': 80},\n",
    "]\n",
    "CONCAT_DIM = 256 * len(TARGET_SLICES) # 4 * 256 = 1024\n",
    "\n",
    "# --- Hyperparameters (relevant for DataLoader) ---\n",
    "BATCH_SIZE = 32 # Adjust based on GPU memory\n",
    "NUM_WORKERS = 4 # Dataloader workers\n",
    "\n",
    "# --- Configuration for Test Evaluation ---\n",
    "# Directory where the fusion model was saved\n",
    "FUSION_MODEL_DIR = Path(\"./attention_fusion_model\")\n",
    "# Path to the specific saved model file\n",
    "MODEL_SAVE_PATH = FUSION_MODEL_DIR / \"best_fusion_model.pth\"\n",
    "TEST_SPLIT = 'test' # Explicitly define the split for clarity\n",
    "\n",
    "# Model parameters (MUST match the trained model)\n",
    "NUM_SLICES_TEST = len(TARGET_SLICES)\n",
    "EMBED_DIM_PER_SLICE_TEST = 256\n",
    "NUM_HEADS_TEST = 8 # Ensure this matches the num_heads used during training\n",
    "MLP_HIDDEN_DIM1_TEST = 128\n",
    "MLP_HIDDEN_DIM2_TEST = 64\n",
    "MLP_HIDDEN_DIM3_TEST = 32\n",
    "DROPOUT_RATE_TEST = 0.3 # Usually dropout is inactive in eval mode, but good practice to match\n",
    "\n",
    "# --- Dataset Definition (Modified for Concatenating Slices) ---\n",
    "class CombinedSliceDataset(Dataset):\n",
    "    def __init__(self, feature_root, csv_path, split, target_slices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_root (Path): Base directory containing split folders.\n",
    "            csv_path (Path): Path to the CSV file with metadata.\n",
    "            split (str): The dataset split ('train', 'validation', or 'test').\n",
    "            target_slices (list): List of dicts, each specifying {'orientation': str, 'index': int}.\n",
    "        \"\"\"\n",
    "        self.feature_root = Path(feature_root)\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.split = split\n",
    "        self.target_slices = target_slices\n",
    "        self.split_dir = self.feature_root / self.split\n",
    "\n",
    "        print(f\"\\n[Dataset Init - Combined] Split: {self.split}\")\n",
    "        print(f\"Target Slices: {self.target_slices}\")\n",
    "        print(f\"Scanning subjects in: {self.split_dir}\")\n",
    "        print(f\"Loading metadata from: {self.csv_path}\")\n",
    "\n",
    "        # Basic checks\n",
    "        if not self.feature_root.is_dir(): raise FileNotFoundError(f\"Feature root not found: {self.feature_root}\")\n",
    "        if not self.split_dir.is_dir(): raise FileNotFoundError(f\"Split directory not found: {self.split_dir}\")\n",
    "        if not self.csv_path.is_file(): raise FileNotFoundError(f\"CSV file not found: {self.csv_path}\")\n",
    "\n",
    "        # Load CSV and build mapping\n",
    "        try:\n",
    "            df = pd.read_csv(self.csv_path)\n",
    "            self.meta_dict = df.set_index('filename')['age'].to_dict()\n",
    "            self.subject_dir_to_filename = {}\n",
    "            nii_gz_keys = {k.replace(\".nii.gz\", \"\") for k in self.meta_dict.keys()}\n",
    "            nii_keys = {k.replace(\".nii\", \"\") for k in self.meta_dict.keys()}\n",
    "            potential_subject_dirs = [d.name for d in self.split_dir.iterdir() if d.is_dir()]\n",
    "            mapped_count = 0\n",
    "            for subj_dir_name in potential_subject_dirs:\n",
    "                matched_key = None\n",
    "                # Using the same heuristic, adjust if needed based on actual directory names\n",
    "                base_subj_name = subj_dir_name.split('_mri_brainmask')[0]\n",
    "                for key_base in nii_gz_keys:\n",
    "                    if key_base.startswith(base_subj_name):\n",
    "                         matched_key = key_base + \".nii.gz\"\n",
    "                         break\n",
    "                if not matched_key:\n",
    "                     for key_base in nii_keys:\n",
    "                          if key_base.startswith(base_subj_name):\n",
    "                               matched_key = key_base + \".nii\"\n",
    "                               break\n",
    "                if matched_key and matched_key in self.meta_dict:\n",
    "                    self.subject_dir_to_filename[subj_dir_name] = matched_key\n",
    "                    mapped_count += 1\n",
    "            print(f\"Loaded metadata for {len(self.meta_dict)} subjects from CSV.\")\n",
    "            print(f\"Successfully mapped {mapped_count} subject directories in '{self.split}' to CSV entries.\")\n",
    "            if mapped_count < len(potential_subject_dirs):\n",
    "                 print(f\"Warning: Could not map {len(potential_subject_dirs) - mapped_count} directories to CSV.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading/processing CSV or mapping directories: {e}\")\n",
    "\n",
    "        # Find subject directories that have ALL required slice files\n",
    "        self.valid_subject_dirs = []\n",
    "        subjects_missing_slices = 0\n",
    "        subjects_without_meta = 0\n",
    "\n",
    "        print(f\"Scanning {len(potential_subject_dirs)} potential subject directories for all target slices...\")\n",
    "        all_subject_dirs_in_split = [d for d in self.split_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "        for subject_dir in all_subject_dirs_in_split:\n",
    "            subject_dir_name = subject_dir.name\n",
    "            # Check if metadata exists for this subject directory\n",
    "            if subject_dir_name in self.subject_dir_to_filename:\n",
    "                all_slices_found = True\n",
    "                for slice_info in self.target_slices:\n",
    "                    slice_filename = f\"slice_{slice_info['orientation']}_{slice_info['index']}.npy\"\n",
    "                    expected_slice_path = subject_dir / slice_filename\n",
    "                    if not expected_slice_path.is_file():\n",
    "                        all_slices_found = False\n",
    "                        # print(f\"Debug: Missing {expected_slice_path} for subject {subject_dir_name}\") # Optional debug\n",
    "                        break # No need to check further slices for this subject\n",
    "                if all_slices_found:\n",
    "                    self.valid_subject_dirs.append(subject_dir)\n",
    "                else:\n",
    "                    subjects_missing_slices += 1\n",
    "            else:\n",
    "                subjects_without_meta += 1 # Count dirs we couldn't map to CSV\n",
    "\n",
    "        if subjects_without_meta > 0:\n",
    "             print(f\"Info: {subjects_without_meta} subject directories were skipped (no metadata mapping).\")\n",
    "        if subjects_missing_slices > 0:\n",
    "            print(f\"Warning: {subjects_missing_slices} subjects with metadata were missing at least one target slice.\")\n",
    "\n",
    "        if not self.valid_subject_dirs:\n",
    "             raise RuntimeError(f\"No subjects found with all required slices in {self.split_dir} with matching metadata.\")\n",
    "\n",
    "        print(f\"Found {len(self.valid_subject_dirs)} valid subjects with all required slices in split {self.split}.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_subject_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_dir = self.valid_subject_dirs[idx]\n",
    "        subject_dir_name = subject_dir.name\n",
    "        original_filename = self.subject_dir_to_filename.get(subject_dir_name)\n",
    "        if not original_filename:\n",
    "             raise ValueError(f\"Internal Error: Could not find original filename for valid subject dir {subject_dir_name}\")\n",
    "\n",
    "        pooled_embeddings = []\n",
    "        try:\n",
    "            for slice_info in self.target_slices:\n",
    "                slice_filename = f\"slice_{slice_info['orientation']}_{slice_info['index']}.npy\"\n",
    "                slice_path = subject_dir / slice_filename\n",
    "                embedding = np.load(slice_path)\n",
    "                embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "\n",
    "                # Apply Global Average Pooling (GAP)\n",
    "                if len(embedding_tensor.shape) == 4 and embedding_tensor.shape[0] == 1:\n",
    "                     pooled = F.adaptive_avg_pool2d(embedding_tensor, (1, 1)).squeeze() # -> (C,) e.g., (256,)\n",
    "                elif len(embedding_tensor.shape) == 3: # Assume (C, H, W)\n",
    "                     pooled = F.adaptive_avg_pool2d(embedding_tensor.unsqueeze(0), (1, 1)).squeeze() # Add batch, pool, squeeze -> (C,)\n",
    "                elif len(embedding_tensor.shape) == 1: # Assume already pooled (C,)\n",
    "                     pooled = embedding_tensor\n",
    "                else:\n",
    "                     raise ValueError(f\"Unexpected embedding shape {embedding_tensor.shape} for {slice_path}\")\n",
    "\n",
    "                if pooled.shape[0] != 256: # Ensure channel dim is correct after pooling\n",
    "                     raise ValueError(f\"Pooled embedding channel dim is not 256 for {slice_path}: {pooled.shape}\")\n",
    "\n",
    "                pooled_embeddings.append(pooled)\n",
    "\n",
    "            # Concatenate the pooled embeddings\n",
    "            concatenated_embedding = torch.cat(pooled_embeddings, dim=0) # Shape: (1024,)\n",
    "\n",
    "            # Get age\n",
    "            age = self.meta_dict[original_filename]\n",
    "            age_tensor = torch.tensor(age, dtype=torch.float32)\n",
    "\n",
    "            return concatenated_embedding, age_tensor\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading/processing slices for subject {subject_dir_name}: {e}\")\n",
    "            raise e # Re-raise\n",
    "\n",
    "# --- Model Definition (Attention Fusion MLP) ---\n",
    "class AttentionFusionMLP(nn.Module):\n",
    "    def __init__(self, num_slices=4, embed_dim_per_slice=256, num_heads=4,\n",
    "                 mlp_hidden_dim1=128, mlp_hidden_dim2=64, mlp_hidden_dim3=32, dropout_rate=0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_slices (int): Number of input slices (e.g., 4).\n",
    "            embed_dim_per_slice (int): Dimension of the embedding from each slice (e.g., 256).\n",
    "            num_heads (int): Number of attention heads for fusing slice features. Must divide embed_dim_per_slice.\n",
    "            mlp_hidden_dim1, mlp_hidden_dim2, mlp_hidden_dim3 (int): Hidden dimensions for the final MLP.\n",
    "            dropout_rate (float): Dropout rate for the MLP.\n",
    "        \"\"\"\n",
    "        super(AttentionFusionMLP, self).__init__()\n",
    "        self.num_slices = num_slices\n",
    "        self.embed_dim = embed_dim_per_slice\n",
    "\n",
    "        if self.embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim_per_slice ({self.embed_dim}) must be divisible by num_heads ({num_heads})\")\n",
    "\n",
    "        # Attention layer to fuse features across slices\n",
    "        self.fusion_attention = nn.MultiheadAttention(embed_dim=self.embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.norm_attn = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "        # Learnable query vector\n",
    "        self.query_vector = nn.Parameter(torch.randn(1, 1, self.embed_dim)) # (1, 1, embed_dim) for broadcasting\n",
    "\n",
    "        # MLP for final age prediction\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, mlp_hidden_dim1), nn.BatchNorm1d(mlp_hidden_dim1), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mlp_hidden_dim1, mlp_hidden_dim2), nn.BatchNorm1d(mlp_hidden_dim2), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mlp_hidden_dim2, mlp_hidden_dim3), nn.BatchNorm1d(mlp_hidden_dim3), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mlp_hidden_dim3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, concat_dim) e.g., (32, 1024)\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Reshape concatenated features into (batch_size, num_slices, embed_dim)\n",
    "        x_reshaped = x.view(batch_size, self.num_slices, self.embed_dim)\n",
    "\n",
    "        # Expand the learnable query vector to match the batch size\n",
    "        query = self.query_vector.expand(batch_size, -1, -1) # -> (batch_size, 1, embed_dim)\n",
    "\n",
    "        # Apply attention: Query attends to the slice features (Keys and Values)\n",
    "        attn_output, attn_weights = self.fusion_attention(query=query, key=x_reshaped, value=x_reshaped)\n",
    "\n",
    "        # Apply layer normalization\n",
    "        fused_features = self.norm_attn(attn_output) # Still (batch_size, 1, embed_dim)\n",
    "\n",
    "        # Squeeze the sequence dimension\n",
    "        fused_features = fused_features.squeeze(1) # -> (batch_size, embed_dim)\n",
    "\n",
    "        # Pass the fused features through the MLP\n",
    "        output = self.mlp(fused_features) # -> (batch_size, 1)\n",
    "\n",
    "        return output.squeeze(-1) # -> (batch_size,)\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    num_samples = 0\n",
    "    for features, ages in loader:\n",
    "        features, ages = features.to(device), ages.to(device)\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, ages)\n",
    "        total_loss += loss.item() * features.size(0)\n",
    "        # Calculate MAE directly using L1Loss reduction='sum' for batch, then sum across batches\n",
    "        mae = F.l1_loss(predictions, ages, reduction='sum')\n",
    "        total_mae += mae.item()\n",
    "        num_samples += features.size(0)\n",
    "\n",
    "    avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "    avg_mae = total_mae / num_samples if num_samples > 0 else float('inf')\n",
    "    return avg_loss, avg_mae\n",
    "\n",
    "# --- Main Test Set Evaluation Logic ---\n",
    "if __name__ == \"__main__\": # Ensures this runs when script is executed\n",
    "\n",
    "    print(f\"\\n--- Evaluating Attention Fusion MLP on Test Set ---\")\n",
    "    print(f\"Loading model from: {MODEL_SAVE_PATH}\")\n",
    "    print(f\"Using test data from: {FEATURE_ROOT} (split: {TEST_SPLIT})\")\n",
    "    print(f\"CSV Path: {CSV_PATH}\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "\n",
    "    test_dataset = None\n",
    "    test_loader = None\n",
    "    model = None\n",
    "    criterion = None\n",
    "\n",
    "    try:\n",
    "        # 1. Create Test Dataset and DataLoader\n",
    "        print(\"\\nSetting up test dataset...\")\n",
    "        test_dataset = CombinedSliceDataset(FEATURE_ROOT, CSV_PATH, TEST_SPLIT, TARGET_SLICES)\n",
    "        print(\"Setting up test dataloader...\")\n",
    "        test_loader = DataLoader(test_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=False, # Important for evaluation\n",
    "                                 num_workers=NUM_WORKERS,\n",
    "                                 pin_memory=True)\n",
    "\n",
    "        # 2. Initialize Model\n",
    "        print(\"\\nInitializing AttentionFusionMLP model structure...\")\n",
    "        # Instantiate with the EXACT same parameters as the trained model\n",
    "        model = AttentionFusionMLP(\n",
    "            num_slices=NUM_SLICES_TEST,\n",
    "            embed_dim_per_slice=EMBED_DIM_PER_SLICE_TEST,\n",
    "            num_heads=NUM_HEADS_TEST,\n",
    "            mlp_hidden_dim1=MLP_HIDDEN_DIM1_TEST,\n",
    "            mlp_hidden_dim2=MLP_HIDDEN_DIM2_TEST,\n",
    "            mlp_hidden_dim3=MLP_HIDDEN_DIM3_TEST,\n",
    "            dropout_rate=DROPOUT_RATE_TEST\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # 3. Load Trained Weights\n",
    "        if MODEL_SAVE_PATH.is_file():\n",
    "            print(f\"Loading model state dict from {MODEL_SAVE_PATH}...\")\n",
    "            model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "            model.eval() # Set model to evaluation mode\n",
    "            print(\"Model loaded successfully.\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Model file not found at {MODEL_SAVE_PATH}. Cannot evaluate.\")\n",
    "\n",
    "        # 4. Define Criterion\n",
    "        criterion = nn.L1Loss() # MAE Loss\n",
    "\n",
    "        # 5. Evaluate\n",
    "        print(\"\\nStarting evaluation on the test set...\")\n",
    "        test_loss, test_mae = evaluate(model, test_loader, criterion, DEVICE)\n",
    "\n",
    "        # 6. Print Results\n",
    "        print(f\"\\n--- Attention Fusion Model Test Set Results ---\")\n",
    "        print(f\"Test Loss (calculated using L1Loss): {test_loss:.4f}\")\n",
    "        print(f\"Test MAE: {test_mae:.3f}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(\"Skipping test set evaluation.\")\n",
    "    except (FileNotFoundError, ValueError, RuntimeError) as e:\n",
    "        print(f\"\\nError during dataset/dataloader creation or evaluation: {e}\")\n",
    "        print(\"Skipping test set evaluation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during test set evaluation: {e}\")\n",
    "        print(\"Skipping test set evaluation.\")\n",
    "    finally:\n",
    "        # 7. Clean up\n",
    "        print(\"\\nCleaning up test evaluation resources...\")\n",
    "        if 'model' in locals() and model is not None: del model\n",
    "        if 'test_loader' in locals() and test_loader is not None: del test_loader\n",
    "        if 'test_dataset' in locals() and test_dataset is not None: del test_dataset\n",
    "        if 'criterion' in locals() and criterion is not None: del criterion\n",
    "        if 'test_loss' in locals(): del test_loss\n",
    "        if 'test_mae' in locals(): del test_mae\n",
    "        gc.collect()\n",
    "        if DEVICE.startswith('cuda'):\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"Test evaluation finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
